{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDDQN  (Double Dueling Deep Q Learning with Prioritized Experience Replay)  DoomüïπÔ∏è\n",
    "In this notebook we'll implement an agent <b>that plays Doom by using a Dueling Double Deep Q learning architecture with Prioritized Experience Replay.</b> <br>\n",
    "\n",
    "Our agent playing Doom after 3 hours of training of **CPU**, remember that our agent needs about 2 days of **GPU** to have optimal score, we'll train from beginning to end the most important architectures (PPO with transfer):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/projects/doomdeathmatc.gif\" alt=\"Doom Deathmatch\"/>\n",
    "\n",
    "But we can see that our agent **understand that he needs to kill enemies before being able to move forward (if he moves forward without killing ennemies he will be killed before getting the vest)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
    "<br>\n",
    "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients‚Ä¶), and how to implement them with Tensorflow.**\n",
    "<br><br>\n",
    "    \n",
    "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
    "<br>\n",
    "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
    "<br>\n",
    "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
    "<br><br>\n",
    "</p> \n",
    "\n",
    "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "\n",
    "\n",
    "## Any questions üë®‚Äçüíª\n",
    "<p> If you have any questions, feel free to ask me: </p>\n",
    "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
    "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
    "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
    "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
    "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
    "    \n",
    "## How to help  üôå\n",
    "3 ways:\n",
    "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
    "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
    "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
    "<br>\n",
    "\n",
    "## Important note ü§î\n",
    "<b> You can run it on your computer but it's better to run it on GPU based services</b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
    "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
    "<br>\n",
    "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
    "\n",
    "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites üèóÔ∏è\n",
    "Before diving on the notebook **you need to understand**:\n",
    "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
    "- Q-learning [Article](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)\n",
    "- Deep Q-Learning [Article](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)\n",
    "- Improvments in Deep Q-learning [Article]()\n",
    "- You can follow this notebook using my [video tutorial](https://www.youtube.com/embed/-Ynjw0Vl3i4?showinfo=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-Ynjw0Vl3i4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-Ynjw0Vl3i4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "- Now that we imported the libraries/dependencies, we will create our environment.\n",
    "- Doom environment takes:\n",
    "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
    "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
    "- Note: We have 7 possible actions: turn left, turn right, move left, move right, shoot (attack)...`[[0,0,0,0,1]...]` so we don't need to do one hot encoding (thanks to <a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out). \n",
    "\n",
    "### Our environment\n",
    "<img src=\"https://simoninithomas.github.io/Deep_reinforcement_learning_Course/assets/img/video%20projects/deadlycorridor.png\" style=\"max-width:500px;\" alt=\"Vizdoom deadly corridor\"/>\n",
    "\n",
    "The purpose of this scenario is to teach the agent to navigate towards his fundamental goal (the vest) and make sure he survives at the same time.\n",
    "\n",
    "- Map is a corridor with shooting monsters on both sides (6 monsters in total). \n",
    "- A green vest is placed at the oposite end of the corridor. \n",
    "- **Reward is proportional (negative or positive) to change of the distance between the player and the vest.** \n",
    "- If player ignores monsters on the sides and runs straight for the vest he will be killed somewhere along the way. \n",
    "- To ensure this behavior doom_skill = 5 (config) is needed.\n",
    "\n",
    "<br>\n",
    "REWARDS:\n",
    "\n",
    "- +dX for getting closer to the vest. -dX for getting further from the vest.\n",
    "- death penalty = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we create our environment\n",
    "\"\"\"\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case deadly_corridor scenario)\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    \n",
    "    game.init()\n",
    "\n",
    "    # Here we create an hot encoded version of our actions (5 possible actions)\n",
    "    # possible_actions = [[1, 0, 0, 0, 0], [0, 1, 0, 0, 0]...]\n",
    "    possible_actions = np.identity(3,dtype=int).tolist()\n",
    "    print(possible_actions)\n",
    "    \n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the preprocessing functions ‚öôÔ∏è\n",
    "### preprocess_frame\n",
    "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
    "<br><br>\n",
    "Our steps:\n",
    "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
    "- Crop the screen (in our case we remove the roof because it contains no information)\n",
    "- We normalize pixel values\n",
    "- Finally we resize the preprocessed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess_frame:\n",
    "    Take a frame.\n",
    "    Resize it.\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "        \n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "    Normalize it.\n",
    "    \n",
    "    return preprocessed_frame\n",
    "    \n",
    "    \"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Crop the screen (remove part that contains no information)\n",
    "    # [Up: Down, Left: right]\n",
    "    cropped_frame = frame[15:-5,20:-20]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(cropped_frame, [100,120])\n",
    "    \n",
    "    return preprocessed_frame # 100x120x1 frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_frame(game):\n",
    "    game.new_episode()\n",
    "    frame = game.get_state().screen_buffer\n",
    "    processed_frame = preprocess_frame(frame)\n",
    "    plt.imshow(processed_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19bf75e5fd0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_frame(game)\n",
    "print(game.get_available_buttons_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack_frames\n",
    "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
    "\n",
    "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\n",
    "\n",
    "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
    "\n",
    "- First we preprocess frame\n",
    "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
    "- Finally we **build the stacked state**\n",
    "\n",
    "This is how work stack:\n",
    "- For the first frame, we feed 4 frames\n",
    "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
    "- And so on\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\n",
    "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((100,120), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((100,120), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
    "\n",
    "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
    "- Then, you'll add the training hyperparameters when you implement the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [100,120,4]      # Our input is a stack of 4 frames hence 100x120x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size()              # 7 possible actions\n",
    "learning_rate =  0.001    # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 5000         # Total episodes for training\n",
    "max_steps = 5000              # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# FIXED Q TARGETS HYPERPARAMETERS \n",
    "max_tau = 10000 #Tau is the C step where we update our target network\n",
    "\n",
    "# EXPLORATION HYPERPARAMETERS for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.00005            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q LEARNING hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "## If you have GPU change to 1million\n",
    "pretrain_length = 80000 # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 80000   # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Dueling Double Deep Q-learning Neural Network model (aka DDDQN) üß†\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1500/1*FkHqwA2eSGixdS-3dvVoMA.png\" alt=\"Dueling Double Deep Q Learning Model\" />\n",
    "This is our Dueling Double Deep Q-learning model:\n",
    "- We take a stack of 4 frames as input\n",
    "- It passes through 3 convnets\n",
    "- Then it is flatened\n",
    "- Then it is passed through 2 streams\n",
    "    - One that calculates V(s)\n",
    "    - The other that calculates A(s,a)\n",
    "- Finally an agregating layer\n",
    "- It outputs a Q value for each actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQNNet:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name\n",
    "        \n",
    "        \n",
    "        # We use tf.variable_scope here to know which network we're using (DQN or target_net)\n",
    "        # it will be useful when we will update our w- parameters (by copy the DQN parameters)\n",
    "        with tf.variable_scope(self.name):\n",
    "            \n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "            # [None, 100, 120, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            \n",
    "            #\n",
    "            self.ISWeights_ = tf.placeholder(tf.float32, [None,1], name='IS_weights')\n",
    "            \n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, action_size], name=\"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\"\n",
    "            First convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            # Input is 100x120x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Second convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\")\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Third convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
    "            \n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            \n",
    "            ## Here we separate into two streams\n",
    "            # The one that calculate V(s)\n",
    "            self.value_fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"value_fc\")\n",
    "            \n",
    "            self.value = tf.layers.dense(inputs = self.value_fc,\n",
    "                                        units = 1,\n",
    "                                        activation = None,\n",
    "                                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"value\")\n",
    "            \n",
    "            # The one that calculate A(s,a)\n",
    "            self.advantage_fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"advantage_fc\")\n",
    "            \n",
    "            self.advantage = tf.layers.dense(inputs = self.advantage_fc,\n",
    "                                        units = self.action_size,\n",
    "                                        activation = None,\n",
    "                                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"advantages\")\n",
    "            \n",
    "            # Agregating layer\n",
    "            # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "            self.output = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "              \n",
    "            # Q is our predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            # The loss is modified because of PER \n",
    "            self.absolute_errors = tf.abs(self.target_Q - self.Q)# for updating Sumtree\n",
    "            \n",
    "            self.loss = tf.reduce_mean(self.ISWeights_ * tf.squared_difference(self.target_Q, self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"DQNetwork\")\n",
    "\n",
    "# Instantiate the target network\n",
    "TargetNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"TargetNetwork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Prioritized Experience Replay üîÅ\n",
    "Now that we create our Neural Network, **we need to implement the Prioritized Experience Replay method.** <br>\n",
    "\n",
    "As explained in the article, **we can't use a simple array to do that because sampling from it will be not efficient, so we use a binary tree data type (in a binary tree each node has no + than 2 children).** More precisely, a sumtree, which is a binary tree where parents nodes are the sum of the children nodes.\n",
    "\n",
    "If you don't know what is a binary tree check this awesome video https://www.youtube.com/watch?v=oSWTXtMglKE\n",
    "\n",
    "\n",
    "This SumTree implementation was taken from Morvan Zhou in his chinese course about Reinforcement Learning\n",
    "\n",
    "To summarize:\n",
    "- **Step 1**: We construct a SumTree, which is a Binary Sum tree where leaves contains the priorities and a data array where index points to the index of leaves.\n",
    "    <img src=\"https://cdn-images-1.medium.com/max/1200/1*Go9DNr7YY-wMGdIQ7HQduQ.png\" alt=\"SumTree\"/>\n",
    "    <br><br>\n",
    "    - **def __init__**: Initialize our SumTree data object with all nodes = 0 and data (data array) with all = 0.\n",
    "    - **def add**: add our priority score in the sumtree leaf and experience (S, A, R, S', Done) in data.\n",
    "    - **def update**: we update the leaf priority score and propagate through tree.\n",
    "    - **def get_leaf**: retrieve priority score, index and experience associated with a leaf.\n",
    "    - **def total_priority**: get the root node value to calculate the total priority score of our replay buffer.\n",
    "<br><br>\n",
    "- **Step 2**: We create a Memory object that will contain our sumtree and data.\n",
    "    - **def __init__**: generates our sumtree and data by instantiating the SumTree object.\n",
    "    - **def store**: we store a new experience in our tree. Each new experience will **have priority = max_priority** (and then this priority will be corrected during the training (when we'll calculating the TD error hence the priority score).\n",
    "    - **def sample**:\n",
    "         - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "         - Then a value is uniformly sampled from each range\n",
    "         - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "         - Then, we calculate IS weights for each minibatch element\n",
    "    - **def update_batch**: update the priorities on the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version of Morvan Zhou: \n",
    "    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n",
    "        \n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "        0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we add our priority score in the sumtree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "tree_index  0 0  0  We fill the leaves from left to right\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "        \n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "        \n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "        \n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    \n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propagate the change through tree\n",
    "    \"\"\"\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            \n",
    "            \"\"\"\n",
    "            Here we want to access the line above\n",
    "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "            \n",
    "                0\n",
    "               / \\\n",
    "              1   2\n",
    "             / \\ / \\\n",
    "            3  4 5  [6] \n",
    "            \n",
    "            If we are in leaf at index 6, we updated the priority score\n",
    "            We need then to update index 2 node\n",
    "            So tree_index = (tree_index - 1) // 2\n",
    "            tree_index = (6-1)//2\n",
    "            tree_index = 2 (because // round the result)\n",
    "            \"\"\"\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for experiences\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True: # the while loop is faster than the method in the reference code\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            \n",
    "            else: # downward search, always search for a higher priority node\n",
    "                \n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                    \n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "            \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we don't use deque anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree \n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        \n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "        \n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "        \n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        \n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "    \n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "                                   \n",
    "            b_idx[i]= index\n",
    "            \n",
    "            experience = [data]\n",
    "            \n",
    "            memory_b.append(experience)\n",
    "        \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll **deal with the empty memory problem**: we pre-populate our memory by taking random actions and storing the experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "\n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        #experience = np.hstack((state, [action, reward], next_state, done))\n",
    "        \n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/dddqn/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dddqn/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "tf.summary.scalar(\"Advantage\", DQNetwork.advantage)\n",
    "tf.summary.scalar(\"Value\", DQNetwork.value)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train our Agent üèÉ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Our algorithm:\n",
    "<br>\n",
    "* Initialize the weights for DQN\n",
    "* Initialize target value weights w- <- w\n",
    "* Init the environment\n",
    "* Initialize the decay rate (that will use to reduce epsilon) \n",
    "<br><br>\n",
    "* **For** episode to max_episode **do** \n",
    "    * Make new episode\n",
    "    * Set step to 0\n",
    "    * Observe the first state $s_0$\n",
    "    <br><br>\n",
    "    * **While** step < max_steps **do**:\n",
    "        * Increase decay_rate\n",
    "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "        \n",
    "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
    "        * Set target $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma Q(s',argmax_{a'}{Q(s', a', w), w^-)}$\n",
    "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "        * Every C steps, reset: $w^- \\leftarrow w$\n",
    "    * **endfor**\n",
    "    <br><br>\n",
    "* **endfor**\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With œµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps us to copy one set of variables to another\n",
    "# In our case we use it when we want to copy the parameters of DQN to Target_network\n",
    "# Thanks of the very good implementation of Arthur Juliani https://github.com/awjuliani\n",
    "def update_target_graph():\n",
    "    \n",
    "    # Get the parameters of our DQNNetwork\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"DQNetwork\")\n",
    "    \n",
    "    # Get the parameters of our Target_network\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TargetNetwork\")\n",
    "\n",
    "    op_holder = []\n",
    "    \n",
    "    # Update our target_network parameters with DQNNetwork parameters\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "tags and values not the same shape: [] != [64,3] (tag 'Advantage')\n\t [[Node: Advantage = ScalarSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Advantage/tags, DQNetwork/advantages/BiasAdd/_69)]]\n\nCaused by op 'Advantage', defined at:\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\gen.py\", line 1152, in inner\n    self.run()\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\gen.py\", line 1069, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\gen.py\", line 307, in wrapper\n    yielded = next(result)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\gen.py\", line 307, in wrapper\n    yielded = next(result)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\gen.py\", line 307, in wrapper\n    yielded = next(result)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-c82ba7da7fce>\", line 6, in <module>\n    tf.summary.scalar(\"Advantage\", DQNetwork.advantage)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\summary\\summary.py\", line 90, in scalar\n    val = _gen_logging_ops.scalar_summary(tags=tag, values=tensor, name=scope)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\", line 622, in scalar_summary\n    \"ScalarSummary\", tags=tags, values=values, name=name)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): tags and values not the same shape: [] != [64,3] (tag 'Advantage')\n\t [[Node: Advantage = ScalarSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Advantage/tags, DQNetwork/advantages/BiasAdd/_69)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: tags and values not the same shape: [] != [64,3] (tag 'Advantage')\n\t [[Node: Advantage = ScalarSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Advantage/tags, DQNetwork/advantages/BiasAdd/_69)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-159d8dfc98d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    155\u001b[0m                                                    \u001b[0mDQNetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_Q\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargets_mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                                                    \u001b[0mDQNetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mactions_mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                                               DQNetwork.ISWeights_: ISWeights_mb})\n\u001b[0m\u001b[0;32m    158\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: tags and values not the same shape: [] != [64,3] (tag 'Advantage')\n\t [[Node: Advantage = ScalarSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Advantage/tags, DQNetwork/advantages/BiasAdd/_69)]]\n\nCaused by op 'Advantage', defined at:\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\gen.py\", line 1152, in inner\n    self.run()\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\gen.py\", line 1069, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\gen.py\", line 307, in wrapper\n    yielded = next(result)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\gen.py\", line 307, in wrapper\n    yielded = next(result)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tornado\\gen.py\", line 307, in wrapper\n    yielded = next(result)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-c82ba7da7fce>\", line 6, in <module>\n    tf.summary.scalar(\"Advantage\", DQNetwork.advantage)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\summary\\summary.py\", line 90, in scalar\n    val = _gen_logging_ops.scalar_summary(tags=tag, values=tensor, name=scope)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\", line 622, in scalar_summary\n    \"ScalarSummary\", tags=tags, values=values, name=name)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): tags and values not the same shape: [] != [64,3] (tag 'Advantage')\n\t [[Node: Advantage = ScalarSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Advantage/tags, DQNetwork/advantages/BiasAdd/_69)]]\n"
     ]
    }
   ],
   "source": [
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "        \n",
    "        # Set tau = 0\n",
    "        tau = 0\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "        \n",
    "        # Update the parameters of our TargetNetwork with DQN_weights\n",
    "        update_target = update_target_graph()\n",
    "        sess.run(update_target)\n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "            game.new_episode()\n",
    "            \n",
    "            state = game.get_state().screen_buffer\n",
    "            \n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                \n",
    "                # Increase the C step\n",
    "                tau += 1\n",
    "                \n",
    "                # Increase decay_step\n",
    "                decay_step +=1\n",
    "                \n",
    "                # With œµ select a random action atat, otherwise select a = argmaxQ(st,a)\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "                # Do the action\n",
    "                reward = game.make_action(action)\n",
    "\n",
    "                # Look if the episode is finished\n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                # Add the reward to total reward\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((120,140), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    if episode > 0:\n",
    "                        if total_episodes % 50 == 0:\n",
    "                            print('Episode: {}'.format(episode),\n",
    "                                  'Total reward: {}'.format(total_reward),\n",
    "                                  'Training loss: {:.4f}'.format(loss),\n",
    "                                  'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    # Add experience to memory\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "\n",
    "                else:\n",
    "                    # Get the next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "\n",
    "                    # Add experience to memory\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "                    \n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "                ### LEARNING PART            \n",
    "                # Obtain random mini-batch from memory\n",
    "                tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "                \n",
    "                states_mb = np.array([each[0][0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[0][1] for each in batch])\n",
    "                rewards_mb = np.array([each[0][2] for each in batch]) \n",
    "                next_states_mb = np.array([each[0][3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[0][4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                \n",
    "                ### DOUBLE DQN Logic\n",
    "                # Use DQNNetwork to select the action to take at next_state (a') (action with the highest Q-value)\n",
    "                # Use TargetNetwork to calculate the Q_val of Q(s',a')\n",
    "                \n",
    "                # Get Q values for next_state \n",
    "                q_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                # Calculate Qtarget for all actions that state\n",
    "                q_target_next_state = sess.run(TargetNetwork.output, feed_dict = {TargetNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                \n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma * Qtarget(s',a') \n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "                    \n",
    "                    # We got a'\n",
    "                    action = np.argmax(q_next_state[i])\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        # Take the Qtarget for action a'\n",
    "                        target = rewards_mb[i] + gamma * q_target_next_state[i][action]\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                \n",
    "                _, loss, absolute_errors = sess.run([DQNetwork.optimizer, DQNetwork.loss, DQNetwork.absolute_errors],\n",
    "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                               DQNetwork.target_Q: targets_mb,\n",
    "                                               DQNetwork.actions_: actions_mb,\n",
    "                                              DQNetwork.ISWeights_: ISWeights_mb})\n",
    "              \n",
    "                \n",
    "                \n",
    "                # Update priority\n",
    "                memory.batch_update(tree_idx, absolute_errors)\n",
    "                \n",
    "                \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                   DQNetwork.target_Q: targets_mb,\n",
    "                                                   DQNetwork.actions_: actions_mb,\n",
    "                                              DQNetwork.ISWeights_: ISWeights_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "                if tau > max_tau:\n",
    "                    # Update the parameters of our TargetNetwork with DQN_weights\n",
    "                    update_target = update_target_graph()\n",
    "                    sess.run(update_target)\n",
    "                    tau = 0\n",
    "                    print(\"Model updated\")\n",
    "\n",
    "            # Save model every 5 episodes\n",
    "            if episode % 20 == 0:\n",
    "                save_path = saver.save(sess, \"/models/ddqn/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Watch our Agent play üëÄ\n",
    "Now that we trained our agent, we can test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration (TESTING)\n",
    "    game.load_config(\"deadly_corridor.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case deadly_corridor scenario)\n",
    "    game.set_doom_scenario_path(\"deadly_corridor.wad\")\n",
    "    \n",
    "    game.init()    \n",
    "    \n",
    "    # Load the model\n",
    "    saver = tf.train.import_meta_graph(\"/models/ddqn/model.meta\")\n",
    "    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint(\"/models/ddqn\"))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(100):\n",
    "        \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "        while not game.is_episode_finished():\n",
    "            ## EPSILON GREEDY STRATEGY\n",
    "            # Choose action a from state s using epsilon greedy.\n",
    "            ## First we randomize a number\n",
    "            exp_exp_tradeoff = np.random.rand()\n",
    "            \n",
    "\n",
    "            explore_probability = 0.01\n",
    "    \n",
    "            \n",
    "            if exp_exp_tradeoff < explore_probability:\n",
    "                action = random.choice(possible_actions)\n",
    "            else:\n",
    "                # Get action from Q-network (exploitation)\n",
    "                # Estimate the Qs values state\n",
    "                Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "\n",
    "                # Take the biggest Q value (= the best action)\n",
    "                choice = np.argmax(Qs)\n",
    "                action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "        \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "        \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    \n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
