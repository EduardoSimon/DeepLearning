{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\vizdoom-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "- Now that we imported the libraries/dependencies, we will create our environment.\n",
    "- Doom environment takes:\n",
    "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
    "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
    "- Note: We have 3 possible actions `[[0,0,1], [1,0,0], [0,1,0]]` so we don't need to do one hot encoding (thanks to < a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out. \n",
    "\n",
    "### Our environment\n",
    "<img src=\"assets/doom.png\" style=\"max-width:500px;\" alt=\"Doom\"/>\n",
    "                                    \n",
    "- A monster is spawned **randomly somewhere along the opposite wall**. \n",
    "- Player can only go **left/right and shoot**. \n",
    "- 1 hit is enough **to kill the monster**. \n",
    "- Episode finishes when **monster is killed or on timeout (300)**.\n",
    "<br><br>\n",
    "REWARDS:\n",
    "\n",
    "- +101 for killing the monster \n",
    "- -5 for missing \n",
    "- Episode ends after killing the monster or on timeout.\n",
    "- living reward = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we create our environment\n",
    "\"\"\"\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"deadly_corridor.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case basic scenario)\n",
    "    game.set_doom_scenario_path(\"deadly_corridor.wad\")\n",
    "    \n",
    "    possible_actions = np.identity(7,dtype=int).tolist()\n",
    "    \n",
    "    return game, possible_actions\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game,possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the preprocessing functions ‚öôÔ∏è\n",
    "### preprocess_frame\n",
    "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
    "<br><br>\n",
    "Our steps:\n",
    "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
    "- Crop the screen (in our case we remove the roof because it contains no information)\n",
    "- We normalize pixel values\n",
    "- Finally we resize the preprocessed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess_frame:\n",
    "    Take a frame.\n",
    "    Resize it.\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "        \n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "    Normalize it.\n",
    "    \n",
    "    return preprocessed_frame\n",
    "    \n",
    "    \"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Greyscale frame already done in our vizdoom config\n",
    "    # x = np.mean(frame,-1)\n",
    "    # Crop the screen (remove the roof because it contains no information)\n",
    "    cropped_frame = frame[15:-5,20:-20]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [100,120])\n",
    "    \n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack_frames\n",
    "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
    "\n",
    "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\n",
    "\n",
    "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
    "\n",
    "- First we preprocess frame\n",
    "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
    "- Finally we **build the stacked state**\n",
    "\n",
    "This is how work stack:\n",
    "- For the first frame, we feed 4 frames\n",
    "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
    "- And so on\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\n",
    "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((100,120), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((100,120), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
    "\n",
    "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
    "- Then, you'll add the training hyperparameters when you implement the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [100,120,4]      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size()              # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.00025     # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 5000      # Total episodes for training\n",
    "max_steps = 5000              # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.000005         # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "#targetq hyperparameters\n",
    "max_tau = 10000\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = 100000  # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 100000         # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Deep Q-learning Neural Network model üß†\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/doom/assets/model.png\" alt=\"Model\" />\n",
    "This is our Deep Q-learning model:\n",
    "- We take a stack of 4 frames as input\n",
    "- It passes through 3 convnets\n",
    "- Then it is flatened\n",
    "- Finally it passes through 2 FC layers\n",
    "- It outputs a Q value for each actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name\n",
    "        \n",
    "        with tf.variable_scope(self.name):\n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "            # [None, 100,120,4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.ISWeights_ = tf.placeholder(tf.float32, [None,1], name='IS_weights')\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, action_size], name=\"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\"\n",
    "            First convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            \n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
    "\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Second convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\")\n",
    "            ## --> [9, 9, 64]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Third convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "        \n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
    "            ## --> [3, 3, 128]\n",
    "            \n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            ## --> [1152]\n",
    "            \n",
    "            ##This branch of the net calculates the value of the state, how good is to be at that state\n",
    "            ##if you are about to die no matter what action you choose the state has low value\n",
    "            ##--input: flatten layer, 1152\n",
    "            ##--output: 512 neurons\n",
    "            self.value_fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                            units = 512,\n",
    "                                            activation = tf.nn.elu,\n",
    "                                            kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                            name = \"value_fc\")\n",
    "            \n",
    "            ##--input: flatten layer, 512\n",
    "            ##--output: 1 neuron with the V value\n",
    "            self.value = tf.layers.dense(inputs = self.value_fc,\n",
    "                                        units = 1,\n",
    "                                        activation = None,\n",
    "                                        kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                        name = \"value\")\n",
    "            \n",
    "            #calculates the advantage of an action over the rest for that state\n",
    "            #input: flatten layer, 1152\n",
    "            #output: 512 neurons\n",
    "            self.advantage_fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                              units = 512,\n",
    "                                              activation = tf.nn.elu,\n",
    "                                              kernel_initializer= tf.contrib.layers.xavier_initializer(),\n",
    "                                              name = \"advantage_fc\")\n",
    "            \n",
    "            self.advantage = tf.layers.dense(inputs = self.advantage_fc,\n",
    "                                           units = self.action_size,\n",
    "                                           activation = None,\n",
    "                                           kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                            name = \"advantage\")\n",
    "            \n",
    "        \n",
    "            #aggregation layer\n",
    "            self.output = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage,axis = 1, keepdims = True))\n",
    "\n",
    "            # Q is our predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            # The loss is modified because of PER \n",
    "            self.absolute_errors = tf.abs(self.target_Q - self.Q)# for updating Sumtree\n",
    "              \n",
    "            # The loss is the difference between our predicted Q_values and the Q_target\n",
    "            # Sum(Qtarget - Q)^2\n",
    "            self.loss = tf.reduce_mean(self.ISWeights_ * tf.squared_difference(self.target_Q,self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version of Morvan Zhou: \n",
    "    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n",
    "        \n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "        0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we add our priority score in the sumtree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "tree_index  0 0  0  We fill the leaves from left to right\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "        \n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "        \n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "        \n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    \n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propagate the change through tree\n",
    "    \"\"\"\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            \n",
    "            \"\"\"\n",
    "            Here we want to access the line above\n",
    "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "            \n",
    "                0\n",
    "               / \\\n",
    "              1   2\n",
    "             / \\ / \\\n",
    "            3  4 5  [6] \n",
    "            \n",
    "            If we are in leaf at index 6, we updated the priority score\n",
    "            We need then to update index 2 node\n",
    "            So tree_index = (tree_index - 1) // 2\n",
    "            tree_index = (6-1)//2\n",
    "            tree_index = 2 (because // round the result)\n",
    "            \"\"\"\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for experiences\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True: # the while loop is faster than the method in the reference code\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            \n",
    "            else: # downward search, always search for a higher priority node\n",
    "                \n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                    \n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "            \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DeepQNetwork = DQNetwork(state_size, action_size, learning_rate,\"DeepQNetwork\")\n",
    "\n",
    "#create the QTargetNetwork\n",
    "TargetQNetwork = DQNetwork(state_size,action_size, learning_rate, \"TargetQNetwork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_network():\n",
    "    #every tau steps we copy the weights from our deep network to our TargetNetwok\n",
    "    \n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"DQNetwork\")\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"TargetQNetwork\")\n",
    "    \n",
    "    op_holder = []\n",
    "    \n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Experience Replay üîÅ\n",
    "Now that we create our Neural Network, **we need to implement the Experience Replay method.** <br><br>\n",
    "Here we'll create the Memory object that creates a deque.A deque (double ended queue) is a data type that **removes the oldest element each time that you add a new element.**\n",
    "\n",
    "This part was taken from Udacity : <a href=\"https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb\" Cartpole DQN</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree \n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        \n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "        \n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "        \n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        \n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "    \n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "                                   \n",
    "            b_idx[i]= index\n",
    "            \n",
    "            experience = [data]\n",
    "            \n",
    "            memory_b.append(experience)\n",
    "        \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll **deal with the empty memory problem**: we pre-populate our memory by taking random actions and storing the experience (state, action, reward, new_state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory = Memory(memory_size)\n",
    "\n",
    "game.init()\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "\n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        #experience = np.hstack((state, [action, reward], next_state, done))\n",
    "        \n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state\n",
    "        \n",
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/dqn/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DeepQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train our Agent üèÉ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Our algorithm:\n",
    "<br>\n",
    "* Initialize the weights\n",
    "* Init the environment\n",
    "* Initialize the decay rate (that will use to reduce epsilon) \n",
    "<br><br>\n",
    "* **For** episode to max_episode **do** \n",
    "    * Make new episode\n",
    "    * Set step to 0\n",
    "    * Observe the first state $s_0$\n",
    "    <br><br>\n",
    "    * **While** step < max_steps **do**:\n",
    "        * Increase decay_rate\n",
    "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
    "        * Set $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma \\max_{a'}{Q(s', a')}$\n",
    "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "    * **endfor**\n",
    "    <br><br>\n",
    "* **endfor**\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With œµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DeepQNetwork.output, feed_dict = {DeepQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: -115.99858093261719 Training loss: 0.3062 Explore P: 0.9996\n",
      "Model Saved\n",
      "Episode: 1 Total reward: -103.89242553710938 Training loss: 0.6124 Explore P: 0.9992\n",
      "Episode: 2 Total reward: -102.23280334472656 Training loss: 0.4451 Explore P: 0.9988\n",
      "Episode: 3 Total reward: -106.90557861328125 Training loss: 20.7345 Explore P: 0.9984\n",
      "Episode: 4 Total reward: -89.5968017578125 Training loss: 0.1705 Explore P: 0.9980\n",
      "Episode: 5 Total reward: -76.04985046386719 Training loss: 0.1700 Explore P: 0.9976\n",
      "Model Saved\n",
      "Episode: 6 Total reward: -115.51789855957031 Training loss: 0.1713 Explore P: 0.9972\n",
      "Episode: 7 Total reward: -104.93006896972656 Training loss: 9.9324 Explore P: 0.9967\n",
      "Episode: 8 Total reward: -115.19496154785156 Training loss: 0.3095 Explore P: 0.9956\n",
      "Episode: 9 Total reward: -72.86117553710938 Training loss: 20.8214 Explore P: 0.9952\n",
      "Episode: 10 Total reward: -107.9078369140625 Training loss: 10.3431 Explore P: 0.9948\n",
      "Model Saved\n",
      "Episode: 11 Total reward: -95.64230346679688 Training loss: 0.4830 Explore P: 0.9944\n",
      "Episode: 12 Total reward: -68.32989501953125 Training loss: 0.2015 Explore P: 0.9940\n",
      "Episode: 13 Total reward: -115.99713134765625 Training loss: 0.4085 Explore P: 0.9935\n",
      "Episode: 14 Total reward: -73.55172729492188 Training loss: 0.3086 Explore P: 0.9931\n",
      "Episode: 15 Total reward: -94.244140625 Training loss: 19.6207 Explore P: 0.9926\n",
      "Model Saved\n",
      "Episode: 16 Total reward: -107.2767333984375 Training loss: 0.5299 Explore P: 0.9922\n",
      "Episode: 17 Total reward: -107.63679504394531 Training loss: 10.2744 Explore P: 0.9918\n",
      "Episode: 18 Total reward: -107.2109375 Training loss: 0.6740 Explore P: 0.9914\n",
      "Episode: 19 Total reward: -98.39518737792969 Training loss: 12.0702 Explore P: 0.9908\n",
      "Episode: 20 Total reward: -89.42356872558594 Training loss: 11.4873 Explore P: 0.9904\n",
      "Model Saved\n",
      "Episode: 21 Total reward: -114.63623046875 Training loss: 21.9962 Explore P: 0.9900\n",
      "Episode: 22 Total reward: -109.24830627441406 Training loss: 9.6482 Explore P: 0.9896\n",
      "Episode: 23 Total reward: -115.96217346191406 Training loss: 21.7313 Explore P: 0.9894\n",
      "Episode: 24 Total reward: -92.05339050292969 Training loss: 0.2091 Explore P: 0.9890\n",
      "Episode: 25 Total reward: -99.68431091308594 Training loss: 0.3271 Explore P: 0.9885\n",
      "Model Saved\n",
      "Episode: 26 Total reward: -98.89706420898438 Training loss: 9.5744 Explore P: 0.9881\n",
      "Episode: 27 Total reward: -106.13626098632812 Training loss: 0.3915 Explore P: 0.9858\n",
      "Episode: 28 Total reward: -115.335205078125 Training loss: 0.4644 Explore P: 0.9854\n",
      "Episode: 29 Total reward: -85.93130493164062 Training loss: 18.6358 Explore P: 0.9850\n",
      "Episode: 30 Total reward: -114.88760375976562 Training loss: 19.6412 Explore P: 0.9846\n",
      "Model Saved\n",
      "Episode: 31 Total reward: -115.9698486328125 Training loss: 10.8518 Explore P: 0.9842\n",
      "Episode: 32 Total reward: -100.81327819824219 Training loss: 0.6184 Explore P: 0.9840\n",
      "Episode: 33 Total reward: -95.47683715820312 Training loss: 0.6127 Explore P: 0.9836\n",
      "Episode: 34 Total reward: -93.0286865234375 Training loss: 9.8933 Explore P: 0.9832\n",
      "Episode: 35 Total reward: -114.51577758789062 Training loss: 10.2305 Explore P: 0.9830\n",
      "Model Saved\n",
      "Episode: 36 Total reward: -115.09147644042969 Training loss: 0.5530 Explore P: 0.9826\n",
      "Episode: 37 Total reward: -95.23295593261719 Training loss: 12.4774 Explore P: 0.9823\n",
      "Episode: 38 Total reward: -104.79112243652344 Training loss: 21.6193 Explore P: 0.9819\n",
      "Episode: 39 Total reward: -72.39750671386719 Training loss: 11.0308 Explore P: 0.9815\n",
      "Episode: 40 Total reward: -96.82302856445312 Training loss: 10.1974 Explore P: 0.9813\n",
      "Model Saved\n",
      "Episode: 41 Total reward: -100.79026794433594 Training loss: 0.9998 Explore P: 0.9809\n",
      "Episode: 42 Total reward: -110.23551940917969 Training loss: 10.7235 Explore P: 0.9805\n",
      "Episode: 43 Total reward: -77.81669616699219 Training loss: 11.5538 Explore P: 0.9796\n",
      "Episode: 44 Total reward: -82.14494323730469 Training loss: 0.8938 Explore P: 0.9792\n",
      "Episode: 45 Total reward: -114.86305236816406 Training loss: 6.0332 Explore P: 0.9786\n",
      "Model Saved\n",
      "Episode: 46 Total reward: -83.661376953125 Training loss: 0.5619 Explore P: 0.9782\n",
      "Episode: 47 Total reward: -95.56024169921875 Training loss: 0.7833 Explore P: 0.9778\n",
      "Episode: 48 Total reward: -108.79264831542969 Training loss: 0.6787 Explore P: 0.9776\n",
      "Episode: 49 Total reward: -113.32891845703125 Training loss: 0.8163 Explore P: 0.9772\n",
      "Episode: 50 Total reward: -113.03094482421875 Training loss: 14.9223 Explore P: 0.9768\n",
      "Model Saved\n",
      "Episode: 51 Total reward: -92.363525390625 Training loss: 0.8575 Explore P: 0.9764\n",
      "Episode: 52 Total reward: -95.68995666503906 Training loss: 6.5772 Explore P: 0.9761\n",
      "Episode: 53 Total reward: -107.54794311523438 Training loss: 12.6924 Explore P: 0.9754\n",
      "Episode: 54 Total reward: -115.97563171386719 Training loss: 4.2014 Explore P: 0.9748\n",
      "Episode: 55 Total reward: -115.97857666015625 Training loss: 9.7736 Explore P: 0.9746\n",
      "Model Saved\n",
      "Episode: 56 Total reward: -99.95100402832031 Training loss: 8.0470 Explore P: 0.9741\n",
      "Episode: 57 Total reward: -110.82896423339844 Training loss: 10.2121 Explore P: 0.9736\n",
      "Episode: 58 Total reward: -108.00325012207031 Training loss: 1.2107 Explore P: 0.9732\n",
      "Episode: 59 Total reward: -106.00682067871094 Training loss: 5.0110 Explore P: 0.9728\n",
      "Episode: 60 Total reward: -101.93260192871094 Training loss: 12.2089 Explore P: 0.9723\n",
      "Model Saved\n",
      "Episode: 61 Total reward: -115.93382263183594 Training loss: 1.4857 Explore P: 0.9719\n",
      "Episode: 62 Total reward: -115.99501037597656 Training loss: 9.6068 Explore P: 0.9712\n",
      "Episode: 63 Total reward: -114.62611389160156 Training loss: 0.8637 Explore P: 0.9708\n",
      "Episode: 64 Total reward: -115.13580322265625 Training loss: 9.2886 Explore P: 0.9704\n",
      "Episode: 65 Total reward: -114.86994934082031 Training loss: 1.1117 Explore P: 0.9698\n",
      "Model Saved\n",
      "Episode: 66 Total reward: -76.81378173828125 Training loss: 11.4564 Explore P: 0.9694\n",
      "Episode: 67 Total reward: -99.15591430664062 Training loss: 10.8959 Explore P: 0.9687\n",
      "Episode: 68 Total reward: -106.5728759765625 Training loss: 0.5316 Explore P: 0.9683\n",
      "Episode: 69 Total reward: -102.51277160644531 Training loss: 38.0323 Explore P: 0.9677\n",
      "Episode: 70 Total reward: -102.81617736816406 Training loss: 7.9368 Explore P: 0.9673\n",
      "Model Saved\n",
      "Episode: 71 Total reward: -114.88137817382812 Training loss: 41.5571 Explore P: 0.9669\n",
      "Episode: 72 Total reward: -115.58102416992188 Training loss: 0.7150 Explore P: 0.9664\n",
      "Episode: 73 Total reward: -86.53691101074219 Training loss: 1.1581 Explore P: 0.9660\n",
      "Episode: 74 Total reward: -107.98562622070312 Training loss: 6.3459 Explore P: 0.9656\n",
      "Episode: 75 Total reward: -102.68775939941406 Training loss: 27.1685 Explore P: 0.9652\n",
      "Model Saved\n",
      "Episode: 76 Total reward: -94.33676147460938 Training loss: 0.8198 Explore P: 0.9648\n",
      "Episode: 77 Total reward: -112.85255432128906 Training loss: 0.7142 Explore P: 0.9645\n",
      "Episode: 78 Total reward: -104.4149169921875 Training loss: 14.7127 Explore P: 0.9641\n",
      "Episode: 79 Total reward: -106.02619934082031 Training loss: 14.6544 Explore P: 0.9637\n",
      "Episode: 80 Total reward: -115.97146606445312 Training loss: 0.2759 Explore P: 0.9633\n",
      "Model Saved\n",
      "Episode: 81 Total reward: -100.45881652832031 Training loss: 0.6338 Explore P: 0.9629\n",
      "Episode: 82 Total reward: -112.62957763671875 Training loss: 12.5444 Explore P: 0.9625\n",
      "Episode: 83 Total reward: -94.55357360839844 Training loss: 12.6667 Explore P: 0.9621\n",
      "Episode: 84 Total reward: -114.18714904785156 Training loss: 1.0739 Explore P: 0.9613\n",
      "Episode: 85 Total reward: -111.77159118652344 Training loss: 38.4370 Explore P: 0.9609\n",
      "Model Saved\n",
      "Episode: 86 Total reward: -102.74819946289062 Training loss: 2.0419 Explore P: 0.9606\n",
      "Episode: 87 Total reward: -101.76461791992188 Training loss: 0.5921 Explore P: 0.9602\n",
      "Episode: 88 Total reward: -79.4859619140625 Training loss: 1.2757 Explore P: 0.9597\n",
      "Episode: 89 Total reward: -105.61732482910156 Training loss: 1.0669 Explore P: 0.9589\n",
      "Episode: 90 Total reward: -99.34510803222656 Training loss: 1.0431 Explore P: 0.9587\n",
      "Model Saved\n",
      "Episode: 91 Total reward: -111.41023254394531 Training loss: 5.1223 Explore P: 0.9576\n",
      "Episode: 92 Total reward: -78.45515441894531 Training loss: 13.6159 Explore P: 0.9572\n",
      "Episode: 93 Total reward: -92.41981506347656 Training loss: 0.7813 Explore P: 0.9568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 94 Total reward: -82.12098693847656 Training loss: 11.7134 Explore P: 0.9564\n",
      "Episode: 95 Total reward: -115.98101806640625 Training loss: 0.4596 Explore P: 0.9560\n",
      "Model Saved\n",
      "Episode: 96 Total reward: -94.08897399902344 Training loss: 0.5619 Explore P: 0.9556\n",
      "Episode: 97 Total reward: -113.15425109863281 Training loss: 0.3889 Explore P: 0.9553\n",
      "Episode: 98 Total reward: -97.69773864746094 Training loss: 12.4555 Explore P: 0.9542\n",
      "Episode: 99 Total reward: -111.57856750488281 Training loss: 2.2992 Explore P: 0.9537\n",
      "Episode: 100 Total reward: -84.38200378417969 Training loss: 0.8334 Explore P: 0.9531\n",
      "Model Saved\n",
      "Episode: 101 Total reward: -89.73367309570312 Training loss: 1.4960 Explore P: 0.9526\n",
      "Episode: 102 Total reward: -89.63766479492188 Training loss: 10.5701 Explore P: 0.9522\n",
      "Episode: 103 Total reward: -112.34382629394531 Training loss: 1.2647 Explore P: 0.9518\n",
      "Episode: 104 Total reward: -93.84669494628906 Training loss: 0.3860 Explore P: 0.9515\n",
      "Episode: 105 Total reward: -99.750732421875 Training loss: 0.4093 Explore P: 0.9511\n",
      "Model Saved\n",
      "Episode: 106 Total reward: -74.39747619628906 Training loss: 13.5951 Explore P: 0.9507\n",
      "Episode: 107 Total reward: -98.666015625 Training loss: 3.0532 Explore P: 0.9502\n",
      "Episode: 108 Total reward: -115.98052978515625 Training loss: 0.2925 Explore P: 0.9499\n",
      "Episode: 109 Total reward: -115.97212219238281 Training loss: 24.7117 Explore P: 0.9495\n",
      "Episode: 110 Total reward: -86.25323486328125 Training loss: 11.9479 Explore P: 0.9491\n",
      "Model Saved\n",
      "Episode: 111 Total reward: -112.17631530761719 Training loss: 2.0232 Explore P: 0.9487\n",
      "Episode: 112 Total reward: -98.031982421875 Training loss: 17.9507 Explore P: 0.9478\n",
      "Episode: 113 Total reward: -111.385986328125 Training loss: 2.8709 Explore P: 0.9475\n",
      "Episode: 114 Total reward: -99.65861511230469 Training loss: 14.9100 Explore P: 0.9469\n",
      "Episode: 115 Total reward: -115.99880981445312 Training loss: 1.9415 Explore P: 0.9465\n",
      "Model Saved\n",
      "Episode: 116 Total reward: -87.62319946289062 Training loss: 1.8165 Explore P: 0.9461\n",
      "Episode: 117 Total reward: -110.70387268066406 Training loss: 9.5110 Explore P: 0.9457\n",
      "Episode: 118 Total reward: -115.29493713378906 Training loss: 1.2031 Explore P: 0.9453\n",
      "Episode: 119 Total reward: -72.17962646484375 Training loss: 15.5760 Explore P: 0.9449\n",
      "Episode: 120 Total reward: -100.50672912597656 Training loss: 17.0372 Explore P: 0.9445\n",
      "Model Saved\n",
      "Episode: 121 Total reward: -83.89590454101562 Training loss: 0.7164 Explore P: 0.9441\n",
      "Episode: 122 Total reward: -69.32264709472656 Training loss: 1.2267 Explore P: 0.9438\n",
      "Episode: 123 Total reward: -99.56591796875 Training loss: 1.2650 Explore P: 0.9434\n",
      "Episode: 124 Total reward: -115.82733154296875 Training loss: 11.1714 Explore P: 0.9430\n",
      "Episode: 125 Total reward: -111.73597717285156 Training loss: 0.8546 Explore P: 0.9426\n",
      "Model Saved\n",
      "Episode: 126 Total reward: -115.96337890625 Training loss: 13.0338 Explore P: 0.9423\n",
      "Episode: 127 Total reward: -115.73173522949219 Training loss: 0.5873 Explore P: 0.9419\n",
      "Episode: 128 Total reward: -94.32487487792969 Training loss: 0.4189 Explore P: 0.9415\n",
      "Episode: 129 Total reward: -97.26556396484375 Training loss: 0.3326 Explore P: 0.9412\n",
      "Episode: 130 Total reward: -82.46722412109375 Training loss: 0.3777 Explore P: 0.9408\n",
      "Model Saved\n",
      "Episode: 131 Total reward: -110.44448852539062 Training loss: 0.3440 Explore P: 0.9404\n",
      "Episode: 132 Total reward: -94.13346862792969 Training loss: 8.5656 Explore P: 0.9402\n",
      "Episode: 133 Total reward: -75.75457763671875 Training loss: 0.7937 Explore P: 0.9398\n",
      "Episode: 134 Total reward: -115.97601318359375 Training loss: 7.8735 Explore P: 0.9396\n",
      "Episode: 135 Total reward: -82.28739929199219 Training loss: 3.4596 Explore P: 0.9392\n",
      "Model Saved\n",
      "Episode: 136 Total reward: -105.62165832519531 Training loss: 0.4570 Explore P: 0.9388\n",
      "Episode: 137 Total reward: -101.8900146484375 Training loss: 14.0544 Explore P: 0.9384\n",
      "Episode: 138 Total reward: -103.61604309082031 Training loss: 17.4404 Explore P: 0.9381\n",
      "Episode: 139 Total reward: -86.56742858886719 Training loss: 13.2378 Explore P: 0.9377\n",
      "Episode: 140 Total reward: -76.50900268554688 Training loss: 8.1817 Explore P: 0.9373\n",
      "Model Saved\n",
      "Episode: 141 Total reward: -63.824493408203125 Training loss: 0.5523 Explore P: 0.9369\n",
      "Episode: 142 Total reward: -83.85398864746094 Training loss: 0.9515 Explore P: 0.9362\n",
      "Episode: 143 Total reward: -104.50816345214844 Training loss: 23.6966 Explore P: 0.9359\n",
      "Episode: 144 Total reward: -110.90155029296875 Training loss: 0.6762 Explore P: 0.9355\n",
      "Episode: 145 Total reward: -109.86375427246094 Training loss: 0.9019 Explore P: 0.9353\n",
      "Model Saved\n",
      "Episode: 146 Total reward: -33.69415283203125 Training loss: 0.3600 Explore P: 0.9347\n",
      "Episode: 147 Total reward: -99.78462219238281 Training loss: 2.2649 Explore P: 0.9343\n",
      "Episode: 148 Total reward: -109.02796936035156 Training loss: 10.8459 Explore P: 0.9335\n",
      "Episode: 149 Total reward: -83.373046875 Training loss: 0.6797 Explore P: 0.9332\n",
      "Episode: 150 Total reward: -104.41117858886719 Training loss: 18.9824 Explore P: 0.9327\n",
      "Model Saved\n",
      "Episode: 151 Total reward: -109.91769409179688 Training loss: 0.9540 Explore P: 0.9323\n",
      "Episode: 152 Total reward: -99.60435485839844 Training loss: 0.7327 Explore P: 0.9319\n",
      "Episode: 153 Total reward: -115.95523071289062 Training loss: 0.7185 Explore P: 0.9315\n",
      "Episode: 154 Total reward: -86.00311279296875 Training loss: 13.8167 Explore P: 0.9311\n",
      "Episode: 155 Total reward: -90.14176940917969 Training loss: 0.4679 Explore P: 0.9308\n",
      "Model Saved\n",
      "Episode: 156 Total reward: -111.35400390625 Training loss: 11.8329 Explore P: 0.9306\n",
      "Episode: 157 Total reward: -115.99716186523438 Training loss: 0.3946 Explore P: 0.9301\n",
      "Episode: 158 Total reward: -102.99113464355469 Training loss: 1.9284 Explore P: 0.9297\n",
      "Episode: 159 Total reward: -90.58474731445312 Training loss: 12.2972 Explore P: 0.9293\n",
      "Episode: 160 Total reward: -115.98072814941406 Training loss: 4.4093 Explore P: 0.9291\n",
      "Model Saved\n",
      "Episode: 161 Total reward: -104.16616821289062 Training loss: 3.4708 Explore P: 0.9287\n",
      "Episode: 162 Total reward: -112.50163269042969 Training loss: 1.0578 Explore P: 0.9283\n",
      "Episode: 163 Total reward: -61.87483215332031 Training loss: 14.6790 Explore P: 0.9280\n",
      "Episode: 164 Total reward: -115.84031677246094 Training loss: 0.7988 Explore P: 0.9278\n",
      "Episode: 165 Total reward: -104.94087219238281 Training loss: 0.4226 Explore P: 0.9275\n",
      "Model Saved\n",
      "Episode: 166 Total reward: -106.59910583496094 Training loss: 1.4662 Explore P: 0.9272\n",
      "Episode: 167 Total reward: -102.9344482421875 Training loss: 18.7488 Explore P: 0.9262\n",
      "Episode: 168 Total reward: -108.73454284667969 Training loss: 12.7216 Explore P: 0.9259\n",
      "Episode: 169 Total reward: -101.02365112304688 Training loss: 14.6317 Explore P: 0.9255\n",
      "Episode: 170 Total reward: -72.019287109375 Training loss: 11.1233 Explore P: 0.9251\n",
      "Model Saved\n",
      "Episode: 171 Total reward: -111.3477783203125 Training loss: 12.5536 Explore P: 0.9248\n",
      "Episode: 172 Total reward: -115.11624145507812 Training loss: 9.8042 Explore P: 0.9246\n",
      "Episode: 173 Total reward: -108.63179016113281 Training loss: 12.8589 Explore P: 0.9242\n",
      "Episode: 174 Total reward: -115.95339965820312 Training loss: 1.8279 Explore P: 0.9238\n",
      "Episode: 175 Total reward: -113.72212219238281 Training loss: 23.4203 Explore P: 0.9236\n",
      "Model Saved\n",
      "Episode: 176 Total reward: -113.41218566894531 Training loss: 8.3141 Explore P: 0.9232\n",
      "Episode: 177 Total reward: -29.235382080078125 Training loss: 0.6281 Explore P: 0.9228\n",
      "Episode: 178 Total reward: -104.05168151855469 Training loss: 13.7045 Explore P: 0.9225\n",
      "Episode: 179 Total reward: -115.09197998046875 Training loss: 1.2538 Explore P: 0.9221\n",
      "Episode: 180 Total reward: -61.24485778808594 Training loss: 2.2287 Explore P: 0.9213\n",
      "Model Saved\n",
      "Episode: 181 Total reward: -115.97203063964844 Training loss: 1.8628 Explore P: 0.9210\n",
      "Episode: 182 Total reward: -90.49185180664062 Training loss: 1.8272 Explore P: 0.9206\n",
      "Episode: 183 Total reward: -114.9652099609375 Training loss: 0.6248 Explore P: 0.9202\n",
      "Episode: 184 Total reward: -113.8118896484375 Training loss: 1.4478 Explore P: 0.9198\n",
      "Episode: 185 Total reward: -98.53117370605469 Training loss: 14.6688 Explore P: 0.9192\n",
      "Model Saved\n",
      "Episode: 186 Total reward: -88.83102416992188 Training loss: 7.2953 Explore P: 0.9188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 187 Total reward: -103.77287292480469 Training loss: 0.4769 Explore P: 0.9183\n",
      "Episode: 188 Total reward: -114.32127380371094 Training loss: 1.3570 Explore P: 0.9179\n",
      "Episode: 189 Total reward: -114.76132202148438 Training loss: 2.4155 Explore P: 0.9176\n",
      "Episode: 190 Total reward: -87.49594116210938 Training loss: 0.9952 Explore P: 0.9172\n",
      "Model Saved\n",
      "Episode: 191 Total reward: -115.83930969238281 Training loss: 10.9802 Explore P: 0.9168\n",
      "Episode: 192 Total reward: -97.80282592773438 Training loss: 0.2595 Explore P: 0.9164\n",
      "Episode: 193 Total reward: -115.73477172851562 Training loss: 0.8558 Explore P: 0.9161\n",
      "Episode: 194 Total reward: -62.76287841796875 Training loss: 0.8721 Explore P: 0.9157\n",
      "Episode: 195 Total reward: -105.42851257324219 Training loss: 14.9905 Explore P: 0.9152\n",
      "Model Saved\n",
      "Episode: 196 Total reward: -104.12919616699219 Training loss: 0.2220 Explore P: 0.9148\n",
      "Episode: 197 Total reward: -40.43463134765625 Training loss: 14.4782 Explore P: 0.9144\n",
      "Episode: 198 Total reward: -97.49726867675781 Training loss: 0.3800 Explore P: 0.9140\n",
      "Episode: 199 Total reward: -76.31080627441406 Training loss: 0.2944 Explore P: 0.9136\n",
      "Episode: 200 Total reward: -115.93009948730469 Training loss: 3.1509 Explore P: 0.9131\n",
      "Model Saved\n",
      "Episode: 201 Total reward: -106.77781677246094 Training loss: 13.7927 Explore P: 0.9127\n",
      "Episode: 202 Total reward: -115.97576904296875 Training loss: 0.2768 Explore P: 0.9125\n",
      "Episode: 203 Total reward: -102.71820068359375 Training loss: 0.2596 Explore P: 0.9122\n",
      "Episode: 204 Total reward: -82.39564514160156 Training loss: 0.8293 Explore P: 0.9118\n",
      "Episode: 205 Total reward: -94.36183166503906 Training loss: 2.6346 Explore P: 0.9115\n",
      "Model Saved\n",
      "Episode: 206 Total reward: -113.46125793457031 Training loss: 2.1385 Explore P: 0.9108\n",
      "Episode: 207 Total reward: -89.05252075195312 Training loss: 2.2726 Explore P: 0.9098\n",
      "Episode: 208 Total reward: -115.54478454589844 Training loss: 0.5594 Explore P: 0.9094\n",
      "Episode: 209 Total reward: -113.88487243652344 Training loss: 1.4068 Explore P: 0.9091\n",
      "Episode: 210 Total reward: -104.34819030761719 Training loss: 13.3378 Explore P: 0.9085\n",
      "Model Saved\n",
      "Episode: 211 Total reward: -53.44740295410156 Training loss: 1.0958 Explore P: 0.9077\n",
      "Episode: 212 Total reward: -106.273681640625 Training loss: 0.7141 Explore P: 0.9073\n",
      "Episode: 213 Total reward: -92.14033508300781 Training loss: 20.5495 Explore P: 0.9069\n",
      "Episode: 214 Total reward: -96.5845947265625 Training loss: 0.6577 Explore P: 0.9066\n",
      "Episode: 215 Total reward: -64.67462158203125 Training loss: 3.4227 Explore P: 0.9062\n",
      "Model Saved\n",
      "Episode: 216 Total reward: -76.49333190917969 Training loss: 1.1927 Explore P: 0.9059\n",
      "Episode: 217 Total reward: -96.05632019042969 Training loss: 0.5234 Explore P: 0.9054\n",
      "Episode: 218 Total reward: -112.96745300292969 Training loss: 14.7794 Explore P: 0.9051\n",
      "Episode: 219 Total reward: -108.20405578613281 Training loss: 0.3634 Explore P: 0.9047\n",
      "Episode: 220 Total reward: -104.45405578613281 Training loss: 4.2866 Explore P: 0.9043\n",
      "Model Saved\n",
      "Episode: 221 Total reward: -51.76304626464844 Training loss: 0.3912 Explore P: 0.9040\n",
      "Episode: 222 Total reward: -111.78135681152344 Training loss: 1.7471 Explore P: 0.9034\n",
      "Episode: 223 Total reward: -75.11491394042969 Training loss: 0.4069 Explore P: 0.9031\n",
      "Episode: 224 Total reward: -91.05014038085938 Training loss: 0.7191 Explore P: 0.9027\n",
      "Episode: 225 Total reward: -110.9586181640625 Training loss: 0.8651 Explore P: 0.9023\n",
      "Model Saved\n",
      "Episode: 226 Total reward: -95.26348876953125 Training loss: 18.2758 Explore P: 0.9018\n",
      "Episode: 227 Total reward: -102.99876403808594 Training loss: 13.1389 Explore P: 0.9014\n",
      "Episode: 228 Total reward: -87.26994323730469 Training loss: 1.6266 Explore P: 0.9010\n",
      "Episode: 229 Total reward: -88.43669128417969 Training loss: 0.3298 Explore P: 0.9006\n",
      "Episode: 230 Total reward: -115.94619750976562 Training loss: 0.5885 Explore P: 0.9004\n",
      "Model Saved\n",
      "Episode: 231 Total reward: -114.68804931640625 Training loss: 0.9205 Explore P: 0.8998\n",
      "Episode: 232 Total reward: -73.14544677734375 Training loss: 5.0497 Explore P: 0.8994\n",
      "Episode: 233 Total reward: -59.439971923828125 Training loss: 8.4680 Explore P: 0.8991\n",
      "Episode: 234 Total reward: -103.46147155761719 Training loss: 0.6403 Explore P: 0.8987\n",
      "Episode: 235 Total reward: -78.809326171875 Training loss: 8.0218 Explore P: 0.8983\n",
      "Model Saved\n",
      "Episode: 236 Total reward: -89.97308349609375 Training loss: 14.7957 Explore P: 0.8977\n",
      "Episode: 237 Total reward: -112.79771423339844 Training loss: 0.3076 Explore P: 0.8974\n",
      "Episode: 238 Total reward: -104.37130737304688 Training loss: 2.6961 Explore P: 0.8969\n",
      "Episode: 239 Total reward: -90.39604187011719 Training loss: 0.6886 Explore P: 0.8964\n",
      "Episode: 240 Total reward: -101.49256896972656 Training loss: 7.4724 Explore P: 0.8960\n",
      "Model Saved\n",
      "Episode: 241 Total reward: -107.8070068359375 Training loss: 0.2542 Explore P: 0.8955\n",
      "Episode: 242 Total reward: -112.31901550292969 Training loss: 6.6860 Explore P: 0.8952\n",
      "Episode: 243 Total reward: -109.75225830078125 Training loss: 2.9747 Explore P: 0.8948\n",
      "Episode: 244 Total reward: -115.99713134765625 Training loss: 13.9272 Explore P: 0.8945\n",
      "Episode: 245 Total reward: -109.3748779296875 Training loss: 1.2017 Explore P: 0.8936\n",
      "Model Saved\n",
      "Episode: 246 Total reward: -113.73284912109375 Training loss: 0.2223 Explore P: 0.8929\n",
      "Episode: 247 Total reward: -74.34918212890625 Training loss: 0.9811 Explore P: 0.8926\n",
      "Episode: 248 Total reward: -115.58651733398438 Training loss: 10.9608 Explore P: 0.8920\n",
      "Episode: 249 Total reward: -110.34393310546875 Training loss: 0.3635 Explore P: 0.8915\n",
      "Episode: 250 Total reward: -68.25550842285156 Training loss: 4.0796 Explore P: 0.8911\n",
      "Model Saved\n",
      "Episode: 251 Total reward: -97.04847717285156 Training loss: 0.3845 Explore P: 0.8907\n",
      "Episode: 252 Total reward: -111.70242309570312 Training loss: 14.0337 Explore P: 0.8902\n",
      "Episode: 253 Total reward: -109.21267700195312 Training loss: 2.4739 Explore P: 0.8899\n",
      "Episode: 254 Total reward: -114.44380187988281 Training loss: 2.5849 Explore P: 0.8896\n",
      "Episode: 255 Total reward: -105.10719299316406 Training loss: 0.1840 Explore P: 0.8892\n",
      "Model Saved\n",
      "Episode: 256 Total reward: -97.55862426757812 Training loss: 2.7203 Explore P: 0.8889\n",
      "Episode: 257 Total reward: -89.51083374023438 Training loss: 5.8665 Explore P: 0.8885\n",
      "Episode: 258 Total reward: -98.47467041015625 Training loss: 1.1811 Explore P: 0.8881\n",
      "Episode: 259 Total reward: -104.42707824707031 Training loss: 1.1812 Explore P: 0.8879\n",
      "Episode: 260 Total reward: -100.77793884277344 Training loss: 1.4639 Explore P: 0.8872\n",
      "Model Saved\n",
      "Episode: 261 Total reward: -115.51020812988281 Training loss: 3.4489 Explore P: 0.8869\n",
      "Episode: 262 Total reward: -112.69680786132812 Training loss: 0.6545 Explore P: 0.8864\n",
      "Episode: 263 Total reward: -113.63856506347656 Training loss: 1.2726 Explore P: 0.8861\n",
      "Episode: 264 Total reward: -74.28715515136719 Training loss: 0.2820 Explore P: 0.8857\n",
      "Episode: 265 Total reward: -111.76983642578125 Training loss: 10.4488 Explore P: 0.8854\n",
      "Model Saved\n",
      "Episode: 266 Total reward: -99.69174194335938 Training loss: 0.3196 Explore P: 0.8850\n",
      "Episode: 267 Total reward: -69.51544189453125 Training loss: 0.3760 Explore P: 0.8846\n",
      "Episode: 268 Total reward: -84.62295532226562 Training loss: 10.5483 Explore P: 0.8843\n",
      "Episode: 269 Total reward: -97.03083801269531 Training loss: 5.9321 Explore P: 0.8839\n",
      "Episode: 270 Total reward: -115.97700500488281 Training loss: 3.0545 Explore P: 0.8836\n",
      "Model Saved\n",
      "Episode: 271 Total reward: -83.39012145996094 Training loss: 2.1460 Explore P: 0.8832\n",
      "Episode: 272 Total reward: -89.54814147949219 Training loss: 0.7170 Explore P: 0.8829\n",
      "Episode: 273 Total reward: -88.58992004394531 Training loss: 1.0716 Explore P: 0.8825\n",
      "Episode: 274 Total reward: -113.87565612792969 Training loss: 0.2277 Explore P: 0.8820\n",
      "Episode: 275 Total reward: -104.83378601074219 Training loss: 15.0998 Explore P: 0.8816\n",
      "Model Saved\n",
      "Episode: 276 Total reward: -95.20259094238281 Training loss: 0.3180 Explore P: 0.8812\n",
      "Episode: 277 Total reward: -90.53182983398438 Training loss: 0.8646 Explore P: 0.8800\n",
      "Episode: 278 Total reward: -107.23600769042969 Training loss: 2.5759 Explore P: 0.8795\n",
      "Episode: 279 Total reward: -112.74128723144531 Training loss: 9.0919 Explore P: 0.8791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 280 Total reward: -66.16658020019531 Training loss: 0.4431 Explore P: 0.8788\n",
      "Model Saved\n",
      "Episode: 281 Total reward: -112.11479187011719 Training loss: 21.0753 Explore P: 0.8784\n",
      "Episode: 282 Total reward: -114.21015930175781 Training loss: 0.1380 Explore P: 0.8782\n",
      "Episode: 283 Total reward: -79.78514099121094 Training loss: 0.7057 Explore P: 0.8779\n",
      "Episode: 284 Total reward: -75.12216186523438 Training loss: 0.2505 Explore P: 0.8775\n",
      "Episode: 285 Total reward: -115.77424621582031 Training loss: 0.1796 Explore P: 0.8771\n",
      "Model Saved\n",
      "Episode: 286 Total reward: -106.8531494140625 Training loss: 0.2215 Explore P: 0.8766\n",
      "Episode: 287 Total reward: -94.25498962402344 Training loss: 10.7350 Explore P: 0.8763\n",
      "Episode: 288 Total reward: -70.81039428710938 Training loss: 0.9406 Explore P: 0.8757\n",
      "Episode: 289 Total reward: -76.46037292480469 Training loss: 14.6402 Explore P: 0.8751\n",
      "Episode: 290 Total reward: -105.51100158691406 Training loss: 0.3838 Explore P: 0.8747\n",
      "Model Saved\n",
      "Episode: 291 Total reward: -115.98043823242188 Training loss: 0.4769 Explore P: 0.8744\n",
      "Episode: 292 Total reward: -76.93728637695312 Training loss: 10.4970 Explore P: 0.8740\n",
      "Episode: 293 Total reward: -100.47560119628906 Training loss: 0.2737 Explore P: 0.8737\n",
      "Episode: 294 Total reward: -33.004364013671875 Training loss: 2.5797 Explore P: 0.8731\n",
      "Episode: 295 Total reward: -100.10400390625 Training loss: 0.3092 Explore P: 0.8727\n",
      "Model Saved\n",
      "Episode: 296 Total reward: -102.09127807617188 Training loss: 0.9955 Explore P: 0.8725\n",
      "Episode: 297 Total reward: -114.01605224609375 Training loss: 2.2118 Explore P: 0.8720\n",
      "Episode: 298 Total reward: -109.47994995117188 Training loss: 0.2723 Explore P: 0.8717\n",
      "Episode: 299 Total reward: -115.89688110351562 Training loss: 0.3975 Explore P: 0.8713\n",
      "Episode: 300 Total reward: -82.2423095703125 Training loss: 0.2710 Explore P: 0.8710\n",
      "Model Saved\n",
      "Episode: 301 Total reward: -90.37713623046875 Training loss: 3.5516 Explore P: 0.8703\n",
      "Episode: 302 Total reward: -109.87319946289062 Training loss: 0.6334 Explore P: 0.8699\n",
      "Episode: 303 Total reward: -91.86756896972656 Training loss: 0.6069 Explore P: 0.8696\n",
      "Episode: 304 Total reward: -114.32485961914062 Training loss: 0.2665 Explore P: 0.8694\n",
      "Episode: 305 Total reward: -111.16049194335938 Training loss: 0.3992 Explore P: 0.8689\n",
      "Model Saved\n",
      "Episode: 306 Total reward: -85.76268005371094 Training loss: 0.5899 Explore P: 0.8687\n",
      "Episode: 307 Total reward: -36.61335754394531 Training loss: 1.7237 Explore P: 0.8684\n",
      "Episode: 308 Total reward: -66.52896118164062 Training loss: 2.3126 Explore P: 0.8680\n",
      "Episode: 309 Total reward: -103.24516296386719 Training loss: 0.8876 Explore P: 0.8677\n",
      "Episode: 310 Total reward: -102.41032409667969 Training loss: 13.9668 Explore P: 0.8674\n",
      "Model Saved\n",
      "Episode: 311 Total reward: -99.24935913085938 Training loss: 0.3929 Explore P: 0.8670\n",
      "Episode: 312 Total reward: -79.66558837890625 Training loss: 0.7470 Explore P: 0.8667\n",
      "Episode: 313 Total reward: -79.02593994140625 Training loss: 0.1361 Explore P: 0.8663\n",
      "Episode: 314 Total reward: -88.5765380859375 Training loss: 21.7744 Explore P: 0.8657\n",
      "Episode: 315 Total reward: -84.89195251464844 Training loss: 1.1035 Explore P: 0.8655\n",
      "Model Saved\n",
      "Episode: 316 Total reward: -106.93759155273438 Training loss: 16.1831 Explore P: 0.8652\n",
      "Episode: 317 Total reward: -90.79513549804688 Training loss: 0.6802 Explore P: 0.8648\n",
      "Episode: 318 Total reward: -85.42477416992188 Training loss: 0.7106 Explore P: 0.8645\n",
      "Episode: 319 Total reward: -113.77243041992188 Training loss: 1.2150 Explore P: 0.8641\n",
      "Episode: 320 Total reward: -115.94821166992188 Training loss: 0.7954 Explore P: 0.8638\n",
      "Model Saved\n",
      "Episode: 321 Total reward: -70.90547180175781 Training loss: 0.4817 Explore P: 0.8634\n",
      "Episode: 322 Total reward: -100.76861572265625 Training loss: 0.2083 Explore P: 0.8629\n",
      "Episode: 323 Total reward: -101.36369323730469 Training loss: 2.1520 Explore P: 0.8627\n",
      "Episode: 324 Total reward: -72.94918823242188 Training loss: 13.2991 Explore P: 0.8624\n",
      "Episode: 325 Total reward: -85.77742004394531 Training loss: 0.4212 Explore P: 0.8620\n",
      "Model Saved\n",
      "Episode: 326 Total reward: -95.03657531738281 Training loss: 2.2110 Explore P: 0.8618\n",
      "Episode: 327 Total reward: -111.24717712402344 Training loss: 0.3568 Explore P: 0.8615\n",
      "Episode: 328 Total reward: -103.4840087890625 Training loss: 0.2417 Explore P: 0.8610\n",
      "Episode: 329 Total reward: -109.18205261230469 Training loss: 3.5563 Explore P: 0.8608\n",
      "Episode: 330 Total reward: -109.52012634277344 Training loss: 0.1931 Explore P: 0.8602\n",
      "Model Saved\n",
      "Episode: 331 Total reward: -89.62074279785156 Training loss: 0.4085 Explore P: 0.8599\n",
      "Episode: 332 Total reward: -78.73097229003906 Training loss: 0.7856 Explore P: 0.8594\n",
      "Episode: 333 Total reward: -108.17338562011719 Training loss: 0.6642 Explore P: 0.8590\n",
      "Episode: 334 Total reward: -115.15937805175781 Training loss: 0.5699 Explore P: 0.8587\n",
      "Episode: 335 Total reward: -66.7537841796875 Training loss: 0.7541 Explore P: 0.8584\n",
      "Model Saved\n",
      "Episode: 336 Total reward: -106.54061889648438 Training loss: 17.4031 Explore P: 0.8580\n",
      "Episode: 337 Total reward: -105.56983947753906 Training loss: 14.2822 Explore P: 0.8577\n",
      "Episode: 338 Total reward: -99.70108032226562 Training loss: 0.2878 Explore P: 0.8573\n",
      "Episode: 339 Total reward: -87.76054382324219 Training loss: 0.2337 Explore P: 0.8571\n",
      "Episode: 340 Total reward: -79.90638732910156 Training loss: 0.6409 Explore P: 0.8567\n",
      "Model Saved\n",
      "Episode: 341 Total reward: -103.29043579101562 Training loss: 0.2178 Explore P: 0.8564\n",
      "Episode: 342 Total reward: -110.84982299804688 Training loss: 0.4535 Explore P: 0.8561\n",
      "Episode: 343 Total reward: -104.2969970703125 Training loss: 0.3821 Explore P: 0.8557\n",
      "Episode: 344 Total reward: -88.27699279785156 Training loss: 0.4187 Explore P: 0.8554\n",
      "Episode: 345 Total reward: -67.03057861328125 Training loss: 0.4165 Explore P: 0.8550\n",
      "Model Saved\n",
      "Episode: 346 Total reward: -95.06301879882812 Training loss: 0.2865 Explore P: 0.8546\n",
      "Episode: 347 Total reward: -90.50105285644531 Training loss: 1.3114 Explore P: 0.8542\n",
      "Episode: 348 Total reward: -81.9744873046875 Training loss: 0.7040 Explore P: 0.8539\n",
      "Episode: 349 Total reward: -77.54684448242188 Training loss: 0.6038 Explore P: 0.8535\n",
      "Episode: 350 Total reward: -115.23046875 Training loss: 5.6172 Explore P: 0.8526\n",
      "Model Saved\n",
      "Episode: 351 Total reward: -115.99856567382812 Training loss: 14.2187 Explore P: 0.8523\n",
      "Episode: 352 Total reward: -114.42547607421875 Training loss: 0.6392 Explore P: 0.8519\n",
      "Episode: 353 Total reward: -93.16879272460938 Training loss: 30.2272 Explore P: 0.8517\n",
      "Episode: 354 Total reward: -84.67245483398438 Training loss: 12.5904 Explore P: 0.8514\n",
      "Episode: 355 Total reward: -81.44718933105469 Training loss: 0.5689 Explore P: 0.8511\n",
      "Model Saved\n",
      "Episode: 356 Total reward: -98.48556518554688 Training loss: 0.3699 Explore P: 0.8509\n",
      "Episode: 357 Total reward: -115.66268920898438 Training loss: 13.5787 Explore P: 0.8505\n",
      "Episode: 358 Total reward: -57.51884460449219 Training loss: 7.1351 Explore P: 0.8502\n",
      "Episode: 359 Total reward: -95.31234741210938 Training loss: 0.2874 Explore P: 0.8500\n",
      "Episode: 360 Total reward: -106.52726745605469 Training loss: 2.5139 Explore P: 0.8497\n",
      "Model Saved\n",
      "Episode: 361 Total reward: -110.95816040039062 Training loss: 0.6953 Explore P: 0.8489\n",
      "Episode: 362 Total reward: -82.60595703125 Training loss: 0.3769 Explore P: 0.8486\n",
      "Episode: 363 Total reward: -70.90245056152344 Training loss: 0.1434 Explore P: 0.8482\n",
      "Episode: 364 Total reward: -89.54803466796875 Training loss: 1.1604 Explore P: 0.8477\n",
      "Episode: 365 Total reward: -111.24169921875 Training loss: 2.4165 Explore P: 0.8477\n",
      "Model Saved\n",
      "Episode: 366 Total reward: -77.89863586425781 Training loss: 1.2157 Explore P: 0.8473\n",
      "Episode: 367 Total reward: -113.56076049804688 Training loss: 0.2628 Explore P: 0.8470\n",
      "Episode: 368 Total reward: -59.01678466796875 Training loss: 0.4648 Explore P: 0.8466\n",
      "Episode: 369 Total reward: -104.68389892578125 Training loss: 0.5413 Explore P: 0.8463\n",
      "Episode: 370 Total reward: -115.99935913085938 Training loss: 0.1850 Explore P: 0.8461\n",
      "Model Saved\n",
      "Episode: 371 Total reward: -64.28372192382812 Training loss: 0.2125 Explore P: 0.8458\n",
      "Episode: 372 Total reward: -115.95341491699219 Training loss: 0.1893 Explore P: 0.8455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 373 Total reward: -83.20372009277344 Training loss: 1.6205 Explore P: 0.8451\n",
      "Episode: 374 Total reward: -70.28773498535156 Training loss: 1.7852 Explore P: 0.8446\n",
      "Episode: 375 Total reward: -95.43223571777344 Training loss: 0.2728 Explore P: 0.8442\n",
      "Model Saved\n",
      "Episode: 376 Total reward: -114.42521667480469 Training loss: 0.3101 Explore P: 0.8440\n",
      "Episode: 377 Total reward: -86.51982116699219 Training loss: 0.3361 Explore P: 0.8435\n",
      "Episode: 378 Total reward: -100.27497863769531 Training loss: 14.2838 Explore P: 0.8431\n",
      "Episode: 379 Total reward: -73.62715148925781 Training loss: 0.3647 Explore P: 0.8428\n",
      "Episode: 380 Total reward: -50.926483154296875 Training loss: 0.3366 Explore P: 0.8423\n",
      "Model Saved\n",
      "Episode: 381 Total reward: -114.95895385742188 Training loss: 15.1790 Explore P: 0.8419\n",
      "Episode: 382 Total reward: -103.85812377929688 Training loss: 16.6373 Explore P: 0.8415\n",
      "Episode: 383 Total reward: -114.78256225585938 Training loss: 0.2259 Explore P: 0.8408\n",
      "Episode: 384 Total reward: -105.69024658203125 Training loss: 0.3136 Explore P: 0.8404\n",
      "Episode: 385 Total reward: -90.58453369140625 Training loss: 0.4807 Explore P: 0.8401\n",
      "Model Saved\n",
      "Episode: 386 Total reward: -79.45362854003906 Training loss: 0.6658 Explore P: 0.8397\n",
      "Episode: 387 Total reward: -82.74201965332031 Training loss: 0.3431 Explore P: 0.8394\n",
      "Episode: 388 Total reward: -80.40031433105469 Training loss: 1.0033 Explore P: 0.8389\n",
      "Episode: 389 Total reward: -105.10676574707031 Training loss: 0.9088 Explore P: 0.8383\n",
      "Episode: 390 Total reward: -87.64894104003906 Training loss: 0.9072 Explore P: 0.8380\n",
      "Model Saved\n",
      "Episode: 391 Total reward: -115.51957702636719 Training loss: 0.1778 Explore P: 0.8376\n",
      "Episode: 392 Total reward: -100.08978271484375 Training loss: 0.4244 Explore P: 0.8368\n",
      "Episode: 393 Total reward: -115.97785949707031 Training loss: 0.2666 Explore P: 0.8365\n",
      "Episode: 394 Total reward: -100.76315307617188 Training loss: 0.1956 Explore P: 0.8361\n",
      "Episode: 395 Total reward: -115.98855590820312 Training loss: 0.9049 Explore P: 0.8357\n",
      "Model Saved\n",
      "Episode: 396 Total reward: -65.44850158691406 Training loss: 0.3288 Explore P: 0.8353\n",
      "Episode: 397 Total reward: -115.99945068359375 Training loss: 0.2705 Explore P: 0.8350\n",
      "Episode: 398 Total reward: -87.4251708984375 Training loss: 0.1865 Explore P: 0.8345\n",
      "Episode: 399 Total reward: -99.95277404785156 Training loss: 0.2611 Explore P: 0.8342\n",
      "Episode: 400 Total reward: -94.74630737304688 Training loss: 1.1882 Explore P: 0.8336\n",
      "Model Saved\n",
      "Episode: 401 Total reward: -86.95518493652344 Training loss: 5.8329 Explore P: 0.8333\n",
      "Episode: 402 Total reward: -114.90298461914062 Training loss: 0.4181 Explore P: 0.8330\n",
      "Episode: 403 Total reward: -84.02023315429688 Training loss: 0.6882 Explore P: 0.8326\n",
      "Episode: 404 Total reward: -68.68821716308594 Training loss: 0.6938 Explore P: 0.8323\n",
      "Episode: 405 Total reward: -22.308059692382812 Training loss: 4.9826 Explore P: 0.8320\n",
      "Model Saved\n",
      "Episode: 406 Total reward: -61.23486328125 Training loss: 0.1618 Explore P: 0.8307\n",
      "Episode: 407 Total reward: -89.83827209472656 Training loss: 13.9041 Explore P: 0.8303\n",
      "Episode: 408 Total reward: -100.42955017089844 Training loss: 0.4276 Explore P: 0.8300\n",
      "Episode: 409 Total reward: -115.67706298828125 Training loss: 1.4035 Explore P: 0.8296\n",
      "Episode: 410 Total reward: -113.85771179199219 Training loss: 0.1519 Explore P: 0.8293\n",
      "Model Saved\n",
      "Episode: 411 Total reward: -107.5892333984375 Training loss: 16.3352 Explore P: 0.8291\n",
      "Episode: 412 Total reward: -79.09211730957031 Training loss: 9.2447 Explore P: 0.8288\n",
      "Episode: 413 Total reward: -76.34519958496094 Training loss: 2.8866 Explore P: 0.8284\n",
      "Episode: 414 Total reward: -64.52850341796875 Training loss: 0.1951 Explore P: 0.8281\n",
      "Episode: 415 Total reward: -108.74636840820312 Training loss: 14.8209 Explore P: 0.8278\n",
      "Model Saved\n",
      "Episode: 416 Total reward: -40.48594665527344 Training loss: 0.3894 Explore P: 0.8272\n",
      "Episode: 417 Total reward: -101.5123291015625 Training loss: 0.3966 Explore P: 0.8270\n",
      "Episode: 418 Total reward: -113.7177734375 Training loss: 4.1387 Explore P: 0.8267\n",
      "Episode: 419 Total reward: -76.88494873046875 Training loss: 0.3320 Explore P: 0.8265\n",
      "Episode: 420 Total reward: -115.1943359375 Training loss: 19.5026 Explore P: 0.8264\n",
      "Model Saved\n",
      "Episode: 421 Total reward: -104.903564453125 Training loss: 0.4250 Explore P: 0.8260\n",
      "Episode: 422 Total reward: -21.856613159179688 Training loss: 1.9091 Explore P: 0.8257\n",
      "Episode: 423 Total reward: -64.47962951660156 Training loss: 0.8870 Explore P: 0.8254\n",
      "Episode: 424 Total reward: -101.17048645019531 Training loss: 0.1952 Explore P: 0.8253\n",
      "Episode: 425 Total reward: -66.72711181640625 Training loss: 0.2789 Explore P: 0.8250\n",
      "Model Saved\n",
      "Episode: 426 Total reward: -102.78819274902344 Training loss: 0.3400 Explore P: 0.8247\n",
      "Episode: 427 Total reward: -106.052001953125 Training loss: 1.3895 Explore P: 0.8243\n",
      "Episode: 428 Total reward: -108.85848999023438 Training loss: 0.6935 Explore P: 0.8242\n",
      "Episode: 429 Total reward: -114.76348876953125 Training loss: 0.1491 Explore P: 0.8239\n",
      "Episode: 430 Total reward: -98.38191223144531 Training loss: 0.8039 Explore P: 0.8237\n",
      "Model Saved\n",
      "Episode: 431 Total reward: -82.29110717773438 Training loss: 9.5147 Explore P: 0.8233\n",
      "Episode: 432 Total reward: -95.09176635742188 Training loss: 0.3294 Explore P: 0.8230\n",
      "Episode: 433 Total reward: -111.31857299804688 Training loss: 0.4157 Explore P: 0.8228\n",
      "Episode: 434 Total reward: -115.20808410644531 Training loss: 2.9465 Explore P: 0.8225\n",
      "Episode: 435 Total reward: -105.6278076171875 Training loss: 1.2257 Explore P: 0.8223\n",
      "Model Saved\n",
      "Episode: 436 Total reward: -71.29994201660156 Training loss: 0.5015 Explore P: 0.8219\n",
      "Episode: 437 Total reward: -29.268295288085938 Training loss: 0.7418 Explore P: 0.8213\n",
      "Episode: 438 Total reward: -100.8636474609375 Training loss: 0.3731 Explore P: 0.8208\n",
      "Episode: 439 Total reward: -84.44989013671875 Training loss: 0.3105 Explore P: 0.8200\n",
      "Episode: 440 Total reward: -58.17149353027344 Training loss: 0.1690 Explore P: 0.8197\n",
      "Model Saved\n",
      "Episode: 441 Total reward: -76.09103393554688 Training loss: 0.4489 Explore P: 0.8194\n",
      "Episode: 442 Total reward: -50.96812438964844 Training loss: 0.3182 Explore P: 0.8190\n",
      "Episode: 443 Total reward: -68.68177795410156 Training loss: 2.9038 Explore P: 0.8187\n",
      "Episode: 444 Total reward: -112.44284057617188 Training loss: 0.1951 Explore P: 0.8184\n",
      "Episode: 445 Total reward: -43.84916687011719 Training loss: 1.7088 Explore P: 0.8180\n",
      "Model Saved\n",
      "Episode: 446 Total reward: -101.59898376464844 Training loss: 0.3409 Explore P: 0.8176\n",
      "Episode: 447 Total reward: -70.77168273925781 Training loss: 0.3410 Explore P: 0.8173\n",
      "Episode: 448 Total reward: -69.69412231445312 Training loss: 0.7969 Explore P: 0.8170\n",
      "Episode: 449 Total reward: -59.01124572753906 Training loss: 13.1944 Explore P: 0.8166\n",
      "Episode: 450 Total reward: -64.9273681640625 Training loss: 0.2080 Explore P: 0.8158\n",
      "Model Saved\n",
      "Episode: 451 Total reward: -73.77253723144531 Training loss: 0.3681 Explore P: 0.8155\n",
      "Episode: 452 Total reward: -100.70710754394531 Training loss: 0.2919 Explore P: 0.8152\n",
      "Episode: 453 Total reward: -104.67103576660156 Training loss: 0.1509 Explore P: 0.8149\n",
      "Episode: 454 Total reward: -104.66372680664062 Training loss: 0.4939 Explore P: 0.8145\n",
      "Episode: 455 Total reward: -85.12516784667969 Training loss: 1.1102 Explore P: 0.8141\n",
      "Model Saved\n",
      "Episode: 456 Total reward: -40.07102966308594 Training loss: 0.7733 Explore P: 0.8136\n",
      "Episode: 457 Total reward: -81.38809204101562 Training loss: 0.2209 Explore P: 0.8133\n",
      "Episode: 458 Total reward: -99.8271484375 Training loss: 0.6435 Explore P: 0.8130\n",
      "Episode: 459 Total reward: -111.69059753417969 Training loss: 0.3875 Explore P: 0.8127\n",
      "Episode: 460 Total reward: -115.27569580078125 Training loss: 0.2152 Explore P: 0.8119\n",
      "Model Saved\n",
      "Episode: 461 Total reward: -64.97366333007812 Training loss: 0.7317 Explore P: 0.8112\n",
      "Episode: 462 Total reward: -55.370758056640625 Training loss: 0.2420 Explore P: 0.8108\n",
      "Episode: 463 Total reward: -74.36515808105469 Training loss: 15.5217 Explore P: 0.8104\n",
      "Episode: 464 Total reward: -78.39718627929688 Training loss: 0.3688 Explore P: 0.8102\n",
      "Episode: 465 Total reward: -97.14891052246094 Training loss: 0.1698 Explore P: 0.8099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Episode: 466 Total reward: -104.73997497558594 Training loss: 1.1153 Explore P: 0.8096\n",
      "Episode: 467 Total reward: -102.33126831054688 Training loss: 5.3575 Explore P: 0.8092\n",
      "Episode: 468 Total reward: -73.35488891601562 Training loss: 0.2021 Explore P: 0.8089\n",
      "Episode: 469 Total reward: -87.968017578125 Training loss: 0.2317 Explore P: 0.8086\n",
      "Episode: 470 Total reward: -84.24415588378906 Training loss: 0.7397 Explore P: 0.8082\n",
      "Model Saved\n",
      "Episode: 471 Total reward: -114.18896484375 Training loss: 0.5833 Explore P: 0.8079\n",
      "Episode: 472 Total reward: -104.57099914550781 Training loss: 1.3057 Explore P: 0.8073\n",
      "Episode: 473 Total reward: -89.37989807128906 Training loss: 0.3035 Explore P: 0.8069\n",
      "Episode: 474 Total reward: -75.82432556152344 Training loss: 9.3056 Explore P: 0.8066\n",
      "Episode: 475 Total reward: -114.98403930664062 Training loss: 0.5117 Explore P: 0.8062\n",
      "Model Saved\n",
      "Episode: 476 Total reward: -73.32766723632812 Training loss: 0.1577 Explore P: 0.8058\n",
      "Episode: 477 Total reward: -87.21597290039062 Training loss: 0.6510 Explore P: 0.8055\n",
      "Episode: 478 Total reward: -85.04586791992188 Training loss: 0.3090 Explore P: 0.8052\n",
      "Episode: 479 Total reward: -74.05694580078125 Training loss: 0.4315 Explore P: 0.8048\n",
      "Episode: 480 Total reward: -42.18751525878906 Training loss: 0.2710 Explore P: 0.8045\n",
      "Model Saved\n",
      "Episode: 481 Total reward: -61.90373229980469 Training loss: 30.4060 Explore P: 0.8042\n",
      "Episode: 482 Total reward: -88.335693359375 Training loss: 0.2631 Explore P: 0.8037\n",
      "Episode: 483 Total reward: -92.86978149414062 Training loss: 1.1797 Explore P: 0.8031\n",
      "Episode: 484 Total reward: -98.51310729980469 Training loss: 0.8785 Explore P: 0.8026\n",
      "Episode: 485 Total reward: -109.72999572753906 Training loss: 0.4831 Explore P: 0.8023\n",
      "Model Saved\n",
      "Episode: 486 Total reward: -76.10501098632812 Training loss: 1.1752 Explore P: 0.8019\n",
      "Episode: 487 Total reward: -75.08978271484375 Training loss: 1.1558 Explore P: 0.8016\n",
      "Episode: 488 Total reward: -88.54295349121094 Training loss: 0.3477 Explore P: 0.8012\n",
      "Episode: 489 Total reward: -95.1824951171875 Training loss: 0.3772 Explore P: 0.8010\n",
      "Episode: 490 Total reward: -109.09834289550781 Training loss: 18.5902 Explore P: 0.8007\n",
      "Model Saved\n",
      "Episode: 491 Total reward: -96.5267333984375 Training loss: 0.3648 Explore P: 0.8002\n",
      "Episode: 492 Total reward: -88.61674499511719 Training loss: 1.9285 Explore P: 0.7997\n",
      "Episode: 493 Total reward: -115.97560119628906 Training loss: 7.9214 Explore P: 0.7992\n",
      "Episode: 494 Total reward: -71.56072998046875 Training loss: 0.4589 Explore P: 0.7989\n",
      "Episode: 495 Total reward: -75.47999572753906 Training loss: 0.3522 Explore P: 0.7986\n",
      "Model Saved\n",
      "Episode: 496 Total reward: -113.86207580566406 Training loss: 1.3388 Explore P: 0.7983\n",
      "Episode: 497 Total reward: -66.56944274902344 Training loss: 0.5162 Explore P: 0.7978\n",
      "Episode: 498 Total reward: -109.33894348144531 Training loss: 2.9835 Explore P: 0.7976\n",
      "Episode: 499 Total reward: -112.33621215820312 Training loss: 0.1421 Explore P: 0.7973\n",
      "Episode: 500 Total reward: -107.56344604492188 Training loss: 0.7219 Explore P: 0.7971\n",
      "Model Saved\n",
      "Episode: 501 Total reward: -90.81907653808594 Training loss: 0.5364 Explore P: 0.7965\n",
      "Episode: 502 Total reward: -74.63957214355469 Training loss: 0.3371 Explore P: 0.7962\n",
      "Episode: 503 Total reward: -105.38200378417969 Training loss: 0.2309 Explore P: 0.7959\n",
      "Episode: 504 Total reward: -92.9500732421875 Training loss: 0.2144 Explore P: 0.7955\n",
      "Episode: 505 Total reward: -68.3287353515625 Training loss: 18.6536 Explore P: 0.7951\n",
      "Model Saved\n",
      "Episode: 506 Total reward: -104.12336730957031 Training loss: 0.5141 Explore P: 0.7945\n",
      "Episode: 507 Total reward: -70.53671264648438 Training loss: 0.4475 Explore P: 0.7940\n",
      "Episode: 508 Total reward: -111.89010620117188 Training loss: 0.2635 Explore P: 0.7937\n",
      "Episode: 509 Total reward: -98.20481872558594 Training loss: 0.7801 Explore P: 0.7933\n",
      "Episode: 510 Total reward: -91.96389770507812 Training loss: 3.3250 Explore P: 0.7930\n",
      "Model Saved\n",
      "Episode: 511 Total reward: -92.40884399414062 Training loss: 0.6046 Explore P: 0.7927\n",
      "Episode: 512 Total reward: -91.90538024902344 Training loss: 1.3174 Explore P: 0.7923\n",
      "Episode: 513 Total reward: -81.2432861328125 Training loss: 0.1386 Explore P: 0.7917\n",
      "Episode: 514 Total reward: -81.99502563476562 Training loss: 0.6601 Explore P: 0.7914\n",
      "Episode: 515 Total reward: -80.52622985839844 Training loss: 1.1589 Explore P: 0.7910\n",
      "Model Saved\n",
      "Episode: 516 Total reward: -80.02568054199219 Training loss: 0.9883 Explore P: 0.7907\n",
      "Episode: 517 Total reward: -108.96177673339844 Training loss: 0.1998 Explore P: 0.7904\n",
      "Episode: 518 Total reward: -86.68785095214844 Training loss: 0.3336 Explore P: 0.7901\n",
      "Episode: 519 Total reward: -50.4656982421875 Training loss: 2.7763 Explore P: 0.7898\n",
      "Episode: 520 Total reward: -89.64268493652344 Training loss: 4.7044 Explore P: 0.7894\n",
      "Model Saved\n",
      "Episode: 521 Total reward: -54.53413391113281 Training loss: 0.1874 Explore P: 0.7888\n",
      "Episode: 522 Total reward: -115.97410583496094 Training loss: 1.3515 Explore P: 0.7885\n",
      "Episode: 523 Total reward: -74.24459838867188 Training loss: 0.1580 Explore P: 0.7882\n",
      "Episode: 524 Total reward: -78.00955200195312 Training loss: 0.6398 Explore P: 0.7878\n",
      "Episode: 525 Total reward: -79.12440490722656 Training loss: 0.1664 Explore P: 0.7875\n",
      "Model Saved\n",
      "Episode: 526 Total reward: -44.76612854003906 Training loss: 0.2776 Explore P: 0.7870\n",
      "Episode: 527 Total reward: -68.71644592285156 Training loss: 4.7185 Explore P: 0.7867\n",
      "Episode: 528 Total reward: -64.39480590820312 Training loss: 0.5615 Explore P: 0.7864\n",
      "Episode: 529 Total reward: -46.67997741699219 Training loss: 0.8773 Explore P: 0.7856\n",
      "Episode: 530 Total reward: -81.13365173339844 Training loss: 0.1931 Explore P: 0.7854\n",
      "Model Saved\n",
      "Episode: 531 Total reward: -86.82852172851562 Training loss: 0.1585 Explore P: 0.7850\n",
      "Episode: 532 Total reward: -92.35601806640625 Training loss: 2.3333 Explore P: 0.7845\n",
      "Episode: 533 Total reward: -105.03269958496094 Training loss: 1.6726 Explore P: 0.7840\n",
      "Episode: 534 Total reward: -95.2064208984375 Training loss: 0.7907 Explore P: 0.7837\n",
      "Episode: 535 Total reward: -98.90644836425781 Training loss: 0.4884 Explore P: 0.7834\n",
      "Model Saved\n",
      "Episode: 536 Total reward: -101.40257263183594 Training loss: 0.3004 Explore P: 0.7831\n",
      "Episode: 537 Total reward: -110.67205810546875 Training loss: 1.3581 Explore P: 0.7828\n",
      "Episode: 538 Total reward: -44.64292907714844 Training loss: 0.3968 Explore P: 0.7825\n",
      "Episode: 539 Total reward: -87.9825439453125 Training loss: 0.1771 Explore P: 0.7822\n",
      "Episode: 540 Total reward: -82.32339477539062 Training loss: 0.2573 Explore P: 0.7818\n",
      "Model Saved\n",
      "Episode: 541 Total reward: -54.64213562011719 Training loss: 0.3407 Explore P: 0.7815\n",
      "Episode: 542 Total reward: -94.13320922851562 Training loss: 2.4654 Explore P: 0.7812\n",
      "Episode: 543 Total reward: -87.15846252441406 Training loss: 1.4348 Explore P: 0.7808\n",
      "Episode: 544 Total reward: -107.120849609375 Training loss: 0.5372 Explore P: 0.7805\n",
      "Episode: 545 Total reward: -110.75987243652344 Training loss: 0.3966 Explore P: 0.7801\n",
      "Model Saved\n",
      "Episode: 546 Total reward: -115.47186279296875 Training loss: 0.8469 Explore P: 0.7797\n",
      "Episode: 547 Total reward: -72.548828125 Training loss: 0.3376 Explore P: 0.7793\n",
      "Episode: 548 Total reward: -56.12982177734375 Training loss: 0.8147 Explore P: 0.7789\n",
      "Episode: 549 Total reward: -66.62132263183594 Training loss: 0.1662 Explore P: 0.7786\n",
      "Episode: 550 Total reward: -102.46218872070312 Training loss: 9.6507 Explore P: 0.7782\n",
      "Model Saved\n",
      "Episode: 551 Total reward: -97.58351135253906 Training loss: 0.1378 Explore P: 0.7779\n",
      "Episode: 552 Total reward: -95.30081176757812 Training loss: 0.3483 Explore P: 0.7776\n",
      "Episode: 553 Total reward: -38.40168762207031 Training loss: 0.1882 Explore P: 0.7769\n",
      "Episode: 554 Total reward: -94.64028930664062 Training loss: 0.7620 Explore P: 0.7766\n",
      "Episode: 555 Total reward: -80.2254638671875 Training loss: 0.7542 Explore P: 0.7763\n",
      "Model Saved\n",
      "Episode: 556 Total reward: -29.087417602539062 Training loss: 0.6169 Explore P: 0.7756\n",
      "Episode: 557 Total reward: -84.30340576171875 Training loss: 2.1047 Explore P: 0.7753\n",
      "Episode: 558 Total reward: -72.39942932128906 Training loss: 0.2668 Explore P: 0.7749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 559 Total reward: -110.93797302246094 Training loss: 0.7693 Explore P: 0.7746\n",
      "Episode: 560 Total reward: -80.13720703125 Training loss: 0.2032 Explore P: 0.7743\n",
      "Model Saved\n",
      "Episode: 561 Total reward: -85.03221130371094 Training loss: 0.2777 Explore P: 0.7740\n",
      "Episode: 562 Total reward: -65.24063110351562 Training loss: 1.9034 Explore P: 0.7736\n",
      "Episode: 563 Total reward: -102.83773803710938 Training loss: 0.3091 Explore P: 0.7733\n",
      "Episode: 564 Total reward: -69.25889587402344 Training loss: 1.0537 Explore P: 0.7729\n",
      "Episode: 565 Total reward: -87.15437316894531 Training loss: 0.3314 Explore P: 0.7723\n",
      "Model Saved\n",
      "Episode: 566 Total reward: 9.619552612304688 Training loss: 0.2638 Explore P: 0.7715\n",
      "Episode: 567 Total reward: -64.29466247558594 Training loss: 15.8557 Explore P: 0.7711\n",
      "Episode: 568 Total reward: -64.50265502929688 Training loss: 0.2238 Explore P: 0.7708\n",
      "Episode: 569 Total reward: -22.763015747070312 Training loss: 0.9759 Explore P: 0.7705\n",
      "Episode: 570 Total reward: -94.69612121582031 Training loss: 2.3373 Explore P: 0.7702\n",
      "Model Saved\n",
      "Episode: 571 Total reward: -82.38491821289062 Training loss: 0.8334 Explore P: 0.7699\n",
      "Episode: 572 Total reward: -91.60177612304688 Training loss: 16.3427 Explore P: 0.7694\n",
      "Episode: 573 Total reward: -68.49383544921875 Training loss: 1.4345 Explore P: 0.7691\n",
      "Episode: 574 Total reward: -53.02375793457031 Training loss: 0.5756 Explore P: 0.7687\n",
      "Episode: 575 Total reward: -71.46363830566406 Training loss: 1.1150 Explore P: 0.7683\n",
      "Model Saved\n",
      "Episode: 576 Total reward: -90.89442443847656 Training loss: 0.3056 Explore P: 0.7681\n",
      "Episode: 577 Total reward: -99.25640869140625 Training loss: 0.1664 Explore P: 0.7678\n",
      "Episode: 578 Total reward: -36.719512939453125 Training loss: 0.7294 Explore P: 0.7673\n",
      "Episode: 579 Total reward: -115.31156921386719 Training loss: 1.0506 Explore P: 0.7671\n",
      "Episode: 580 Total reward: -103.34188842773438 Training loss: 3.4135 Explore P: 0.7668\n",
      "Model Saved\n",
      "Episode: 581 Total reward: -54.9412841796875 Training loss: 10.9849 Explore P: 0.7665\n",
      "Episode: 582 Total reward: -91.08845520019531 Training loss: 0.1802 Explore P: 0.7662\n",
      "Episode: 583 Total reward: -57.56126403808594 Training loss: 0.4395 Explore P: 0.7659\n",
      "Episode: 584 Total reward: -79.34600830078125 Training loss: 0.1695 Explore P: 0.7657\n",
      "Episode: 585 Total reward: -75.56362915039062 Training loss: 0.4275 Explore P: 0.7654\n",
      "Model Saved\n",
      "Episode: 586 Total reward: 12.174530029296875 Training loss: 0.3039 Explore P: 0.7645\n",
      "Episode: 587 Total reward: -96.43876647949219 Training loss: 0.1630 Explore P: 0.7643\n",
      "Episode: 588 Total reward: -55.938140869140625 Training loss: 0.6347 Explore P: 0.7640\n",
      "Episode: 589 Total reward: -76.96212768554688 Training loss: 0.2156 Explore P: 0.7636\n",
      "Episode: 590 Total reward: -89.82772827148438 Training loss: 0.1885 Explore P: 0.7632\n",
      "Model Saved\n",
      "Episode: 591 Total reward: -95.87672424316406 Training loss: 1.1469 Explore P: 0.7628\n",
      "Episode: 592 Total reward: -97.40168762207031 Training loss: 14.6060 Explore P: 0.7624\n",
      "Episode: 593 Total reward: -48.858154296875 Training loss: 0.2524 Explore P: 0.7621\n",
      "Episode: 594 Total reward: -98.322998046875 Training loss: 13.3922 Explore P: 0.7619\n",
      "Episode: 595 Total reward: -78.25740051269531 Training loss: 0.3225 Explore P: 0.7617\n",
      "Model Saved\n",
      "Episode: 596 Total reward: -60.87837219238281 Training loss: 0.5727 Explore P: 0.7613\n",
      "Episode: 597 Total reward: -101.15629577636719 Training loss: 0.2060 Explore P: 0.7608\n",
      "Episode: 598 Total reward: -59.264007568359375 Training loss: 0.6054 Explore P: 0.7604\n",
      "Episode: 599 Total reward: -66.7276611328125 Training loss: 1.3875 Explore P: 0.7601\n",
      "Episode: 600 Total reward: -113.45089721679688 Training loss: 1.0344 Explore P: 0.7598\n",
      "Model Saved\n",
      "Episode: 601 Total reward: -81.84848022460938 Training loss: 0.4739 Explore P: 0.7595\n",
      "Episode: 602 Total reward: -112.74305725097656 Training loss: 2.4533 Explore P: 0.7592\n",
      "Episode: 603 Total reward: -80.45396423339844 Training loss: 7.0725 Explore P: 0.7586\n",
      "Episode: 604 Total reward: -89.25346374511719 Training loss: 0.2859 Explore P: 0.7583\n",
      "Episode: 605 Total reward: -30.581451416015625 Training loss: 1.4147 Explore P: 0.7580\n",
      "Model Saved\n",
      "Episode: 606 Total reward: -66.83175659179688 Training loss: 0.8808 Explore P: 0.7574\n",
      "Episode: 607 Total reward: -24.4725341796875 Training loss: 4.0671 Explore P: 0.7570\n",
      "Episode: 608 Total reward: -105.64059448242188 Training loss: 1.7803 Explore P: 0.7567\n",
      "Episode: 609 Total reward: -107.79228210449219 Training loss: 5.2417 Explore P: 0.7562\n",
      "Episode: 610 Total reward: -74.35076904296875 Training loss: 0.4136 Explore P: 0.7559\n",
      "Model Saved\n",
      "Episode: 611 Total reward: -46.10441589355469 Training loss: 0.1847 Explore P: 0.7554\n",
      "Episode: 612 Total reward: -100.1083984375 Training loss: 0.2949 Explore P: 0.7553\n",
      "Episode: 613 Total reward: -95.45448303222656 Training loss: 4.0960 Explore P: 0.7550\n",
      "Episode: 614 Total reward: -32.21800231933594 Training loss: 0.5025 Explore P: 0.7547\n",
      "Episode: 615 Total reward: -71.83230590820312 Training loss: 12.4637 Explore P: 0.7544\n",
      "Model Saved\n",
      "Episode: 616 Total reward: -74.04206848144531 Training loss: 0.1962 Explore P: 0.7541\n",
      "Episode: 617 Total reward: -6.836181640625 Training loss: 0.2594 Explore P: 0.7538\n",
      "Episode: 618 Total reward: -54.25224304199219 Training loss: 0.8133 Explore P: 0.7535\n",
      "Episode: 619 Total reward: -82.63594055175781 Training loss: 12.5381 Explore P: 0.7532\n",
      "Episode: 620 Total reward: -91.41572570800781 Training loss: 0.1394 Explore P: 0.7529\n",
      "Model Saved\n",
      "Episode: 621 Total reward: -115.41502380371094 Training loss: 0.2163 Explore P: 0.7527\n",
      "Episode: 622 Total reward: -52.61785888671875 Training loss: 0.3842 Explore P: 0.7523\n",
      "Episode: 623 Total reward: -46.12493896484375 Training loss: 0.5421 Explore P: 0.7520\n",
      "Episode: 624 Total reward: -115.75299072265625 Training loss: 0.7040 Explore P: 0.7514\n",
      "Episode: 625 Total reward: -92.47126770019531 Training loss: 0.3253 Explore P: 0.7512\n",
      "Model Saved\n",
      "Episode: 626 Total reward: -86.61209106445312 Training loss: 0.5009 Explore P: 0.7509\n",
      "Episode: 627 Total reward: -72.49748229980469 Training loss: 0.2090 Explore P: 0.7504\n",
      "Episode: 628 Total reward: -105.4937744140625 Training loss: 0.1619 Explore P: 0.7503\n",
      "Episode: 629 Total reward: -76.42408752441406 Training loss: 0.2444 Explore P: 0.7500\n",
      "Episode: 630 Total reward: -98.31214904785156 Training loss: 2.6196 Explore P: 0.7496\n",
      "Model Saved\n",
      "Episode: 631 Total reward: -70.24459838867188 Training loss: 0.3877 Explore P: 0.7493\n",
      "Episode: 632 Total reward: -99.52345275878906 Training loss: 0.2038 Explore P: 0.7490\n",
      "Episode: 633 Total reward: -115.6748046875 Training loss: 0.3340 Explore P: 0.7487\n",
      "Episode: 634 Total reward: -87.04058837890625 Training loss: 0.4207 Explore P: 0.7484\n",
      "Episode: 635 Total reward: -65.6356201171875 Training loss: 0.1617 Explore P: 0.7481\n",
      "Model Saved\n",
      "Episode: 636 Total reward: -82.15032958984375 Training loss: 0.1445 Explore P: 0.7478\n",
      "Episode: 637 Total reward: -95.59329223632812 Training loss: 5.9025 Explore P: 0.7474\n",
      "Episode: 638 Total reward: -115.98223876953125 Training loss: 0.2881 Explore P: 0.7471\n",
      "Episode: 639 Total reward: -101.81361389160156 Training loss: 0.3492 Explore P: 0.7469\n",
      "Episode: 640 Total reward: -110.87461853027344 Training loss: 0.4683 Explore P: 0.7463\n",
      "Model Saved\n",
      "Episode: 641 Total reward: -115.8271484375 Training loss: 0.2364 Explore P: 0.7461\n",
      "Episode: 642 Total reward: -69.89231872558594 Training loss: 0.1786 Explore P: 0.7458\n",
      "Episode: 643 Total reward: -102.66484069824219 Training loss: 0.2126 Explore P: 0.7455\n",
      "Episode: 644 Total reward: -75.37554931640625 Training loss: 0.2607 Explore P: 0.7452\n",
      "Episode: 645 Total reward: -34.04437255859375 Training loss: 0.1919 Explore P: 0.7449\n",
      "Model Saved\n",
      "Episode: 646 Total reward: -98.44630432128906 Training loss: 0.2726 Explore P: 0.7445\n",
      "Episode: 647 Total reward: -111.16139221191406 Training loss: 0.2423 Explore P: 0.7441\n",
      "Episode: 648 Total reward: -90.62301635742188 Training loss: 0.3438 Explore P: 0.7439\n",
      "Episode: 649 Total reward: -16.107955932617188 Training loss: 0.4168 Explore P: 0.7434\n",
      "Episode: 650 Total reward: -76.56663513183594 Training loss: 2.7741 Explore P: 0.7431\n",
      "Model Saved\n",
      "Episode: 651 Total reward: -46.73826599121094 Training loss: 0.5220 Explore P: 0.7428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 652 Total reward: -58.33209228515625 Training loss: 0.2402 Explore P: 0.7425\n",
      "Episode: 653 Total reward: -87.84193420410156 Training loss: 12.6414 Explore P: 0.7422\n",
      "Episode: 654 Total reward: -115.12496948242188 Training loss: 5.1549 Explore P: 0.7418\n",
      "Episode: 655 Total reward: -79.83497619628906 Training loss: 0.7295 Explore P: 0.7414\n",
      "Model Saved\n",
      "Episode: 656 Total reward: -42.673126220703125 Training loss: 9.6777 Explore P: 0.7411\n",
      "Episode: 657 Total reward: -80.4168701171875 Training loss: 0.5837 Explore P: 0.7407\n",
      "Episode: 658 Total reward: -92.43092346191406 Training loss: 11.1675 Explore P: 0.7402\n",
      "Episode: 659 Total reward: -106.82963562011719 Training loss: 14.2496 Explore P: 0.7401\n",
      "Episode: 660 Total reward: -101.1947021484375 Training loss: 0.2416 Explore P: 0.7399\n",
      "Model Saved\n",
      "Episode: 661 Total reward: -20.066726684570312 Training loss: 0.2575 Explore P: 0.7396\n",
      "Episode: 662 Total reward: -102.18011474609375 Training loss: 0.1720 Explore P: 0.7394\n",
      "Episode: 663 Total reward: -46.25482177734375 Training loss: 0.2118 Explore P: 0.7391\n",
      "Episode: 664 Total reward: -102.9739990234375 Training loss: 4.7340 Explore P: 0.7388\n",
      "Episode: 665 Total reward: -49.72264099121094 Training loss: 12.0049 Explore P: 0.7385\n",
      "Model Saved\n",
      "Episode: 666 Total reward: -113.67538452148438 Training loss: 0.2028 Explore P: 0.7383\n",
      "Episode: 667 Total reward: -85.74946594238281 Training loss: 0.2703 Explore P: 0.7380\n",
      "Episode: 668 Total reward: -105.81382751464844 Training loss: 0.5959 Explore P: 0.7378\n",
      "Episode: 669 Total reward: -33.940216064453125 Training loss: 0.2019 Explore P: 0.7375\n",
      "Episode: 670 Total reward: -67.78987121582031 Training loss: 0.7577 Explore P: 0.7372\n",
      "Model Saved\n",
      "Episode: 671 Total reward: -112.02297973632812 Training loss: 0.6340 Explore P: 0.7370\n",
      "Episode: 672 Total reward: -97.091552734375 Training loss: 0.8582 Explore P: 0.7367\n",
      "Episode: 673 Total reward: -63.55247497558594 Training loss: 0.6240 Explore P: 0.7364\n",
      "Episode: 674 Total reward: -64.88508605957031 Training loss: 0.4044 Explore P: 0.7361\n",
      "Episode: 675 Total reward: -110.26287841796875 Training loss: 0.9880 Explore P: 0.7360\n",
      "Model Saved\n",
      "Episode: 676 Total reward: -70.35855102539062 Training loss: 0.1931 Explore P: 0.7357\n",
      "Episode: 677 Total reward: -45.49415588378906 Training loss: 0.2941 Explore P: 0.7354\n",
      "Episode: 678 Total reward: -60.05397033691406 Training loss: 1.0857 Explore P: 0.7351\n",
      "Episode: 679 Total reward: -76.59317016601562 Training loss: 0.6030 Explore P: 0.7346\n",
      "Episode: 680 Total reward: -77.5611572265625 Training loss: 0.1855 Explore P: 0.7343\n",
      "Model Saved\n",
      "Episode: 681 Total reward: -55.80511474609375 Training loss: 0.9360 Explore P: 0.7339\n",
      "Episode: 682 Total reward: -61.99964904785156 Training loss: 2.1038 Explore P: 0.7336\n",
      "Episode: 683 Total reward: -85.24017333984375 Training loss: 0.3814 Explore P: 0.7333\n",
      "Episode: 684 Total reward: -79.75860595703125 Training loss: 0.3404 Explore P: 0.7330\n",
      "Episode: 685 Total reward: -71.07931518554688 Training loss: 2.1600 Explore P: 0.7327\n",
      "Model Saved\n",
      "Episode: 686 Total reward: -95.21536254882812 Training loss: 0.5547 Explore P: 0.7324\n",
      "Episode: 687 Total reward: -55.85704040527344 Training loss: 0.3891 Explore P: 0.7321\n",
      "Episode: 688 Total reward: -78.09263610839844 Training loss: 0.5754 Explore P: 0.7318\n",
      "Episode: 689 Total reward: -115.148681640625 Training loss: 0.5186 Explore P: 0.7315\n",
      "Episode: 690 Total reward: -72.1669921875 Training loss: 1.6443 Explore P: 0.7312\n",
      "Model Saved\n",
      "Episode: 691 Total reward: -81.51524353027344 Training loss: 0.3496 Explore P: 0.7309\n",
      "Episode: 692 Total reward: -45.79927062988281 Training loss: 0.2119 Explore P: 0.7306\n",
      "Episode: 693 Total reward: -66.14959716796875 Training loss: 0.4776 Explore P: 0.7303\n",
      "Episode: 694 Total reward: -33.92292785644531 Training loss: 4.1307 Explore P: 0.7300\n",
      "Episode: 695 Total reward: -86.15605163574219 Training loss: 0.1632 Explore P: 0.7298\n",
      "Model Saved\n",
      "Episode: 696 Total reward: -97.32484436035156 Training loss: 1.3818 Explore P: 0.7294\n",
      "Episode: 697 Total reward: -78.85638427734375 Training loss: 1.9967 Explore P: 0.7292\n",
      "Episode: 698 Total reward: -69.75389099121094 Training loss: 12.0573 Explore P: 0.7290\n",
      "Episode: 699 Total reward: -104.59658813476562 Training loss: 2.6017 Explore P: 0.7288\n",
      "Episode: 700 Total reward: -79.16253662109375 Training loss: 0.3079 Explore P: 0.7285\n",
      "Model Saved\n",
      "Episode: 701 Total reward: -69.02243041992188 Training loss: 0.2097 Explore P: 0.7282\n",
      "Episode: 702 Total reward: -31.633560180664062 Training loss: 1.2474 Explore P: 0.7279\n",
      "Episode: 703 Total reward: -89.59481811523438 Training loss: 6.6708 Explore P: 0.7276\n",
      "Episode: 704 Total reward: -80.71952819824219 Training loss: 0.9275 Explore P: 0.7272\n",
      "Episode: 705 Total reward: -78.76319885253906 Training loss: 0.3933 Explore P: 0.7270\n",
      "Model Saved\n",
      "Episode: 706 Total reward: -108.04327392578125 Training loss: 0.7383 Explore P: 0.7267\n",
      "Episode: 707 Total reward: -68.17230224609375 Training loss: 0.7692 Explore P: 0.7264\n",
      "Episode: 708 Total reward: -103.14448547363281 Training loss: 0.9407 Explore P: 0.7261\n",
      "Episode: 709 Total reward: -83.05265808105469 Training loss: 0.1671 Explore P: 0.7259\n",
      "Episode: 710 Total reward: -77.43930053710938 Training loss: 11.3634 Explore P: 0.7256\n",
      "Model Saved\n",
      "Episode: 711 Total reward: -73.01516723632812 Training loss: 0.3333 Explore P: 0.7253\n",
      "Episode: 712 Total reward: -115.73867797851562 Training loss: 0.2474 Explore P: 0.7253\n",
      "Episode: 713 Total reward: -86.810302734375 Training loss: 0.4041 Explore P: 0.7247\n",
      "Episode: 714 Total reward: -57.73103332519531 Training loss: 1.3516 Explore P: 0.7243\n",
      "Episode: 715 Total reward: -115.96485900878906 Training loss: 0.4996 Explore P: 0.7238\n",
      "Model Saved\n",
      "Episode: 716 Total reward: -84.87059020996094 Training loss: 0.9434 Explore P: 0.7235\n",
      "Episode: 717 Total reward: -10.125946044921875 Training loss: 0.2660 Explore P: 0.7232\n",
      "Episode: 718 Total reward: -18.319107055664062 Training loss: 0.1900 Explore P: 0.7226\n",
      "Episode: 719 Total reward: -92.89654541015625 Training loss: 19.6740 Explore P: 0.7223\n",
      "Episode: 720 Total reward: -88.23532104492188 Training loss: 0.2898 Explore P: 0.7220\n",
      "Model Saved\n",
      "Episode: 721 Total reward: -66.80674743652344 Training loss: 0.9501 Explore P: 0.7217\n",
      "Episode: 722 Total reward: -51.7464599609375 Training loss: 0.3242 Explore P: 0.7214\n",
      "Episode: 723 Total reward: -74.78611755371094 Training loss: 12.4492 Explore P: 0.7211\n",
      "Episode: 724 Total reward: -98.01274108886719 Training loss: 0.2939 Explore P: 0.7209\n",
      "Episode: 725 Total reward: -80.08900451660156 Training loss: 5.9504 Explore P: 0.7206\n",
      "Model Saved\n",
      "Episode: 726 Total reward: -93.71055603027344 Training loss: 0.7650 Explore P: 0.7204\n",
      "Episode: 727 Total reward: -73.53358459472656 Training loss: 0.7272 Explore P: 0.7201\n",
      "Episode: 728 Total reward: -75.24456787109375 Training loss: 0.3077 Explore P: 0.7198\n",
      "Episode: 729 Total reward: -69.76004028320312 Training loss: 1.9368 Explore P: 0.7195\n",
      "Episode: 730 Total reward: -32.35308837890625 Training loss: 1.5590 Explore P: 0.7191\n",
      "Model Saved\n",
      "Episode: 731 Total reward: -64.91748046875 Training loss: 0.5326 Explore P: 0.7188\n",
      "Episode: 732 Total reward: -15.536209106445312 Training loss: 0.1618 Explore P: 0.7185\n",
      "Episode: 733 Total reward: -69.33624267578125 Training loss: 14.2767 Explore P: 0.7182\n",
      "Episode: 734 Total reward: -102.1573486328125 Training loss: 0.4138 Explore P: 0.7180\n",
      "Episode: 735 Total reward: -81.89459228515625 Training loss: 0.3714 Explore P: 0.7178\n",
      "Model Saved\n",
      "Episode: 736 Total reward: -76.86297607421875 Training loss: 12.0205 Explore P: 0.7175\n",
      "Episode: 737 Total reward: -70.24833679199219 Training loss: 2.9845 Explore P: 0.7172\n",
      "Episode: 738 Total reward: -113.85733032226562 Training loss: 1.3261 Explore P: 0.7170\n",
      "Episode: 739 Total reward: -64.58860778808594 Training loss: 10.5966 Explore P: 0.7166\n",
      "Episode: 740 Total reward: -65.90681457519531 Training loss: 3.4530 Explore P: 0.7163\n",
      "Model Saved\n",
      "Episode: 741 Total reward: -80.5130615234375 Training loss: 12.1624 Explore P: 0.7160\n",
      "Episode: 742 Total reward: -93.58615112304688 Training loss: 0.2596 Explore P: 0.7159\n",
      "Episode: 743 Total reward: -98.83767700195312 Training loss: 2.1932 Explore P: 0.7156\n",
      "Episode: 744 Total reward: -101.45132446289062 Training loss: 0.9148 Explore P: 0.7152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 745 Total reward: -115.89054870605469 Training loss: 13.9441 Explore P: 0.7148\n",
      "Model Saved\n",
      "Episode: 746 Total reward: -95.88581848144531 Training loss: 17.5176 Explore P: 0.7145\n",
      "Episode: 747 Total reward: -54.07597351074219 Training loss: 0.5659 Explore P: 0.7136\n",
      "Episode: 748 Total reward: -94.544921875 Training loss: 15.1746 Explore P: 0.7133\n",
      "Episode: 749 Total reward: -106.07919311523438 Training loss: 1.3968 Explore P: 0.7130\n",
      "Episode: 750 Total reward: -103.4560546875 Training loss: 0.2607 Explore P: 0.7129\n",
      "Model Saved\n",
      "Episode: 751 Total reward: -64.7669677734375 Training loss: 0.4064 Explore P: 0.7125\n",
      "Episode: 752 Total reward: -38.39347839355469 Training loss: 0.1737 Explore P: 0.7121\n",
      "Episode: 753 Total reward: -61.87028503417969 Training loss: 1.5442 Explore P: 0.7117\n",
      "Episode: 754 Total reward: -54.92510986328125 Training loss: 0.3489 Explore P: 0.7114\n",
      "Episode: 755 Total reward: -34.69401550292969 Training loss: 0.6586 Explore P: 0.7106\n",
      "Model Saved\n",
      "Episode: 756 Total reward: -67.19442749023438 Training loss: 0.2719 Explore P: 0.7103\n",
      "Episode: 757 Total reward: -92.79048156738281 Training loss: 11.2438 Explore P: 0.7100\n",
      "Episode: 758 Total reward: -91.04597473144531 Training loss: 2.7520 Explore P: 0.7097\n",
      "Episode: 759 Total reward: -60.17723083496094 Training loss: 0.2826 Explore P: 0.7094\n",
      "Episode: 760 Total reward: -68.74357604980469 Training loss: 0.6175 Explore P: 0.7091\n",
      "Model Saved\n",
      "Episode: 761 Total reward: -76.72320556640625 Training loss: 0.7823 Explore P: 0.7088\n",
      "Episode: 762 Total reward: -88.71015930175781 Training loss: 0.3419 Explore P: 0.7084\n",
      "Episode: 763 Total reward: -103.29307556152344 Training loss: 11.9284 Explore P: 0.7083\n",
      "Episode: 764 Total reward: -93.04165649414062 Training loss: 1.2293 Explore P: 0.7080\n",
      "Episode: 765 Total reward: -112.2340087890625 Training loss: 0.2898 Explore P: 0.7075\n",
      "Model Saved\n",
      "Episode: 766 Total reward: -115.79414367675781 Training loss: 0.3844 Explore P: 0.7072\n",
      "Episode: 767 Total reward: -73.02238464355469 Training loss: 1.1207 Explore P: 0.7069\n",
      "Episode: 768 Total reward: -75.43197631835938 Training loss: 12.9414 Explore P: 0.7067\n",
      "Episode: 769 Total reward: -69.6195068359375 Training loss: 0.7209 Explore P: 0.7064\n",
      "Episode: 770 Total reward: -91.78866577148438 Training loss: 0.5933 Explore P: 0.7062\n",
      "Model Saved\n",
      "Episode: 771 Total reward: -110.53646850585938 Training loss: 0.4286 Explore P: 0.7061\n",
      "Episode: 772 Total reward: -80.31788635253906 Training loss: 16.4556 Explore P: 0.7058\n",
      "Episode: 773 Total reward: -63.985870361328125 Training loss: 0.3055 Explore P: 0.7053\n",
      "Episode: 774 Total reward: -115.94032287597656 Training loss: 0.2606 Explore P: 0.7050\n",
      "Episode: 775 Total reward: -97.68008422851562 Training loss: 0.3716 Explore P: 0.7046\n",
      "Model Saved\n",
      "Episode: 776 Total reward: -95.85877990722656 Training loss: 1.0720 Explore P: 0.7042\n",
      "Episode: 777 Total reward: -57.75709533691406 Training loss: 0.4270 Explore P: 0.7039\n",
      "Episode: 778 Total reward: -109.44810485839844 Training loss: 0.4882 Explore P: 0.7037\n",
      "Episode: 779 Total reward: -78.504638671875 Training loss: 1.0351 Explore P: 0.7033\n",
      "Episode: 780 Total reward: -37.49531555175781 Training loss: 1.2267 Explore P: 0.7030\n",
      "Model Saved\n",
      "Episode: 781 Total reward: -24.167007446289062 Training loss: 1.4929 Explore P: 0.7027\n",
      "Episode: 782 Total reward: -42.202850341796875 Training loss: 1.7507 Explore P: 0.7024\n",
      "Episode: 783 Total reward: -60.78102111816406 Training loss: 0.4437 Explore P: 0.7022\n",
      "Episode: 784 Total reward: -25.5093994140625 Training loss: 12.9596 Explore P: 0.7015\n",
      "Episode: 785 Total reward: -52.29179382324219 Training loss: 15.2146 Explore P: 0.7012\n",
      "Model Saved\n",
      "Episode: 786 Total reward: -69.1495361328125 Training loss: 0.9183 Explore P: 0.7010\n",
      "Episode: 787 Total reward: -27.806503295898438 Training loss: 0.8387 Explore P: 0.7008\n",
      "Episode: 788 Total reward: -95.75578308105469 Training loss: 0.2884 Explore P: 0.7005\n",
      "Episode: 789 Total reward: -115.45599365234375 Training loss: 0.1359 Explore P: 0.6996\n",
      "Episode: 790 Total reward: -75.04397583007812 Training loss: 14.2627 Explore P: 0.6993\n",
      "Model Saved\n",
      "Episode: 791 Total reward: -83.59712219238281 Training loss: 0.6523 Explore P: 0.6991\n",
      "Episode: 792 Total reward: -113.72541809082031 Training loss: 1.4884 Explore P: 0.6988\n",
      "Episode: 793 Total reward: -52.09031677246094 Training loss: 3.1005 Explore P: 0.6985\n",
      "Episode: 794 Total reward: -101.82330322265625 Training loss: 0.2478 Explore P: 0.6982\n",
      "Episode: 795 Total reward: -72.92095947265625 Training loss: 0.5238 Explore P: 0.6979\n",
      "Model Saved\n",
      "Episode: 796 Total reward: -47.40013122558594 Training loss: 0.6269 Explore P: 0.6977\n",
      "Episode: 797 Total reward: -78.35081481933594 Training loss: 0.2981 Explore P: 0.6973\n",
      "Episode: 798 Total reward: -62.392608642578125 Training loss: 0.4274 Explore P: 0.6971\n",
      "Episode: 799 Total reward: -77.14703369140625 Training loss: 0.2290 Explore P: 0.6968\n",
      "Episode: 800 Total reward: -65.98731994628906 Training loss: 5.1781 Explore P: 0.6965\n",
      "Model Saved\n",
      "Episode: 801 Total reward: -88.68806457519531 Training loss: 0.2730 Explore P: 0.6962\n",
      "Episode: 802 Total reward: -102.16357421875 Training loss: 0.1952 Explore P: 0.6961\n",
      "Episode: 803 Total reward: -99.76705932617188 Training loss: 28.9137 Explore P: 0.6957\n",
      "Episode: 804 Total reward: -49.89935302734375 Training loss: 12.2475 Explore P: 0.6954\n",
      "Episode: 805 Total reward: -80.39511108398438 Training loss: 0.3308 Explore P: 0.6946\n",
      "Model Saved\n",
      "Episode: 806 Total reward: -70.98606872558594 Training loss: 8.0376 Explore P: 0.6943\n",
      "Episode: 807 Total reward: -72.33866882324219 Training loss: 0.9131 Explore P: 0.6939\n",
      "Episode: 808 Total reward: -98.97760009765625 Training loss: 0.4065 Explore P: 0.6936\n",
      "Episode: 809 Total reward: -59.3853759765625 Training loss: 0.1482 Explore P: 0.6934\n",
      "Episode: 810 Total reward: -97.98670959472656 Training loss: 0.1929 Explore P: 0.6931\n",
      "Model Saved\n",
      "Episode: 811 Total reward: -74.11415100097656 Training loss: 2.4113 Explore P: 0.6928\n",
      "Episode: 812 Total reward: -51.61085510253906 Training loss: 0.7460 Explore P: 0.6925\n",
      "Episode: 813 Total reward: -96.35311889648438 Training loss: 17.8170 Explore P: 0.6924\n",
      "Episode: 814 Total reward: -95.06840515136719 Training loss: 11.4835 Explore P: 0.6919\n",
      "Episode: 815 Total reward: -69.88870239257812 Training loss: 0.4900 Explore P: 0.6917\n",
      "Model Saved\n",
      "Episode: 816 Total reward: -94.4439697265625 Training loss: 0.3473 Explore P: 0.6914\n",
      "Episode: 817 Total reward: -67.43074035644531 Training loss: 7.0620 Explore P: 0.6911\n",
      "Episode: 818 Total reward: -50.42768859863281 Training loss: 0.2985 Explore P: 0.6907\n",
      "Episode: 819 Total reward: -99.72718811035156 Training loss: 0.3196 Explore P: 0.6904\n",
      "Episode: 820 Total reward: -79.18896484375 Training loss: 0.4256 Explore P: 0.6901\n",
      "Model Saved\n",
      "Episode: 821 Total reward: -112.05540466308594 Training loss: 0.2911 Explore P: 0.6900\n",
      "Episode: 822 Total reward: -51.78436279296875 Training loss: 0.4353 Explore P: 0.6897\n",
      "Episode: 823 Total reward: -85.3275146484375 Training loss: 19.9386 Explore P: 0.6894\n",
      "Episode: 824 Total reward: -86.83804321289062 Training loss: 1.7416 Explore P: 0.6892\n",
      "Episode: 825 Total reward: -79.03504943847656 Training loss: 1.5435 Explore P: 0.6888\n",
      "Model Saved\n",
      "Episode: 826 Total reward: -111.0487060546875 Training loss: 0.4165 Explore P: 0.6886\n",
      "Episode: 827 Total reward: -55.90937805175781 Training loss: 1.5140 Explore P: 0.6883\n",
      "Episode: 828 Total reward: -104.9095458984375 Training loss: 0.4597 Explore P: 0.6883\n",
      "Episode: 829 Total reward: -65.36885070800781 Training loss: 0.9398 Explore P: 0.6880\n",
      "Episode: 830 Total reward: -95.00682067871094 Training loss: 15.4267 Explore P: 0.6877\n",
      "Model Saved\n",
      "Episode: 831 Total reward: -92.63803100585938 Training loss: 4.1372 Explore P: 0.6874\n",
      "Episode: 832 Total reward: -93.33612060546875 Training loss: 2.2062 Explore P: 0.6871\n",
      "Episode: 833 Total reward: -112.25584411621094 Training loss: 0.2821 Explore P: 0.6869\n",
      "Episode: 834 Total reward: -100.03132629394531 Training loss: 0.2618 Explore P: 0.6867\n",
      "Episode: 835 Total reward: -72.9056396484375 Training loss: 0.8992 Explore P: 0.6864\n",
      "Model Saved\n",
      "Episode: 836 Total reward: -73.24125671386719 Training loss: 0.2195 Explore P: 0.6861\n",
      "Episode: 837 Total reward: -62.32209777832031 Training loss: 0.2810 Explore P: 0.6858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 838 Total reward: -109.33152770996094 Training loss: 3.4704 Explore P: 0.6856\n",
      "Episode: 839 Total reward: -53.52496337890625 Training loss: 5.3110 Explore P: 0.6854\n",
      "Episode: 840 Total reward: -70.88014221191406 Training loss: 16.5006 Explore P: 0.6851\n",
      "Model Saved\n",
      "Episode: 841 Total reward: -84.19572448730469 Training loss: 0.2683 Explore P: 0.6848\n",
      "Episode: 842 Total reward: -81.27275085449219 Training loss: 0.6302 Explore P: 0.6845\n",
      "Episode: 843 Total reward: -90.20562744140625 Training loss: 0.2012 Explore P: 0.6843\n",
      "Episode: 844 Total reward: -27.384475708007812 Training loss: 0.2481 Explore P: 0.6840\n",
      "Episode: 845 Total reward: -70.51200866699219 Training loss: 5.4617 Explore P: 0.6838\n",
      "Model Saved\n",
      "Episode: 846 Total reward: -77.88473510742188 Training loss: 0.6215 Explore P: 0.6835\n",
      "Episode: 847 Total reward: -27.852554321289062 Training loss: 1.8137 Explore P: 0.6833\n",
      "Episode: 848 Total reward: -86.81159973144531 Training loss: 0.4564 Explore P: 0.6829\n",
      "Episode: 849 Total reward: -61.536376953125 Training loss: 17.1928 Explore P: 0.6827\n",
      "Episode: 850 Total reward: -56.48570251464844 Training loss: 0.8351 Explore P: 0.6824\n",
      "Model Saved\n",
      "Episode: 851 Total reward: -72.89053344726562 Training loss: 2.1439 Explore P: 0.6821\n",
      "Episode: 852 Total reward: -115.95649719238281 Training loss: 0.2564 Explore P: 0.6818\n",
      "Episode: 853 Total reward: -45.0662841796875 Training loss: 0.3948 Explore P: 0.6816\n",
      "Episode: 854 Total reward: -39.20286560058594 Training loss: 1.8010 Explore P: 0.6812\n",
      "Episode: 855 Total reward: -78.73614501953125 Training loss: 0.1806 Explore P: 0.6809\n",
      "Model Saved\n",
      "Episode: 856 Total reward: -44.0401611328125 Training loss: 0.7024 Explore P: 0.6806\n",
      "Episode: 857 Total reward: -77.70541381835938 Training loss: 7.7071 Explore P: 0.6800\n",
      "Episode: 858 Total reward: -60.36656188964844 Training loss: 0.3844 Explore P: 0.6797\n",
      "Episode: 859 Total reward: -72.95614624023438 Training loss: 9.0344 Explore P: 0.6794\n",
      "Episode: 860 Total reward: -115.97587585449219 Training loss: 12.6964 Explore P: 0.6793\n",
      "Model Saved\n",
      "Episode: 861 Total reward: -94.59819030761719 Training loss: 0.5461 Explore P: 0.6789\n",
      "Episode: 862 Total reward: -65.07667541503906 Training loss: 1.8828 Explore P: 0.6786\n",
      "Episode: 863 Total reward: -64.01893615722656 Training loss: 0.8841 Explore P: 0.6784\n",
      "Episode: 864 Total reward: -98.72276306152344 Training loss: 1.5065 Explore P: 0.6781\n",
      "Episode: 865 Total reward: -27.960769653320312 Training loss: 0.8778 Explore P: 0.6778\n",
      "Model Saved\n",
      "Episode: 866 Total reward: -50.21307373046875 Training loss: 2.1420 Explore P: 0.6775\n",
      "Episode: 867 Total reward: -81.48500061035156 Training loss: 0.2630 Explore P: 0.6773\n",
      "Episode: 868 Total reward: -51.55438232421875 Training loss: 0.3183 Explore P: 0.6770\n",
      "Episode: 869 Total reward: -19.455368041992188 Training loss: 0.7555 Explore P: 0.6763\n",
      "Episode: 870 Total reward: -106.44357299804688 Training loss: 0.3116 Explore P: 0.6760\n",
      "Model Saved\n",
      "Episode: 871 Total reward: -111.35641479492188 Training loss: 0.6939 Explore P: 0.6758\n",
      "Episode: 872 Total reward: -63.16156005859375 Training loss: 1.1582 Explore P: 0.6755\n",
      "Episode: 873 Total reward: -72.79513549804688 Training loss: 2.7306 Explore P: 0.6752\n",
      "Episode: 874 Total reward: -59.10992431640625 Training loss: 0.5975 Explore P: 0.6750\n",
      "Episode: 875 Total reward: -65.80429077148438 Training loss: 3.7159 Explore P: 0.6747\n",
      "Model Saved\n",
      "Episode: 876 Total reward: -104.08955383300781 Training loss: 16.3112 Explore P: 0.6743\n",
      "Episode: 877 Total reward: -105.70721435546875 Training loss: 0.2275 Explore P: 0.6740\n",
      "Episode: 878 Total reward: -43.712310791015625 Training loss: 0.3264 Explore P: 0.6738\n",
      "Episode: 879 Total reward: -56.66552734375 Training loss: 0.4444 Explore P: 0.6735\n",
      "Episode: 880 Total reward: -94.98797607421875 Training loss: 0.3076 Explore P: 0.6732\n",
      "Model Saved\n",
      "Episode: 881 Total reward: -53.197113037109375 Training loss: 0.2815 Explore P: 0.6730\n",
      "Episode: 882 Total reward: -78.75108337402344 Training loss: 0.3827 Explore P: 0.6727\n",
      "Episode: 883 Total reward: -51.11236572265625 Training loss: 1.1327 Explore P: 0.6724\n",
      "Episode: 884 Total reward: -41.1265869140625 Training loss: 0.3299 Explore P: 0.6721\n",
      "Episode: 885 Total reward: -97.00579833984375 Training loss: 0.4907 Explore P: 0.6719\n",
      "Model Saved\n",
      "Episode: 886 Total reward: -49.55216979980469 Training loss: 1.0125 Explore P: 0.6716\n",
      "Episode: 887 Total reward: -101.70294189453125 Training loss: 0.4754 Explore P: 0.6713\n",
      "Episode: 888 Total reward: -54.48033142089844 Training loss: 0.2551 Explore P: 0.6711\n",
      "Episode: 889 Total reward: -33.17375183105469 Training loss: 0.3733 Explore P: 0.6706\n",
      "Episode: 890 Total reward: -52.229034423828125 Training loss: 13.9831 Explore P: 0.6702\n",
      "Model Saved\n",
      "Episode: 891 Total reward: -67.033203125 Training loss: 14.9136 Explore P: 0.6700\n",
      "Episode: 892 Total reward: -93.85353088378906 Training loss: 0.8556 Explore P: 0.6697\n",
      "Episode: 893 Total reward: -73.75991821289062 Training loss: 0.6980 Explore P: 0.6694\n",
      "Episode: 894 Total reward: -115.97578430175781 Training loss: 7.1377 Explore P: 0.6693\n",
      "Episode: 895 Total reward: -48.66999816894531 Training loss: 0.5227 Explore P: 0.6690\n",
      "Model Saved\n",
      "Episode: 896 Total reward: -53.41107177734375 Training loss: 10.6823 Explore P: 0.6687\n",
      "Episode: 897 Total reward: -69.42219543457031 Training loss: 0.1864 Explore P: 0.6686\n",
      "Episode: 898 Total reward: -58.249908447265625 Training loss: 2.7813 Explore P: 0.6683\n",
      "Episode: 899 Total reward: -70.94131469726562 Training loss: 1.8034 Explore P: 0.6680\n",
      "Episode: 900 Total reward: -57.71356201171875 Training loss: 1.3418 Explore P: 0.6678\n",
      "Model Saved\n",
      "Episode: 901 Total reward: -113.14973449707031 Training loss: 0.5570 Explore P: 0.6675\n",
      "Episode: 902 Total reward: -110.50895690917969 Training loss: 4.5761 Explore P: 0.6672\n",
      "Episode: 903 Total reward: -34.18055725097656 Training loss: 2.2239 Explore P: 0.6669\n",
      "Episode: 904 Total reward: -81.693603515625 Training loss: 0.1979 Explore P: 0.6666\n",
      "Episode: 905 Total reward: -100.44180297851562 Training loss: 1.6219 Explore P: 0.6664\n",
      "Model Saved\n",
      "Episode: 906 Total reward: -60.690277099609375 Training loss: 0.2408 Explore P: 0.6661\n",
      "Episode: 907 Total reward: -88.44644165039062 Training loss: 0.3236 Explore P: 0.6658\n",
      "Episode: 908 Total reward: -84.702392578125 Training loss: 0.2356 Explore P: 0.6656\n",
      "Episode: 909 Total reward: -49.394134521484375 Training loss: 1.2478 Explore P: 0.6653\n",
      "Episode: 910 Total reward: -52.308624267578125 Training loss: 18.4706 Explore P: 0.6650\n",
      "Model Saved\n",
      "Episode: 911 Total reward: -95.89671325683594 Training loss: 0.4895 Explore P: 0.6648\n",
      "Episode: 912 Total reward: -2.6863861083984375 Training loss: 1.4425 Explore P: 0.6645\n",
      "Episode: 913 Total reward: -72.46388244628906 Training loss: 0.3287 Explore P: 0.6643\n",
      "Episode: 914 Total reward: -73.016357421875 Training loss: 0.1837 Explore P: 0.6641\n",
      "Episode: 915 Total reward: -80.44071960449219 Training loss: 0.8074 Explore P: 0.6638\n",
      "Model Saved\n",
      "Episode: 916 Total reward: -58.98115539550781 Training loss: 14.2166 Explore P: 0.6635\n",
      "Episode: 917 Total reward: -59.55030822753906 Training loss: 1.5732 Explore P: 0.6633\n",
      "Episode: 918 Total reward: -90.37232971191406 Training loss: 1.0329 Explore P: 0.6630\n",
      "Episode: 919 Total reward: -68.46961975097656 Training loss: 0.1874 Explore P: 0.6627\n",
      "Episode: 920 Total reward: -60.570098876953125 Training loss: 0.5468 Explore P: 0.6625\n",
      "Model Saved\n",
      "Episode: 921 Total reward: -91.0606689453125 Training loss: 0.3409 Explore P: 0.6623\n",
      "Episode: 922 Total reward: -69.00706481933594 Training loss: 0.2029 Explore P: 0.6619\n",
      "Episode: 923 Total reward: -70.09800720214844 Training loss: 1.0133 Explore P: 0.6617\n",
      "Episode: 924 Total reward: -15.458358764648438 Training loss: 0.2148 Explore P: 0.6613\n",
      "Episode: 925 Total reward: -114.03170776367188 Training loss: 1.1185 Explore P: 0.6611\n",
      "Model Saved\n",
      "Episode: 926 Total reward: -115.46295166015625 Training loss: 0.3943 Explore P: 0.6609\n",
      "Episode: 927 Total reward: -52.50996398925781 Training loss: 0.3423 Explore P: 0.6606\n",
      "Episode: 928 Total reward: -47.31648254394531 Training loss: 0.2936 Explore P: 0.6602\n",
      "Episode: 929 Total reward: -81.43269348144531 Training loss: 0.4126 Explore P: 0.6600\n",
      "Episode: 930 Total reward: -73.86723327636719 Training loss: 2.1609 Explore P: 0.6598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Episode: 931 Total reward: -61.2415771484375 Training loss: 18.6300 Explore P: 0.6594\n",
      "Episode: 932 Total reward: -41.49824523925781 Training loss: 0.4449 Explore P: 0.6591\n",
      "Episode: 933 Total reward: -72.8021240234375 Training loss: 0.2623 Explore P: 0.6589\n",
      "Episode: 934 Total reward: -48.18354797363281 Training loss: 0.2983 Explore P: 0.6586\n",
      "Episode: 935 Total reward: -102.05839538574219 Training loss: 11.7334 Explore P: 0.6585\n",
      "Model Saved\n",
      "Episode: 936 Total reward: -93.12200927734375 Training loss: 0.1802 Explore P: 0.6583\n",
      "Episode: 937 Total reward: -69.50863647460938 Training loss: 0.4184 Explore P: 0.6581\n",
      "Episode: 938 Total reward: -79.55982971191406 Training loss: 0.2851 Explore P: 0.6579\n",
      "Episode: 939 Total reward: -43.74687194824219 Training loss: 2.7371 Explore P: 0.6576\n",
      "Episode: 940 Total reward: -43.46736145019531 Training loss: 1.1369 Explore P: 0.6573\n",
      "Model Saved\n",
      "Episode: 941 Total reward: -68.41046142578125 Training loss: 1.5289 Explore P: 0.6571\n",
      "Episode: 942 Total reward: -65.05908203125 Training loss: 0.2837 Explore P: 0.6568\n",
      "Episode: 943 Total reward: -52.30546569824219 Training loss: 0.6843 Explore P: 0.6567\n",
      "Episode: 944 Total reward: -115.97880554199219 Training loss: 4.1809 Explore P: 0.6564\n",
      "Episode: 945 Total reward: -106.34698486328125 Training loss: 0.3717 Explore P: 0.6560\n",
      "Model Saved\n",
      "Episode: 946 Total reward: -32.31504821777344 Training loss: 0.3263 Explore P: 0.6558\n",
      "Episode: 947 Total reward: -70.60049438476562 Training loss: 0.2771 Explore P: 0.6556\n",
      "Episode: 948 Total reward: -82.96308898925781 Training loss: 0.2528 Explore P: 0.6553\n",
      "Episode: 949 Total reward: -69.34426879882812 Training loss: 0.3698 Explore P: 0.6550\n",
      "Episode: 950 Total reward: -49.04872131347656 Training loss: 0.3631 Explore P: 0.6548\n",
      "Model Saved\n",
      "Episode: 951 Total reward: -50.74092102050781 Training loss: 1.1027 Explore P: 0.6545\n",
      "Episode: 952 Total reward: -86.28425598144531 Training loss: 1.5873 Explore P: 0.6542\n",
      "Episode: 953 Total reward: -71.73223876953125 Training loss: 0.3428 Explore P: 0.6540\n",
      "Episode: 954 Total reward: -76.32316589355469 Training loss: 0.6144 Explore P: 0.6537\n",
      "Episode: 955 Total reward: -63.7569580078125 Training loss: 1.1622 Explore P: 0.6534\n",
      "Model Saved\n",
      "Episode: 956 Total reward: -63.701324462890625 Training loss: 7.8656 Explore P: 0.6532\n",
      "Episode: 957 Total reward: -49.62144470214844 Training loss: 0.3204 Explore P: 0.6529\n",
      "Episode: 958 Total reward: -94.79637145996094 Training loss: 0.2971 Explore P: 0.6527\n",
      "Episode: 959 Total reward: -62.29054260253906 Training loss: 0.2864 Explore P: 0.6524\n",
      "Episode: 960 Total reward: -61.078033447265625 Training loss: 0.7635 Explore P: 0.6521\n",
      "Model Saved\n",
      "Episode: 961 Total reward: -92.76754760742188 Training loss: 8.9799 Explore P: 0.6518\n",
      "Episode: 962 Total reward: -50.89399719238281 Training loss: 0.2970 Explore P: 0.6515\n",
      "Episode: 963 Total reward: -102.51022338867188 Training loss: 0.2011 Explore P: 0.6513\n",
      "Episode: 964 Total reward: -52.276702880859375 Training loss: 1.7042 Explore P: 0.6511\n",
      "Episode: 965 Total reward: -27.104827880859375 Training loss: 1.6323 Explore P: 0.6508\n",
      "Model Saved\n",
      "Episode: 966 Total reward: -72.01264953613281 Training loss: 0.3348 Explore P: 0.6505\n",
      "Episode: 967 Total reward: -42.94755554199219 Training loss: 0.4416 Explore P: 0.6502\n",
      "Episode: 968 Total reward: -83.4637451171875 Training loss: 6.2271 Explore P: 0.6499\n",
      "Episode: 969 Total reward: -102.50849914550781 Training loss: 0.3483 Explore P: 0.6497\n",
      "Episode: 970 Total reward: -64.36604309082031 Training loss: 18.2111 Explore P: 0.6494\n",
      "Model Saved\n",
      "Episode: 971 Total reward: -95.71937561035156 Training loss: 0.2831 Explore P: 0.6491\n",
      "Episode: 972 Total reward: -77.36782836914062 Training loss: 0.5849 Explore P: 0.6486\n",
      "Episode: 973 Total reward: -107.08419799804688 Training loss: 0.2872 Explore P: 0.6483\n",
      "Episode: 974 Total reward: -70.95451354980469 Training loss: 0.7383 Explore P: 0.6480\n",
      "Episode: 975 Total reward: -85.79046630859375 Training loss: 0.6578 Explore P: 0.6478\n",
      "Model Saved\n",
      "Episode: 976 Total reward: -83.16128540039062 Training loss: 0.1838 Explore P: 0.6475\n",
      "Episode: 977 Total reward: -53.31968688964844 Training loss: 14.0054 Explore P: 0.6473\n",
      "Episode: 978 Total reward: -60.62034606933594 Training loss: 0.6456 Explore P: 0.6469\n",
      "Episode: 979 Total reward: -58.66996765136719 Training loss: 0.7577 Explore P: 0.6466\n",
      "Episode: 980 Total reward: -91.41667175292969 Training loss: 12.7185 Explore P: 0.6464\n",
      "Model Saved\n",
      "Episode: 981 Total reward: -70.86102294921875 Training loss: 0.1672 Explore P: 0.6461\n",
      "Episode: 982 Total reward: -69.62199401855469 Training loss: 1.0431 Explore P: 0.6459\n",
      "Episode: 983 Total reward: -54.296295166015625 Training loss: 0.3776 Explore P: 0.6456\n",
      "Episode: 984 Total reward: -58.56439208984375 Training loss: 0.2284 Explore P: 0.6453\n",
      "Episode: 985 Total reward: -59.207183837890625 Training loss: 2.2958 Explore P: 0.6452\n",
      "Model Saved\n",
      "Episode: 986 Total reward: -67.585205078125 Training loss: 0.2262 Explore P: 0.6450\n",
      "Episode: 987 Total reward: -96.86843872070312 Training loss: 0.3575 Explore P: 0.6448\n",
      "Episode: 988 Total reward: -105.17355346679688 Training loss: 0.3074 Explore P: 0.6445\n",
      "Episode: 989 Total reward: -53.73106384277344 Training loss: 0.3363 Explore P: 0.6443\n",
      "Episode: 990 Total reward: -52.450103759765625 Training loss: 2.2350 Explore P: 0.6440\n",
      "Model Saved\n",
      "Episode: 991 Total reward: -51.15745544433594 Training loss: 0.3820 Explore P: 0.6437\n",
      "Episode: 992 Total reward: -108.39273071289062 Training loss: 0.4801 Explore P: 0.6434\n",
      "Episode: 993 Total reward: -32.382049560546875 Training loss: 0.3821 Explore P: 0.6431\n",
      "Episode: 994 Total reward: -36.77302551269531 Training loss: 0.2389 Explore P: 0.6429\n",
      "Episode: 995 Total reward: -63.66853332519531 Training loss: 0.9539 Explore P: 0.6426\n",
      "Model Saved\n",
      "Episode: 996 Total reward: -53.34977722167969 Training loss: 0.9592 Explore P: 0.6423\n",
      "Episode: 997 Total reward: -55.20307922363281 Training loss: 0.1830 Explore P: 0.6420\n",
      "Episode: 998 Total reward: -62.352630615234375 Training loss: 0.3779 Explore P: 0.6417\n",
      "Episode: 999 Total reward: -73.52220153808594 Training loss: 0.8290 Explore P: 0.6415\n",
      "Episode: 1000 Total reward: -91.5399169921875 Training loss: 0.3198 Explore P: 0.6412\n",
      "Model Saved\n",
      "Episode: 1001 Total reward: -58.926116943359375 Training loss: 1.4878 Explore P: 0.6410\n",
      "Episode: 1002 Total reward: -51.57929992675781 Training loss: 0.6142 Explore P: 0.6407\n",
      "Episode: 1003 Total reward: -80.94374084472656 Training loss: 0.1698 Explore P: 0.6406\n",
      "Episode: 1004 Total reward: -72.6226806640625 Training loss: 0.2345 Explore P: 0.6403\n",
      "Episode: 1005 Total reward: -102.61123657226562 Training loss: 0.5934 Explore P: 0.6402\n",
      "Model Saved\n",
      "Episode: 1006 Total reward: -39.87738037109375 Training loss: 0.3673 Explore P: 0.6399\n",
      "Episode: 1007 Total reward: -43.22596740722656 Training loss: 1.2800 Explore P: 0.6396\n",
      "Episode: 1008 Total reward: -77.03713989257812 Training loss: 3.5254 Explore P: 0.6394\n",
      "Episode: 1009 Total reward: -83.34022521972656 Training loss: 9.4797 Explore P: 0.6390\n",
      "Episode: 1010 Total reward: -93.06246948242188 Training loss: 0.2869 Explore P: 0.6388\n",
      "Model Saved\n",
      "Episode: 1011 Total reward: -45.538116455078125 Training loss: 0.2998 Explore P: 0.6385\n",
      "Episode: 1012 Total reward: -78.70179748535156 Training loss: 0.6429 Explore P: 0.6383\n",
      "Episode: 1013 Total reward: -104.66131591796875 Training loss: 0.2343 Explore P: 0.6381\n",
      "Episode: 1014 Total reward: -48.601287841796875 Training loss: 0.2376 Explore P: 0.6379\n",
      "Episode: 1015 Total reward: -49.31901550292969 Training loss: 0.2235 Explore P: 0.6375\n",
      "Model Saved\n",
      "Episode: 1016 Total reward: -86.71125793457031 Training loss: 1.3807 Explore P: 0.6372\n",
      "Episode: 1017 Total reward: -96.19866943359375 Training loss: 0.2716 Explore P: 0.6370\n",
      "Episode: 1018 Total reward: -69.81182861328125 Training loss: 0.3550 Explore P: 0.6367\n",
      "Episode: 1019 Total reward: -91.92662048339844 Training loss: 0.2815 Explore P: 0.6365\n",
      "Episode: 1020 Total reward: -90.08352661132812 Training loss: 0.4426 Explore P: 0.6361\n",
      "Model Saved\n",
      "Episode: 1021 Total reward: -70.69749450683594 Training loss: 0.3352 Explore P: 0.6359\n",
      "Episode: 1022 Total reward: -69.06626892089844 Training loss: 1.6920 Explore P: 0.6355\n",
      "Episode: 1023 Total reward: -97.13252258300781 Training loss: 0.1683 Explore P: 0.6353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1024 Total reward: -63.71565246582031 Training loss: 0.4083 Explore P: 0.6350\n",
      "Episode: 1025 Total reward: -113.44839477539062 Training loss: 0.2332 Explore P: 0.6349\n",
      "Model Saved\n",
      "Episode: 1026 Total reward: -97.58724975585938 Training loss: 0.1821 Explore P: 0.6347\n",
      "Episode: 1027 Total reward: -12.840606689453125 Training loss: 0.5584 Explore P: 0.6343\n",
      "Episode: 1028 Total reward: -44.70875549316406 Training loss: 0.1758 Explore P: 0.6340\n",
      "Episode: 1029 Total reward: -78.897216796875 Training loss: 0.4320 Explore P: 0.6338\n",
      "Episode: 1030 Total reward: -87.35614013671875 Training loss: 0.5264 Explore P: 0.6335\n",
      "Model Saved\n",
      "Episode: 1031 Total reward: -80.76669311523438 Training loss: 0.8729 Explore P: 0.6333\n",
      "Episode: 1032 Total reward: -7.77655029296875 Training loss: 0.2503 Explore P: 0.6330\n",
      "Episode: 1033 Total reward: -90.92988586425781 Training loss: 0.6613 Explore P: 0.6328\n",
      "Episode: 1034 Total reward: -53.602996826171875 Training loss: 0.2359 Explore P: 0.6325\n",
      "Episode: 1035 Total reward: -47.898193359375 Training loss: 1.1685 Explore P: 0.6323\n",
      "Model Saved\n",
      "Episode: 1036 Total reward: -37.37025451660156 Training loss: 0.2045 Explore P: 0.6320\n",
      "Episode: 1037 Total reward: -84.78982543945312 Training loss: 4.1962 Explore P: 0.6318\n",
      "Episode: 1038 Total reward: -63.511444091796875 Training loss: 0.1574 Explore P: 0.6315\n",
      "Episode: 1039 Total reward: -28.8941650390625 Training loss: 0.6310 Explore P: 0.6312\n",
      "Episode: 1040 Total reward: -98.43310546875 Training loss: 0.1689 Explore P: 0.6311\n",
      "Model Saved\n",
      "Episode: 1041 Total reward: -51.952789306640625 Training loss: 0.3565 Explore P: 0.6305\n",
      "Episode: 1042 Total reward: -80.17005920410156 Training loss: 12.1082 Explore P: 0.6302\n",
      "Episode: 1043 Total reward: -21.540374755859375 Training loss: 0.6054 Explore P: 0.6299\n",
      "Episode: 1044 Total reward: -34.484161376953125 Training loss: 0.2561 Explore P: 0.6297\n",
      "Episode: 1045 Total reward: -112.07803344726562 Training loss: 0.2988 Explore P: 0.6295\n",
      "Model Saved\n",
      "Episode: 1046 Total reward: -73.21575927734375 Training loss: 0.5235 Explore P: 0.6293\n",
      "Episode: 1047 Total reward: -56.24128723144531 Training loss: 0.3307 Explore P: 0.6290\n",
      "Episode: 1048 Total reward: -21.503082275390625 Training loss: 0.4171 Explore P: 0.6288\n",
      "Episode: 1049 Total reward: -46.01597595214844 Training loss: 6.5968 Explore P: 0.6285\n",
      "Episode: 1050 Total reward: -84.7366943359375 Training loss: 0.9428 Explore P: 0.6283\n",
      "Model Saved\n",
      "Episode: 1051 Total reward: -65.49824523925781 Training loss: 13.5198 Explore P: 0.6280\n",
      "Episode: 1052 Total reward: -44.84208679199219 Training loss: 2.9959 Explore P: 0.6278\n",
      "Episode: 1053 Total reward: -57.752960205078125 Training loss: 1.2456 Explore P: 0.6275\n",
      "Episode: 1054 Total reward: -66.75021362304688 Training loss: 0.3276 Explore P: 0.6273\n",
      "Episode: 1055 Total reward: -50.49566650390625 Training loss: 0.3180 Explore P: 0.6270\n",
      "Model Saved\n",
      "Episode: 1056 Total reward: -78.58683776855469 Training loss: 11.8390 Explore P: 0.6268\n",
      "Episode: 1057 Total reward: -61.313873291015625 Training loss: 0.3494 Explore P: 0.6265\n",
      "Episode: 1058 Total reward: -95.80224609375 Training loss: 0.2053 Explore P: 0.6263\n",
      "Episode: 1059 Total reward: -92.57388305664062 Training loss: 0.5523 Explore P: 0.6260\n",
      "Episode: 1060 Total reward: -66.88174438476562 Training loss: 0.5264 Explore P: 0.6258\n",
      "Model Saved\n",
      "Episode: 1061 Total reward: -103.33248901367188 Training loss: 1.0258 Explore P: 0.6256\n",
      "Episode: 1062 Total reward: -49.70613098144531 Training loss: 0.5984 Explore P: 0.6254\n",
      "Episode: 1063 Total reward: -57.105010986328125 Training loss: 0.2670 Explore P: 0.6251\n",
      "Episode: 1064 Total reward: -103.54904174804688 Training loss: 2.1152 Explore P: 0.6250\n",
      "Episode: 1065 Total reward: -115.97587585449219 Training loss: 0.1657 Explore P: 0.6248\n",
      "Model Saved\n",
      "Episode: 1066 Total reward: -50.62353515625 Training loss: 0.6425 Explore P: 0.6245\n",
      "Episode: 1067 Total reward: -57.3314208984375 Training loss: 1.0656 Explore P: 0.6243\n",
      "Episode: 1068 Total reward: -12.455841064453125 Training loss: 2.5799 Explore P: 0.6240\n",
      "Episode: 1069 Total reward: -90.67413330078125 Training loss: 19.6133 Explore P: 0.6238\n",
      "Episode: 1070 Total reward: -114.45553588867188 Training loss: 0.3653 Explore P: 0.6234\n",
      "Model Saved\n",
      "Episode: 1071 Total reward: -78.57003784179688 Training loss: 0.9792 Explore P: 0.6232\n",
      "Episode: 1072 Total reward: -49.21540832519531 Training loss: 0.3976 Explore P: 0.6228\n",
      "Episode: 1073 Total reward: -70.07673645019531 Training loss: 0.7387 Explore P: 0.6226\n",
      "Episode: 1074 Total reward: -51.48960876464844 Training loss: 0.4322 Explore P: 0.6223\n",
      "Episode: 1075 Total reward: -77.67678833007812 Training loss: 16.3078 Explore P: 0.6221\n",
      "Model Saved\n",
      "Episode: 1076 Total reward: -93.209716796875 Training loss: 0.3080 Explore P: 0.6218\n",
      "Episode: 1077 Total reward: -115.99432373046875 Training loss: 0.6788 Explore P: 0.6216\n",
      "Episode: 1078 Total reward: -83.3023681640625 Training loss: 2.8629 Explore P: 0.6213\n",
      "Episode: 1079 Total reward: -87.43429565429688 Training loss: 0.2821 Explore P: 0.6211\n",
      "Episode: 1080 Total reward: -50.279937744140625 Training loss: 0.4309 Explore P: 0.6207\n",
      "Model Saved\n",
      "Episode: 1081 Total reward: -57.338104248046875 Training loss: 0.4351 Explore P: 0.6204\n",
      "Episode: 1082 Total reward: -53.58836364746094 Training loss: 0.3125 Explore P: 0.6202\n",
      "Episode: 1083 Total reward: -69.62777709960938 Training loss: 0.4912 Explore P: 0.6199\n",
      "Episode: 1084 Total reward: -57.08648681640625 Training loss: 0.1836 Explore P: 0.6197\n",
      "Episode: 1085 Total reward: -69.89805603027344 Training loss: 2.7947 Explore P: 0.6196\n",
      "Model Saved\n",
      "Episode: 1086 Total reward: 8.465774536132812 Training loss: 1.2482 Explore P: 0.6193\n",
      "Episode: 1087 Total reward: -88.29850769042969 Training loss: 2.3339 Explore P: 0.6190\n",
      "Episode: 1088 Total reward: -7.59716796875 Training loss: 0.2670 Explore P: 0.6187\n",
      "Episode: 1089 Total reward: -96.60057067871094 Training loss: 0.1709 Explore P: 0.6185\n",
      "Episode: 1090 Total reward: -57.85398864746094 Training loss: 0.8849 Explore P: 0.6181\n",
      "Model Saved\n",
      "Episode: 1091 Total reward: -70.89990234375 Training loss: 25.4503 Explore P: 0.6179\n",
      "Episode: 1092 Total reward: -51.102691650390625 Training loss: 0.8328 Explore P: 0.6177\n",
      "Episode: 1093 Total reward: -52.61749267578125 Training loss: 10.9403 Explore P: 0.6174\n",
      "Episode: 1094 Total reward: -111.23460388183594 Training loss: 0.2403 Explore P: 0.6172\n",
      "Episode: 1095 Total reward: -68.82733154296875 Training loss: 0.5594 Explore P: 0.6169\n",
      "Model Saved\n",
      "Episode: 1096 Total reward: -43.10821533203125 Training loss: 0.1751 Explore P: 0.6167\n",
      "Episode: 1097 Total reward: -62.50688171386719 Training loss: 1.4724 Explore P: 0.6164\n",
      "Episode: 1098 Total reward: -50.047698974609375 Training loss: 0.3344 Explore P: 0.6162\n",
      "Episode: 1099 Total reward: -71.76533508300781 Training loss: 0.3061 Explore P: 0.6159\n",
      "Episode: 1100 Total reward: -70.97276306152344 Training loss: 0.5584 Explore P: 0.6157\n",
      "Model Saved\n",
      "Episode: 1101 Total reward: -62.032135009765625 Training loss: 1.3288 Explore P: 0.6154\n",
      "Episode: 1102 Total reward: -93.48822021484375 Training loss: 0.6158 Explore P: 0.6151\n",
      "Episode: 1103 Total reward: -46.0379638671875 Training loss: 0.2806 Explore P: 0.6149\n",
      "Episode: 1104 Total reward: -81.88444519042969 Training loss: 0.7320 Explore P: 0.6146\n",
      "Episode: 1105 Total reward: -47.20021057128906 Training loss: 0.7598 Explore P: 0.6144\n",
      "Model Saved\n",
      "Episode: 1106 Total reward: -98.23558044433594 Training loss: 0.9568 Explore P: 0.6140\n",
      "Episode: 1107 Total reward: -65.06547546386719 Training loss: 0.3648 Explore P: 0.6137\n",
      "Episode: 1108 Total reward: -61.83750915527344 Training loss: 0.3885 Explore P: 0.6135\n",
      "Episode: 1109 Total reward: -62.89862060546875 Training loss: 0.5282 Explore P: 0.6132\n",
      "Episode: 1110 Total reward: -57.41929626464844 Training loss: 1.0732 Explore P: 0.6130\n",
      "Model Saved\n",
      "Episode: 1111 Total reward: -101.69387817382812 Training loss: 0.5998 Explore P: 0.6128\n",
      "Episode: 1112 Total reward: -60.227691650390625 Training loss: 0.1950 Explore P: 0.6125\n",
      "Episode: 1113 Total reward: -93.53645324707031 Training loss: 0.2043 Explore P: 0.6124\n",
      "Episode: 1114 Total reward: -63.564544677734375 Training loss: 0.1729 Explore P: 0.6121\n",
      "Episode: 1115 Total reward: -74.75759887695312 Training loss: 4.2208 Explore P: 0.6118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Episode: 1116 Total reward: -72.46185302734375 Training loss: 0.3030 Explore P: 0.6116\n",
      "Episode: 1117 Total reward: -64.90573120117188 Training loss: 0.3943 Explore P: 0.6113\n",
      "Episode: 1118 Total reward: -40.18852233886719 Training loss: 0.2559 Explore P: 0.6111\n",
      "Episode: 1119 Total reward: -49.9923095703125 Training loss: 3.7441 Explore P: 0.6109\n",
      "Episode: 1120 Total reward: -77.79727172851562 Training loss: 0.2959 Explore P: 0.6107\n",
      "Model Saved\n",
      "Episode: 1121 Total reward: -8.741943359375 Training loss: 5.9518 Explore P: 0.6104\n",
      "Episode: 1122 Total reward: -70.56138610839844 Training loss: 11.0764 Explore P: 0.6102\n",
      "Episode: 1123 Total reward: -75.44866943359375 Training loss: 0.2586 Explore P: 0.6099\n",
      "Episode: 1124 Total reward: -60.54100036621094 Training loss: 0.2526 Explore P: 0.6097\n",
      "Episode: 1125 Total reward: -52.11228942871094 Training loss: 0.4043 Explore P: 0.6093\n",
      "Model Saved\n",
      "Episode: 1126 Total reward: -54.481231689453125 Training loss: 0.4024 Explore P: 0.6090\n",
      "Episode: 1127 Total reward: -52.56268310546875 Training loss: 0.1944 Explore P: 0.6088\n",
      "Episode: 1128 Total reward: -48.24897766113281 Training loss: 0.2491 Explore P: 0.6085\n",
      "Episode: 1129 Total reward: -64.59376525878906 Training loss: 2.7290 Explore P: 0.6084\n",
      "Episode: 1130 Total reward: -81.29745483398438 Training loss: 0.2016 Explore P: 0.6082\n",
      "Model Saved\n",
      "Episode: 1131 Total reward: -104.08035278320312 Training loss: 0.2721 Explore P: 0.6079\n",
      "Episode: 1132 Total reward: -107.10855102539062 Training loss: 0.1803 Explore P: 0.6076\n",
      "Episode: 1133 Total reward: -45.52641296386719 Training loss: 0.2339 Explore P: 0.6073\n",
      "Episode: 1134 Total reward: -64.95245361328125 Training loss: 0.2138 Explore P: 0.6071\n",
      "Episode: 1135 Total reward: -47.985626220703125 Training loss: 0.1447 Explore P: 0.6069\n",
      "Model Saved\n",
      "Episode: 1136 Total reward: -54.41949462890625 Training loss: 0.6780 Explore P: 0.6066\n",
      "Episode: 1137 Total reward: -71.84352111816406 Training loss: 0.2312 Explore P: 0.6064\n",
      "Episode: 1138 Total reward: -68.37631225585938 Training loss: 0.4042 Explore P: 0.6061\n",
      "Episode: 1139 Total reward: -73.90960693359375 Training loss: 0.2539 Explore P: 0.6060\n",
      "Episode: 1140 Total reward: -68.98109436035156 Training loss: 0.3608 Explore P: 0.6057\n",
      "Model Saved\n",
      "Episode: 1141 Total reward: -75.00547790527344 Training loss: 0.2567 Explore P: 0.6055\n",
      "Episode: 1142 Total reward: -48.50822448730469 Training loss: 0.2032 Explore P: 0.6052\n",
      "Episode: 1143 Total reward: -58.3739013671875 Training loss: 1.7402 Explore P: 0.6048\n",
      "Episode: 1144 Total reward: -6.7447052001953125 Training loss: 0.3577 Explore P: 0.6046\n",
      "Episode: 1145 Total reward: -37.77186584472656 Training loss: 2.0552 Explore P: 0.6043\n",
      "Model Saved\n",
      "Episode: 1146 Total reward: -16.398208618164062 Training loss: 0.7779 Explore P: 0.6040\n",
      "Episode: 1147 Total reward: -56.96319580078125 Training loss: 0.7588 Explore P: 0.6038\n",
      "Episode: 1148 Total reward: -82.6561279296875 Training loss: 0.2918 Explore P: 0.6035\n",
      "Episode: 1149 Total reward: -64.26716613769531 Training loss: 0.3649 Explore P: 0.6032\n",
      "Episode: 1150 Total reward: -97.09742736816406 Training loss: 0.3901 Explore P: 0.6029\n",
      "Model Saved\n",
      "Episode: 1151 Total reward: -10.771652221679688 Training loss: 0.4917 Explore P: 0.6027\n",
      "Episode: 1152 Total reward: -36.57879638671875 Training loss: 0.6155 Explore P: 0.6024\n",
      "Episode: 1153 Total reward: -54.21527099609375 Training loss: 1.0889 Explore P: 0.6022\n",
      "Episode: 1154 Total reward: -61.580963134765625 Training loss: 0.2640 Explore P: 0.6019\n",
      "Episode: 1155 Total reward: -77.96345520019531 Training loss: 1.5805 Explore P: 0.6017\n",
      "Model Saved\n",
      "Episode: 1156 Total reward: -93.61697387695312 Training loss: 0.2332 Explore P: 0.6015\n",
      "Episode: 1157 Total reward: -95.48500061035156 Training loss: 4.0832 Explore P: 0.6012\n",
      "Episode: 1158 Total reward: -21.914413452148438 Training loss: 0.2081 Explore P: 0.6007\n",
      "Episode: 1159 Total reward: -79.2293701171875 Training loss: 0.2121 Explore P: 0.6004\n",
      "Episode: 1160 Total reward: -79.84144592285156 Training loss: 4.9125 Explore P: 0.6002\n",
      "Model Saved\n",
      "Episode: 1161 Total reward: -11.916748046875 Training loss: 12.4600 Explore P: 0.5998\n",
      "Episode: 1162 Total reward: -89.88609313964844 Training loss: 1.1920 Explore P: 0.5996\n",
      "Episode: 1163 Total reward: -68.35975646972656 Training loss: 1.2375 Explore P: 0.5994\n",
      "Episode: 1164 Total reward: -65.70643615722656 Training loss: 13.6554 Explore P: 0.5992\n",
      "Episode: 1165 Total reward: -111.54391479492188 Training loss: 0.1758 Explore P: 0.5987\n",
      "Model Saved\n",
      "Episode: 1166 Total reward: -64.45668029785156 Training loss: 0.6658 Explore P: 0.5985\n",
      "Episode: 1167 Total reward: -86.41094970703125 Training loss: 0.6438 Explore P: 0.5983\n",
      "Episode: 1168 Total reward: -86.59922790527344 Training loss: 0.2663 Explore P: 0.5980\n",
      "Episode: 1169 Total reward: -28.390274047851562 Training loss: 0.5204 Explore P: 0.5977\n",
      "Episode: 1170 Total reward: -69.68739318847656 Training loss: 0.3210 Explore P: 0.5975\n",
      "Model Saved\n",
      "Episode: 1171 Total reward: -44.95692443847656 Training loss: 0.6748 Explore P: 0.5972\n",
      "Episode: 1172 Total reward: -45.889923095703125 Training loss: 17.8949 Explore P: 0.5967\n",
      "Episode: 1173 Total reward: -92.55386352539062 Training loss: 6.0831 Explore P: 0.5964\n",
      "Episode: 1174 Total reward: -80.81069946289062 Training loss: 0.8733 Explore P: 0.5962\n",
      "Episode: 1175 Total reward: -3.8926849365234375 Training loss: 0.1821 Explore P: 0.5960\n",
      "Model Saved\n",
      "Episode: 1176 Total reward: -80.63833618164062 Training loss: 0.6336 Explore P: 0.5957\n",
      "Episode: 1177 Total reward: -61.253326416015625 Training loss: 0.2287 Explore P: 0.5956\n",
      "Episode: 1178 Total reward: -42.055511474609375 Training loss: 0.1872 Explore P: 0.5953\n",
      "Episode: 1179 Total reward: -85.58137512207031 Training loss: 0.2394 Explore P: 0.5950\n",
      "Episode: 1180 Total reward: -68.09638977050781 Training loss: 0.3446 Explore P: 0.5948\n",
      "Model Saved\n",
      "Episode: 1181 Total reward: -74.62196350097656 Training loss: 0.2077 Explore P: 0.5947\n",
      "Episode: 1182 Total reward: -63.88114929199219 Training loss: 0.5154 Explore P: 0.5944\n",
      "Episode: 1183 Total reward: 2.588775634765625 Training loss: 12.1035 Explore P: 0.5942\n",
      "Episode: 1184 Total reward: -115.91094970703125 Training loss: 0.4725 Explore P: 0.5940\n",
      "Episode: 1185 Total reward: -108.041748046875 Training loss: 0.8484 Explore P: 0.5938\n",
      "Model Saved\n",
      "Episode: 1186 Total reward: -63.54298400878906 Training loss: 0.7214 Explore P: 0.5936\n",
      "Episode: 1187 Total reward: -44.011627197265625 Training loss: 1.4052 Explore P: 0.5934\n",
      "Episode: 1188 Total reward: -106.22662353515625 Training loss: 0.3018 Explore P: 0.5932\n",
      "Episode: 1189 Total reward: -21.114990234375 Training loss: 0.2098 Explore P: 0.5929\n",
      "Episode: 1190 Total reward: -59.560638427734375 Training loss: 0.6762 Explore P: 0.5927\n",
      "Model Saved\n",
      "Episode: 1191 Total reward: -104.26377868652344 Training loss: 12.3556 Explore P: 0.5925\n",
      "Episode: 1192 Total reward: -66.34747314453125 Training loss: 0.7918 Explore P: 0.5922\n",
      "Episode: 1193 Total reward: -94.82391357421875 Training loss: 0.6961 Explore P: 0.5921\n",
      "Episode: 1194 Total reward: -97.31373596191406 Training loss: 0.4334 Explore P: 0.5920\n",
      "Episode: 1195 Total reward: -63.49082946777344 Training loss: 1.0487 Explore P: 0.5917\n",
      "Model Saved\n",
      "Episode: 1196 Total reward: -42.42999267578125 Training loss: 0.7317 Explore P: 0.5915\n",
      "Episode: 1197 Total reward: -72.66499328613281 Training loss: 0.2174 Explore P: 0.5913\n",
      "Episode: 1198 Total reward: -114.44076538085938 Training loss: 0.5470 Explore P: 0.5909\n",
      "Episode: 1199 Total reward: -115.90174865722656 Training loss: 0.5559 Explore P: 0.5908\n",
      "Episode: 1200 Total reward: -115.97587585449219 Training loss: 0.5745 Explore P: 0.5906\n",
      "Model Saved\n",
      "Episode: 1201 Total reward: -44.42002868652344 Training loss: 1.7262 Explore P: 0.5903\n",
      "Episode: 1202 Total reward: -45.16935729980469 Training loss: 1.0562 Explore P: 0.5900\n",
      "Episode: 1203 Total reward: -84.0125732421875 Training loss: 0.2255 Explore P: 0.5897\n",
      "Episode: 1204 Total reward: 29.952880859375 Training loss: 0.3576 Explore P: 0.5895\n",
      "Episode: 1205 Total reward: -86.79328918457031 Training loss: 0.2294 Explore P: 0.5892\n",
      "Model Saved\n",
      "Episode: 1206 Total reward: -34.412445068359375 Training loss: 0.5486 Explore P: 0.5890\n",
      "Episode: 1207 Total reward: -51.314208984375 Training loss: 15.9196 Explore P: 0.5888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1208 Total reward: -59.593017578125 Training loss: 0.7538 Explore P: 0.5885\n",
      "Episode: 1209 Total reward: -64.252685546875 Training loss: 0.4909 Explore P: 0.5883\n",
      "Episode: 1210 Total reward: 0.830352783203125 Training loss: 0.9149 Explore P: 0.5881\n",
      "Model Saved\n",
      "Episode: 1211 Total reward: -82.36257934570312 Training loss: 3.0975 Explore P: 0.5877\n",
      "Episode: 1212 Total reward: -79.88844299316406 Training loss: 0.2812 Explore P: 0.5875\n",
      "Episode: 1213 Total reward: -16.02667236328125 Training loss: 0.3698 Explore P: 0.5872\n",
      "Episode: 1214 Total reward: -70.24151611328125 Training loss: 6.4088 Explore P: 0.5869\n",
      "Episode: 1215 Total reward: -106.83871459960938 Training loss: 13.3005 Explore P: 0.5867\n",
      "Model Saved\n",
      "Episode: 1216 Total reward: -43.06178283691406 Training loss: 0.3183 Explore P: 0.5865\n",
      "Episode: 1217 Total reward: -81.78581237792969 Training loss: 13.4245 Explore P: 0.5862\n",
      "Episode: 1218 Total reward: -62.252349853515625 Training loss: 1.2333 Explore P: 0.5860\n",
      "Episode: 1219 Total reward: -103.58731079101562 Training loss: 0.2242 Explore P: 0.5857\n",
      "Episode: 1220 Total reward: -79.88883972167969 Training loss: 1.7475 Explore P: 0.5856\n",
      "Model Saved\n",
      "Episode: 1221 Total reward: -88.2689208984375 Training loss: 0.2319 Explore P: 0.5854\n",
      "Episode: 1222 Total reward: -47.0435791015625 Training loss: 0.1840 Explore P: 0.5851\n",
      "Episode: 1223 Total reward: -103.28919982910156 Training loss: 2.5457 Explore P: 0.5849\n",
      "Episode: 1224 Total reward: -48.41874694824219 Training loss: 0.2193 Explore P: 0.5847\n",
      "Episode: 1225 Total reward: -29.219894409179688 Training loss: 1.1295 Explore P: 0.5844\n",
      "Model Saved\n",
      "Episode: 1226 Total reward: -56.56831359863281 Training loss: 0.3014 Explore P: 0.5842\n",
      "Episode: 1227 Total reward: -89.70121765136719 Training loss: 0.3086 Explore P: 0.5840\n",
      "Episode: 1228 Total reward: -46.89100646972656 Training loss: 0.2151 Explore P: 0.5837\n",
      "Episode: 1229 Total reward: -53.031280517578125 Training loss: 0.2356 Explore P: 0.5833\n",
      "Episode: 1230 Total reward: -112.60372924804688 Training loss: 0.3235 Explore P: 0.5831\n",
      "Model Saved\n",
      "Episode: 1231 Total reward: -86.72636413574219 Training loss: 0.3652 Explore P: 0.5830\n",
      "Episode: 1232 Total reward: -91.71955871582031 Training loss: 0.3946 Explore P: 0.5829\n",
      "Episode: 1233 Total reward: -80.93806457519531 Training loss: 0.2408 Explore P: 0.5826\n",
      "Episode: 1234 Total reward: -72.13362121582031 Training loss: 0.2347 Explore P: 0.5825\n",
      "Episode: 1235 Total reward: -71.48469543457031 Training loss: 0.5860 Explore P: 0.5823\n",
      "Model Saved\n",
      "Episode: 1236 Total reward: -17.656509399414062 Training loss: 0.5699 Explore P: 0.5820\n",
      "Episode: 1237 Total reward: -69.89842224121094 Training loss: 0.2732 Explore P: 0.5818\n",
      "Episode: 1238 Total reward: -110.29302978515625 Training loss: 0.3276 Explore P: 0.5815\n",
      "Episode: 1239 Total reward: -86.70892333984375 Training loss: 0.5922 Explore P: 0.5813\n",
      "Episode: 1240 Total reward: -88.08273315429688 Training loss: 0.2340 Explore P: 0.5810\n",
      "Model Saved\n",
      "Episode: 1241 Total reward: -68.68692016601562 Training loss: 4.4816 Explore P: 0.5807\n",
      "Episode: 1242 Total reward: -74.84628295898438 Training loss: 0.2904 Explore P: 0.5806\n",
      "Episode: 1243 Total reward: -68.29989624023438 Training loss: 0.3535 Explore P: 0.5804\n",
      "Episode: 1244 Total reward: -74.12223815917969 Training loss: 0.1839 Explore P: 0.5802\n",
      "Episode: 1245 Total reward: -63.15745544433594 Training loss: 1.4796 Explore P: 0.5800\n",
      "Model Saved\n",
      "Episode: 1246 Total reward: -84.90097045898438 Training loss: 0.2209 Explore P: 0.5798\n",
      "Episode: 1247 Total reward: -66.70831298828125 Training loss: 0.2084 Explore P: 0.5795\n",
      "Episode: 1248 Total reward: -25.463516235351562 Training loss: 0.5881 Explore P: 0.5793\n",
      "Episode: 1249 Total reward: -92.98080444335938 Training loss: 1.9644 Explore P: 0.5792\n",
      "Episode: 1250 Total reward: -12.209716796875 Training loss: 1.2409 Explore P: 0.5789\n",
      "Model Saved\n",
      "Episode: 1251 Total reward: -76.17982482910156 Training loss: 1.1001 Explore P: 0.5787\n",
      "Episode: 1252 Total reward: -94.35162353515625 Training loss: 0.1275 Explore P: 0.5784\n",
      "Episode: 1253 Total reward: -65.27215576171875 Training loss: 0.2420 Explore P: 0.5781\n",
      "Episode: 1254 Total reward: -36.27284240722656 Training loss: 0.2123 Explore P: 0.5779\n",
      "Episode: 1255 Total reward: -79.73992919921875 Training loss: 0.2720 Explore P: 0.5777\n",
      "Model Saved\n",
      "Episode: 1256 Total reward: -12.50390625 Training loss: 0.3607 Explore P: 0.5775\n",
      "Episode: 1257 Total reward: -53.17701721191406 Training loss: 0.4807 Explore P: 0.5773\n",
      "Episode: 1258 Total reward: -52.04786682128906 Training loss: 1.1621 Explore P: 0.5771\n",
      "Episode: 1259 Total reward: -48.04917907714844 Training loss: 0.1905 Explore P: 0.5767\n",
      "Episode: 1260 Total reward: -26.722579956054688 Training loss: 0.6608 Explore P: 0.5765\n",
      "Model Saved\n",
      "Episode: 1261 Total reward: -21.439285278320312 Training loss: 0.5707 Explore P: 0.5763\n",
      "Episode: 1262 Total reward: -56.1405029296875 Training loss: 8.0153 Explore P: 0.5760\n",
      "Episode: 1263 Total reward: -4.2564697265625 Training loss: 5.2967 Explore P: 0.5758\n",
      "Episode: 1264 Total reward: -64.09707641601562 Training loss: 0.4149 Explore P: 0.5757\n",
      "Episode: 1265 Total reward: -47.393157958984375 Training loss: 2.3808 Explore P: 0.5754\n",
      "Model Saved\n",
      "Episode: 1266 Total reward: -40.02836608886719 Training loss: 0.2426 Explore P: 0.5752\n",
      "Episode: 1267 Total reward: -64.14402770996094 Training loss: 0.4044 Explore P: 0.5750\n",
      "Episode: 1268 Total reward: -85.55599975585938 Training loss: 0.2354 Explore P: 0.5748\n",
      "Episode: 1269 Total reward: -94.8397216796875 Training loss: 0.5952 Explore P: 0.5745\n",
      "Episode: 1270 Total reward: -4.9019622802734375 Training loss: 0.7821 Explore P: 0.5743\n",
      "Model Saved\n",
      "Episode: 1271 Total reward: -89.83297729492188 Training loss: 2.7538 Explore P: 0.5741\n",
      "Episode: 1272 Total reward: -115.62896728515625 Training loss: 0.4901 Explore P: 0.5738\n",
      "Episode: 1273 Total reward: -86.19827270507812 Training loss: 3.5674 Explore P: 0.5735\n",
      "Episode: 1274 Total reward: -11.9158935546875 Training loss: 0.5198 Explore P: 0.5733\n",
      "Episode: 1275 Total reward: -0.6242523193359375 Training loss: 0.6601 Explore P: 0.5731\n",
      "Model Saved\n",
      "Episode: 1276 Total reward: -85.44059753417969 Training loss: 0.4789 Explore P: 0.5728\n",
      "Episode: 1277 Total reward: -45.43853759765625 Training loss: 17.3896 Explore P: 0.5725\n",
      "Episode: 1278 Total reward: -82.87858581542969 Training loss: 0.3148 Explore P: 0.5723\n",
      "Episode: 1279 Total reward: -47.578125 Training loss: 0.7183 Explore P: 0.5721\n",
      "Episode: 1280 Total reward: -49.1136474609375 Training loss: 0.2606 Explore P: 0.5719\n",
      "Model Saved\n",
      "Episode: 1281 Total reward: -101.5340576171875 Training loss: 0.2945 Explore P: 0.5716\n",
      "Episode: 1282 Total reward: -70.43850708007812 Training loss: 0.2892 Explore P: 0.5714\n",
      "Episode: 1283 Total reward: -115.99224853515625 Training loss: 0.2430 Explore P: 0.5712\n",
      "Episode: 1284 Total reward: -42.55633544921875 Training loss: 0.3906 Explore P: 0.5710\n",
      "Episode: 1285 Total reward: -24.792648315429688 Training loss: 3.4585 Explore P: 0.5706\n",
      "Model Saved\n",
      "Episode: 1286 Total reward: -71.25901794433594 Training loss: 0.4303 Explore P: 0.5704\n",
      "Episode: 1287 Total reward: -58.8426513671875 Training loss: 0.2855 Explore P: 0.5702\n",
      "Episode: 1288 Total reward: -71.99125671386719 Training loss: 1.2496 Explore P: 0.5699\n",
      "Episode: 1289 Total reward: -45.8504638671875 Training loss: 0.2208 Explore P: 0.5696\n",
      "Episode: 1290 Total reward: -27.552749633789062 Training loss: 0.3321 Explore P: 0.5692\n",
      "Model Saved\n",
      "Episode: 1291 Total reward: -93.95440673828125 Training loss: 9.9690 Explore P: 0.5690\n",
      "Episode: 1292 Total reward: -49.82421875 Training loss: 1.1180 Explore P: 0.5688\n",
      "Episode: 1293 Total reward: -78.57400512695312 Training loss: 0.8018 Explore P: 0.5685\n",
      "Episode: 1294 Total reward: -74.69845581054688 Training loss: 7.7262 Explore P: 0.5683\n",
      "Episode: 1295 Total reward: -21.479324340820312 Training loss: 0.3260 Explore P: 0.5681\n",
      "Model Saved\n",
      "Episode: 1296 Total reward: -29.902206420898438 Training loss: 0.2204 Explore P: 0.5679\n",
      "Episode: 1297 Total reward: -66.52177429199219 Training loss: 0.2249 Explore P: 0.5676\n",
      "Episode: 1298 Total reward: -83.98228454589844 Training loss: 0.3061 Explore P: 0.5674\n",
      "Episode: 1299 Total reward: -99.73121643066406 Training loss: 12.8630 Explore P: 0.5672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1300 Total reward: -55.2110595703125 Training loss: 0.3056 Explore P: 0.5670\n",
      "Model Saved\n",
      "Episode: 1301 Total reward: -100.72206115722656 Training loss: 0.6914 Explore P: 0.5667\n",
      "Episode: 1302 Total reward: -70.04103088378906 Training loss: 0.3193 Explore P: 0.5666\n",
      "Episode: 1303 Total reward: -49.95391845703125 Training loss: 0.5045 Explore P: 0.5664\n",
      "Episode: 1304 Total reward: -54.73457336425781 Training loss: 0.6541 Explore P: 0.5661\n",
      "Episode: 1305 Total reward: -93.03265380859375 Training loss: 0.2391 Explore P: 0.5659\n",
      "Model Saved\n",
      "Episode: 1306 Total reward: 12.479217529296875 Training loss: 0.2057 Explore P: 0.5657\n",
      "Episode: 1307 Total reward: -75.61476135253906 Training loss: 14.0661 Explore P: 0.5656\n",
      "Episode: 1308 Total reward: -71.11785888671875 Training loss: 12.0187 Explore P: 0.5654\n",
      "Episode: 1309 Total reward: -99.31944274902344 Training loss: 0.9118 Explore P: 0.5651\n",
      "Episode: 1310 Total reward: -46.74273681640625 Training loss: 0.1754 Explore P: 0.5649\n",
      "Model Saved\n",
      "Episode: 1311 Total reward: -89.59864807128906 Training loss: 0.7135 Explore P: 0.5646\n",
      "Episode: 1312 Total reward: -53.57872009277344 Training loss: 0.4671 Explore P: 0.5644\n",
      "Episode: 1313 Total reward: -84.8857421875 Training loss: 0.3632 Explore P: 0.5642\n",
      "Episode: 1314 Total reward: -59.562286376953125 Training loss: 1.6595 Explore P: 0.5638\n",
      "Episode: 1315 Total reward: -46.5570068359375 Training loss: 0.1674 Explore P: 0.5636\n",
      "Model Saved\n",
      "Episode: 1316 Total reward: -46.092376708984375 Training loss: 0.7302 Explore P: 0.5633\n",
      "Episode: 1317 Total reward: -56.60768127441406 Training loss: 0.3809 Explore P: 0.5631\n",
      "Episode: 1318 Total reward: -51.47425842285156 Training loss: 0.7434 Explore P: 0.5629\n",
      "Episode: 1319 Total reward: -38.80360412597656 Training loss: 0.1975 Explore P: 0.5627\n",
      "Episode: 1320 Total reward: -73.35624694824219 Training loss: 0.1775 Explore P: 0.5624\n",
      "Model Saved\n",
      "Episode: 1321 Total reward: -70.40521240234375 Training loss: 0.6839 Explore P: 0.5622\n",
      "Episode: 1322 Total reward: -63.60371398925781 Training loss: 0.1744 Explore P: 0.5620\n",
      "Episode: 1323 Total reward: -92.39132690429688 Training loss: 0.2381 Explore P: 0.5618\n",
      "Episode: 1324 Total reward: -2.9534759521484375 Training loss: 0.1728 Explore P: 0.5616\n",
      "Episode: 1325 Total reward: -41.11366271972656 Training loss: 0.3722 Explore P: 0.5614\n",
      "Model Saved\n",
      "Episode: 1326 Total reward: -115.78546142578125 Training loss: 0.2129 Explore P: 0.5612\n",
      "Episode: 1327 Total reward: -19.80523681640625 Training loss: 0.3468 Explore P: 0.5610\n",
      "Episode: 1328 Total reward: -69.62547302246094 Training loss: 0.7064 Explore P: 0.5607\n",
      "Episode: 1329 Total reward: -115.22438049316406 Training loss: 0.4870 Explore P: 0.5605\n",
      "Episode: 1330 Total reward: -39.77897644042969 Training loss: 0.5154 Explore P: 0.5603\n",
      "Model Saved\n",
      "Episode: 1331 Total reward: -97.17807006835938 Training loss: 0.2142 Explore P: 0.5601\n",
      "Episode: 1332 Total reward: -68.49040222167969 Training loss: 1.1181 Explore P: 0.5598\n",
      "Episode: 1333 Total reward: -46.312347412109375 Training loss: 0.2934 Explore P: 0.5596\n",
      "Episode: 1334 Total reward: 35.44081115722656 Training loss: 0.3902 Explore P: 0.5594\n",
      "Episode: 1335 Total reward: -35.894805908203125 Training loss: 0.6150 Explore P: 0.5592\n",
      "Model Saved\n",
      "Episode: 1336 Total reward: -54.30235290527344 Training loss: 0.6327 Explore P: 0.5589\n",
      "Episode: 1337 Total reward: -72.11801147460938 Training loss: 0.3203 Explore P: 0.5588\n",
      "Episode: 1338 Total reward: -88.204345703125 Training loss: 0.7343 Explore P: 0.5587\n",
      "Episode: 1339 Total reward: -91.77877807617188 Training loss: 0.4132 Explore P: 0.5584\n",
      "Episode: 1340 Total reward: -107.885986328125 Training loss: 0.3325 Explore P: 0.5582\n",
      "Model Saved\n",
      "Episode: 1341 Total reward: -82.2564697265625 Training loss: 0.8926 Explore P: 0.5581\n",
      "Episode: 1342 Total reward: -54.23101806640625 Training loss: 0.9983 Explore P: 0.5579\n",
      "Episode: 1343 Total reward: -62.729400634765625 Training loss: 0.2932 Explore P: 0.5576\n",
      "Episode: 1344 Total reward: -92.77569580078125 Training loss: 2.6849 Explore P: 0.5573\n",
      "Episode: 1345 Total reward: -36.69136047363281 Training loss: 0.2633 Explore P: 0.5571\n",
      "Model Saved\n",
      "Episode: 1346 Total reward: -45.672607421875 Training loss: 4.6358 Explore P: 0.5569\n",
      "Episode: 1347 Total reward: -56.59608459472656 Training loss: 10.5206 Explore P: 0.5567\n",
      "Episode: 1348 Total reward: -44.942840576171875 Training loss: 0.9715 Explore P: 0.5563\n",
      "Episode: 1349 Total reward: -94.713134765625 Training loss: 2.5838 Explore P: 0.5561\n",
      "Episode: 1350 Total reward: -29.080978393554688 Training loss: 0.1343 Explore P: 0.5559\n",
      "Model Saved\n",
      "Episode: 1351 Total reward: -71.13232421875 Training loss: 0.2257 Explore P: 0.5557\n",
      "Episode: 1352 Total reward: -73.88160705566406 Training loss: 1.0407 Explore P: 0.5553\n",
      "Episode: 1353 Total reward: -70.73127746582031 Training loss: 0.4775 Explore P: 0.5551\n",
      "Episode: 1354 Total reward: -56.747039794921875 Training loss: 0.5720 Explore P: 0.5549\n",
      "Episode: 1355 Total reward: -46.91542053222656 Training loss: 0.9489 Explore P: 0.5547\n",
      "Model Saved\n",
      "Episode: 1356 Total reward: -76.83753967285156 Training loss: 0.2813 Explore P: 0.5545\n",
      "Episode: 1357 Total reward: -36.372772216796875 Training loss: 0.2644 Explore P: 0.5543\n",
      "Episode: 1358 Total reward: -86.91748046875 Training loss: 0.3566 Explore P: 0.5541\n",
      "Episode: 1359 Total reward: -97.795654296875 Training loss: 1.2200 Explore P: 0.5539\n",
      "Episode: 1360 Total reward: -48.00328063964844 Training loss: 7.2506 Explore P: 0.5537\n",
      "Model Saved\n",
      "Episode: 1361 Total reward: 11.585586547851562 Training loss: 0.8680 Explore P: 0.5535\n",
      "Episode: 1362 Total reward: -87.44273376464844 Training loss: 10.7357 Explore P: 0.5532\n",
      "Episode: 1363 Total reward: -43.109954833984375 Training loss: 0.5540 Explore P: 0.5531\n",
      "Episode: 1364 Total reward: -45.876617431640625 Training loss: 8.9937 Explore P: 0.5529\n",
      "Episode: 1365 Total reward: -22.597640991210938 Training loss: 3.6516 Explore P: 0.5527\n",
      "Model Saved\n",
      "Episode: 1366 Total reward: -24.73736572265625 Training loss: 1.1413 Explore P: 0.5524\n",
      "Episode: 1367 Total reward: -45.731719970703125 Training loss: 0.2965 Explore P: 0.5522\n",
      "Episode: 1368 Total reward: -76.34284973144531 Training loss: 0.2725 Explore P: 0.5521\n",
      "Episode: 1369 Total reward: 6.6275787353515625 Training loss: 0.4069 Explore P: 0.5519\n",
      "Episode: 1370 Total reward: -55.45904541015625 Training loss: 0.2502 Explore P: 0.5516\n",
      "Model Saved\n",
      "Episode: 1371 Total reward: -42.151397705078125 Training loss: 0.2813 Explore P: 0.5514\n",
      "Episode: 1372 Total reward: -69.62893676757812 Training loss: 0.4008 Explore P: 0.5512\n",
      "Episode: 1373 Total reward: -50.69966125488281 Training loss: 7.9862 Explore P: 0.5509\n",
      "Episode: 1374 Total reward: -92.39471435546875 Training loss: 0.2846 Explore P: 0.5507\n",
      "Episode: 1375 Total reward: -49.55799865722656 Training loss: 0.5479 Explore P: 0.5505\n",
      "Model Saved\n",
      "Episode: 1376 Total reward: -82.22439575195312 Training loss: 0.1892 Explore P: 0.5503\n",
      "Episode: 1377 Total reward: -75.3489990234375 Training loss: 0.2536 Explore P: 0.5502\n",
      "Episode: 1378 Total reward: -43.09068298339844 Training loss: 0.6391 Explore P: 0.5499\n",
      "Episode: 1379 Total reward: -80.13026428222656 Training loss: 0.6938 Explore P: 0.5497\n",
      "Episode: 1380 Total reward: -45.56475830078125 Training loss: 0.1629 Explore P: 0.5495\n",
      "Model Saved\n",
      "Episode: 1381 Total reward: -100.11477661132812 Training loss: 14.1217 Explore P: 0.5493\n",
      "Episode: 1382 Total reward: -95.43272399902344 Training loss: 0.1507 Explore P: 0.5490\n",
      "Episode: 1383 Total reward: 18.385269165039062 Training loss: 1.6275 Explore P: 0.5487\n",
      "Episode: 1384 Total reward: -66.31832885742188 Training loss: 1.3524 Explore P: 0.5485\n",
      "Episode: 1385 Total reward: -6.11956787109375 Training loss: 0.9657 Explore P: 0.5483\n",
      "Model Saved\n",
      "Episode: 1386 Total reward: -23.340164184570312 Training loss: 0.2635 Explore P: 0.5481\n",
      "Episode: 1387 Total reward: -53.13716125488281 Training loss: 0.3494 Explore P: 0.5479\n",
      "Episode: 1388 Total reward: -49.90812683105469 Training loss: 0.7376 Explore P: 0.5477\n",
      "Episode: 1389 Total reward: 42.634796142578125 Training loss: 0.2813 Explore P: 0.5474\n",
      "Episode: 1390 Total reward: -43.90464782714844 Training loss: 0.8188 Explore P: 0.5472\n",
      "Model Saved\n",
      "Episode: 1391 Total reward: -33.76863098144531 Training loss: 0.7979 Explore P: 0.5470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1392 Total reward: -95.25445556640625 Training loss: 0.2091 Explore P: 0.5469\n",
      "Episode: 1393 Total reward: -69.1083984375 Training loss: 1.6935 Explore P: 0.5467\n",
      "Episode: 1394 Total reward: -45.42927551269531 Training loss: 1.1948 Explore P: 0.5464\n",
      "Episode: 1395 Total reward: -5.01141357421875 Training loss: 0.9202 Explore P: 0.5462\n",
      "Model Saved\n",
      "Episode: 1396 Total reward: -42.57720947265625 Training loss: 0.1965 Explore P: 0.5460\n",
      "Episode: 1397 Total reward: -15.10198974609375 Training loss: 0.2679 Explore P: 0.5458\n",
      "Episode: 1398 Total reward: 5.1365966796875 Training loss: 0.5015 Explore P: 0.5456\n",
      "Episode: 1399 Total reward: -76.43162536621094 Training loss: 1.0853 Explore P: 0.5453\n",
      "Episode: 1400 Total reward: -71.07650756835938 Training loss: 0.2530 Explore P: 0.5451\n",
      "Model Saved\n",
      "Episode: 1401 Total reward: -87.95417785644531 Training loss: 1.0493 Explore P: 0.5448\n",
      "Episode: 1402 Total reward: -41.091217041015625 Training loss: 0.4042 Explore P: 0.5447\n",
      "Episode: 1403 Total reward: -45.525726318359375 Training loss: 1.2104 Explore P: 0.5445\n",
      "Episode: 1404 Total reward: -84.46687316894531 Training loss: 0.1883 Explore P: 0.5444\n",
      "Episode: 1405 Total reward: -65.91424560546875 Training loss: 0.5166 Explore P: 0.5441\n",
      "Model Saved\n",
      "Episode: 1406 Total reward: -46.63612365722656 Training loss: 0.8886 Explore P: 0.5439\n",
      "Episode: 1407 Total reward: -46.7337646484375 Training loss: 0.4484 Explore P: 0.5437\n",
      "Episode: 1408 Total reward: -56.636627197265625 Training loss: 0.5679 Explore P: 0.5435\n",
      "Episode: 1409 Total reward: -58.4215087890625 Training loss: 0.3781 Explore P: 0.5433\n",
      "Episode: 1410 Total reward: -40.238433837890625 Training loss: 0.2957 Explore P: 0.5431\n",
      "Model Saved\n",
      "Episode: 1411 Total reward: -36.92205810546875 Training loss: 0.1978 Explore P: 0.5429\n",
      "Episode: 1412 Total reward: -52.016357421875 Training loss: 0.1537 Explore P: 0.5427\n",
      "Episode: 1413 Total reward: -92.10099792480469 Training loss: 0.6535 Explore P: 0.5425\n",
      "Episode: 1414 Total reward: -57.688446044921875 Training loss: 1.2832 Explore P: 0.5423\n",
      "Episode: 1415 Total reward: -84.52333068847656 Training loss: 0.3393 Explore P: 0.5420\n",
      "Model Saved\n",
      "Episode: 1416 Total reward: -46.79887390136719 Training loss: 0.6360 Explore P: 0.5418\n",
      "Episode: 1417 Total reward: -55.08433532714844 Training loss: 0.5384 Explore P: 0.5415\n",
      "Episode: 1418 Total reward: -93.63735961914062 Training loss: 4.0816 Explore P: 0.5412\n",
      "Episode: 1419 Total reward: -108.53126525878906 Training loss: 0.2269 Explore P: 0.5410\n",
      "Episode: 1420 Total reward: -55.261688232421875 Training loss: 0.2571 Explore P: 0.5408\n",
      "Model Saved\n",
      "Episode: 1421 Total reward: -10.081069946289062 Training loss: 7.2807 Explore P: 0.5406\n",
      "Episode: 1422 Total reward: -59.20701599121094 Training loss: 0.3093 Explore P: 0.5403\n",
      "Episode: 1423 Total reward: -57.8519287109375 Training loss: 1.1766 Explore P: 0.5401\n",
      "Episode: 1424 Total reward: -5.2872772216796875 Training loss: 0.2368 Explore P: 0.5399\n",
      "Episode: 1425 Total reward: -53.644927978515625 Training loss: 0.4664 Explore P: 0.5397\n",
      "Model Saved\n",
      "Episode: 1426 Total reward: -103.73643493652344 Training loss: 0.2336 Explore P: 0.5395\n",
      "Episode: 1427 Total reward: -41.01576232910156 Training loss: 0.1912 Explore P: 0.5392\n",
      "Episode: 1428 Total reward: -50.05015563964844 Training loss: 0.2691 Explore P: 0.5390\n",
      "Episode: 1429 Total reward: -112.54953002929688 Training loss: 0.9133 Explore P: 0.5388\n",
      "Episode: 1430 Total reward: -37.46754455566406 Training loss: 8.7851 Explore P: 0.5386\n",
      "Model Saved\n",
      "Episode: 1431 Total reward: -80.49165344238281 Training loss: 0.6030 Explore P: 0.5384\n",
      "Episode: 1432 Total reward: -73.47430419921875 Training loss: 0.2194 Explore P: 0.5381\n",
      "Episode: 1433 Total reward: -64.10504150390625 Training loss: 0.1219 Explore P: 0.5380\n",
      "Episode: 1434 Total reward: -72.96827697753906 Training loss: 0.7667 Explore P: 0.5379\n",
      "Episode: 1435 Total reward: -53.80595397949219 Training loss: 0.2105 Explore P: 0.5377\n",
      "Model Saved\n",
      "Episode: 1436 Total reward: -69.78450012207031 Training loss: 0.1362 Explore P: 0.5375\n",
      "Episode: 1437 Total reward: -51.5819091796875 Training loss: 0.2245 Explore P: 0.5373\n",
      "Episode: 1438 Total reward: -100.04611206054688 Training loss: 0.5609 Explore P: 0.5370\n",
      "Episode: 1439 Total reward: -7.2529296875 Training loss: 1.7154 Explore P: 0.5368\n",
      "Episode: 1440 Total reward: -46.669158935546875 Training loss: 0.3040 Explore P: 0.5366\n",
      "Model Saved\n",
      "Episode: 1441 Total reward: 33.4830322265625 Training loss: 10.3450 Explore P: 0.5363\n",
      "Episode: 1442 Total reward: -15.975662231445312 Training loss: 6.7849 Explore P: 0.5361\n",
      "Episode: 1443 Total reward: -71.56219482421875 Training loss: 16.3714 Explore P: 0.5359\n",
      "Episode: 1444 Total reward: -77.54917907714844 Training loss: 0.5435 Explore P: 0.5355\n",
      "Episode: 1445 Total reward: -22.777542114257812 Training loss: 0.4066 Explore P: 0.5353\n",
      "Model Saved\n",
      "Episode: 1446 Total reward: -48.852020263671875 Training loss: 1.1255 Explore P: 0.5351\n",
      "Episode: 1447 Total reward: -94.89846801757812 Training loss: 6.8248 Explore P: 0.5349\n",
      "Episode: 1448 Total reward: -38.44294738769531 Training loss: 2.8735 Explore P: 0.5346\n",
      "Episode: 1449 Total reward: -38.2857666015625 Training loss: 0.2215 Explore P: 0.5344\n",
      "Episode: 1450 Total reward: -69.97640991210938 Training loss: 0.2222 Explore P: 0.5341\n",
      "Model Saved\n",
      "Episode: 1451 Total reward: -46.11155700683594 Training loss: 1.0666 Explore P: 0.5339\n",
      "Episode: 1452 Total reward: -65.00434875488281 Training loss: 6.2320 Explore P: 0.5337\n",
      "Episode: 1453 Total reward: -61.1229248046875 Training loss: 1.7579 Explore P: 0.5335\n",
      "Episode: 1454 Total reward: -34.357452392578125 Training loss: 0.3646 Explore P: 0.5333\n",
      "Episode: 1455 Total reward: -28.97113037109375 Training loss: 0.2131 Explore P: 0.5331\n",
      "Model Saved\n",
      "Episode: 1456 Total reward: -77.68104553222656 Training loss: 0.9707 Explore P: 0.5328\n",
      "Episode: 1457 Total reward: -8.653640747070312 Training loss: 16.7883 Explore P: 0.5326\n",
      "Episode: 1458 Total reward: -62.39585876464844 Training loss: 13.7903 Explore P: 0.5324\n",
      "Episode: 1459 Total reward: -84.38018798828125 Training loss: 3.4808 Explore P: 0.5322\n",
      "Episode: 1460 Total reward: 5.175933837890625 Training loss: 0.5617 Explore P: 0.5320\n",
      "Model Saved\n",
      "Episode: 1461 Total reward: -37.970306396484375 Training loss: 1.4114 Explore P: 0.5318\n",
      "Episode: 1462 Total reward: -3.7409210205078125 Training loss: 0.1775 Explore P: 0.5316\n",
      "Episode: 1463 Total reward: -40.227325439453125 Training loss: 0.5345 Explore P: 0.5314\n",
      "Episode: 1464 Total reward: -102.90863037109375 Training loss: 0.1979 Explore P: 0.5311\n",
      "Episode: 1465 Total reward: -50.83912658691406 Training loss: 0.2713 Explore P: 0.5309\n",
      "Model Saved\n",
      "Episode: 1466 Total reward: -51.84002685546875 Training loss: 0.6525 Explore P: 0.5307\n",
      "Episode: 1467 Total reward: -54.86427307128906 Training loss: 0.2245 Explore P: 0.5304\n",
      "Episode: 1468 Total reward: -66.76585388183594 Training loss: 0.2305 Explore P: 0.5302\n",
      "Episode: 1469 Total reward: -72.80064392089844 Training loss: 0.4125 Explore P: 0.5299\n",
      "Episode: 1470 Total reward: -102.15034484863281 Training loss: 13.0911 Explore P: 0.5298\n",
      "Model Saved\n",
      "Episode: 1471 Total reward: -37.56660461425781 Training loss: 1.0657 Explore P: 0.5296\n",
      "Episode: 1472 Total reward: -29.74798583984375 Training loss: 1.1678 Explore P: 0.5294\n",
      "Episode: 1473 Total reward: -40.98548889160156 Training loss: 0.2294 Explore P: 0.5291\n",
      "Episode: 1474 Total reward: -113.72184753417969 Training loss: 2.3349 Explore P: 0.5290\n",
      "Episode: 1475 Total reward: -31.257766723632812 Training loss: 0.2823 Explore P: 0.5288\n",
      "Model Saved\n",
      "Episode: 1476 Total reward: -70.66374206542969 Training loss: 0.3819 Explore P: 0.5286\n",
      "Episode: 1477 Total reward: -2.0650482177734375 Training loss: 0.2117 Explore P: 0.5284\n",
      "Episode: 1478 Total reward: -39.59210205078125 Training loss: 0.3892 Explore P: 0.5282\n",
      "Episode: 1479 Total reward: -34.0050048828125 Training loss: 0.3245 Explore P: 0.5280\n",
      "Episode: 1480 Total reward: 10.077865600585938 Training loss: 0.2256 Explore P: 0.5278\n",
      "Model Saved\n",
      "Episode: 1481 Total reward: -77.77886962890625 Training loss: 1.1368 Explore P: 0.5276\n",
      "Episode: 1482 Total reward: -28.406356811523438 Training loss: 0.2354 Explore P: 0.5274\n",
      "Episode: 1483 Total reward: -40.879913330078125 Training loss: 2.2092 Explore P: 0.5272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1484 Total reward: -45.91960144042969 Training loss: 14.1671 Explore P: 0.5270\n",
      "Episode: 1485 Total reward: -95.59530639648438 Training loss: 0.5708 Explore P: 0.5268\n",
      "Model Saved\n",
      "Episode: 1486 Total reward: -112.10321044921875 Training loss: 0.3573 Explore P: 0.5266\n",
      "Episode: 1487 Total reward: 0.5525665283203125 Training loss: 10.3311 Explore P: 0.5264\n",
      "Episode: 1488 Total reward: -45.32438659667969 Training loss: 0.9388 Explore P: 0.5262\n",
      "Episode: 1489 Total reward: -13.90594482421875 Training loss: 0.7263 Explore P: 0.5259\n",
      "Episode: 1490 Total reward: -71.04579162597656 Training loss: 0.4714 Explore P: 0.5257\n",
      "Model Saved\n",
      "Episode: 1491 Total reward: -16.681304931640625 Training loss: 0.2321 Explore P: 0.5255\n",
      "Episode: 1492 Total reward: -109.56668090820312 Training loss: 0.3792 Explore P: 0.5252\n",
      "Episode: 1493 Total reward: -84.73527526855469 Training loss: 0.7702 Explore P: 0.5250\n",
      "Episode: 1494 Total reward: -46.289093017578125 Training loss: 0.2015 Explore P: 0.5248\n",
      "Episode: 1495 Total reward: -54.4947509765625 Training loss: 0.8249 Explore P: 0.5246\n",
      "Model Saved\n",
      "Episode: 1496 Total reward: -17.758941650390625 Training loss: 2.7927 Explore P: 0.5243\n",
      "Episode: 1497 Total reward: -56.65888977050781 Training loss: 0.2660 Explore P: 0.5241\n",
      "Episode: 1498 Total reward: -72.43051147460938 Training loss: 0.2824 Explore P: 0.5239\n",
      "Episode: 1499 Total reward: -67.32431030273438 Training loss: 0.6076 Explore P: 0.5237\n",
      "Episode: 1500 Total reward: -57.729095458984375 Training loss: 0.2044 Explore P: 0.5236\n",
      "Model Saved\n",
      "Episode: 1501 Total reward: -60.00262451171875 Training loss: 3.0426 Explore P: 0.5234\n",
      "Episode: 1502 Total reward: -39.120330810546875 Training loss: 0.2590 Explore P: 0.5232\n",
      "Episode: 1503 Total reward: -24.333343505859375 Training loss: 0.2442 Explore P: 0.5230\n",
      "Episode: 1504 Total reward: -55.30723571777344 Training loss: 5.5875 Explore P: 0.5228\n",
      "Episode: 1505 Total reward: -63.5810546875 Training loss: 0.3353 Explore P: 0.5226\n",
      "Model Saved\n",
      "Episode: 1506 Total reward: -70.511474609375 Training loss: 0.3662 Explore P: 0.5225\n",
      "Episode: 1507 Total reward: -61.642181396484375 Training loss: 0.6782 Explore P: 0.5222\n",
      "Episode: 1508 Total reward: -41.79350280761719 Training loss: 10.9478 Explore P: 0.5220\n",
      "Episode: 1509 Total reward: -59.51142883300781 Training loss: 0.2335 Explore P: 0.5218\n",
      "Episode: 1510 Total reward: 11.794586181640625 Training loss: 0.5093 Explore P: 0.5216\n",
      "Model Saved\n",
      "Episode: 1511 Total reward: -40.158447265625 Training loss: 2.6665 Explore P: 0.5214\n",
      "Episode: 1512 Total reward: -54.93360900878906 Training loss: 0.4837 Explore P: 0.5212\n",
      "Episode: 1513 Total reward: -11.944412231445312 Training loss: 1.0138 Explore P: 0.5210\n",
      "Episode: 1514 Total reward: -46.024383544921875 Training loss: 0.6130 Explore P: 0.5208\n",
      "Episode: 1515 Total reward: -46.17060852050781 Training loss: 1.6113 Explore P: 0.5206\n",
      "Model Saved\n",
      "Episode: 1516 Total reward: -64.03518676757812 Training loss: 0.2284 Explore P: 0.5204\n",
      "Episode: 1517 Total reward: -67.13844299316406 Training loss: 0.3949 Explore P: 0.5202\n",
      "Episode: 1518 Total reward: -90.21220397949219 Training loss: 9.4454 Explore P: 0.5198\n",
      "Episode: 1519 Total reward: 25.049774169921875 Training loss: 0.3560 Explore P: 0.5196\n",
      "Episode: 1520 Total reward: -23.48565673828125 Training loss: 1.1381 Explore P: 0.5194\n",
      "Model Saved\n",
      "Episode: 1521 Total reward: -88.36764526367188 Training loss: 0.8034 Explore P: 0.5193\n",
      "Episode: 1522 Total reward: -34.28338623046875 Training loss: 0.3923 Explore P: 0.5191\n",
      "Episode: 1523 Total reward: -89.94642639160156 Training loss: 1.8651 Explore P: 0.5190\n",
      "Episode: 1524 Total reward: -60.00596618652344 Training loss: 0.3571 Explore P: 0.5188\n",
      "Episode: 1525 Total reward: 5.72918701171875 Training loss: 14.9882 Explore P: 0.5186\n",
      "Model Saved\n",
      "Episode: 1526 Total reward: -103.65635681152344 Training loss: 0.2026 Explore P: 0.5184\n",
      "Episode: 1527 Total reward: -62.40788269042969 Training loss: 0.2720 Explore P: 0.5182\n",
      "Episode: 1528 Total reward: -43.422119140625 Training loss: 0.5976 Explore P: 0.5180\n",
      "Episode: 1529 Total reward: -51.141754150390625 Training loss: 0.2469 Explore P: 0.5178\n",
      "Episode: 1530 Total reward: -64.43913269042969 Training loss: 0.2071 Explore P: 0.5176\n",
      "Model Saved\n",
      "Episode: 1531 Total reward: -93.33914184570312 Training loss: 0.4597 Explore P: 0.5175\n",
      "Episode: 1532 Total reward: -77.99874877929688 Training loss: 0.1894 Explore P: 0.5173\n",
      "Episode: 1533 Total reward: -2.6929168701171875 Training loss: 1.2256 Explore P: 0.5171\n",
      "Episode: 1534 Total reward: -25.202362060546875 Training loss: 0.3839 Explore P: 0.5169\n",
      "Episode: 1535 Total reward: -114.29232788085938 Training loss: 1.0080 Explore P: 0.5168\n",
      "Model Saved\n",
      "Episode: 1536 Total reward: -82.65524291992188 Training loss: 0.3797 Explore P: 0.5166\n",
      "Episode: 1537 Total reward: -33.889892578125 Training loss: 0.2096 Explore P: 0.5164\n",
      "Episode: 1538 Total reward: -75.56486511230469 Training loss: 0.3782 Explore P: 0.5162\n",
      "Episode: 1539 Total reward: -46.930328369140625 Training loss: 0.3254 Explore P: 0.5160\n",
      "Episode: 1540 Total reward: -61.170745849609375 Training loss: 0.2376 Explore P: 0.5158\n",
      "Model Saved\n",
      "Episode: 1541 Total reward: -91.08584594726562 Training loss: 0.3304 Explore P: 0.5156\n",
      "Episode: 1542 Total reward: -52.98942565917969 Training loss: 0.4504 Explore P: 0.5154\n",
      "Episode: 1543 Total reward: -65.04054260253906 Training loss: 0.3016 Explore P: 0.5152\n",
      "Episode: 1544 Total reward: -27.876632690429688 Training loss: 1.4272 Explore P: 0.5150\n",
      "Episode: 1545 Total reward: -3.3173675537109375 Training loss: 1.0305 Explore P: 0.5148\n",
      "Model Saved\n",
      "Episode: 1546 Total reward: -49.22856140136719 Training loss: 6.2755 Explore P: 0.5146\n",
      "Episode: 1547 Total reward: -42.37835693359375 Training loss: 14.5599 Explore P: 0.5144\n",
      "Episode: 1548 Total reward: -28.579452514648438 Training loss: 0.6433 Explore P: 0.5142\n",
      "Episode: 1549 Total reward: -64.11503601074219 Training loss: 0.2105 Explore P: 0.5140\n",
      "Episode: 1550 Total reward: -8.764541625976562 Training loss: 0.6655 Explore P: 0.5138\n",
      "Model Saved\n",
      "Episode: 1551 Total reward: 55.777435302734375 Training loss: 1.0070 Explore P: 0.5135\n",
      "Episode: 1552 Total reward: 26.419769287109375 Training loss: 0.2570 Explore P: 0.5133\n",
      "Episode: 1553 Total reward: 10.040679931640625 Training loss: 0.3052 Explore P: 0.5131\n",
      "Episode: 1554 Total reward: 35.40391540527344 Training loss: 0.2523 Explore P: 0.5129\n",
      "Episode: 1555 Total reward: -4.41021728515625 Training loss: 0.1614 Explore P: 0.5127\n",
      "Model Saved\n",
      "Episode: 1556 Total reward: -30.000106811523438 Training loss: 2.8358 Explore P: 0.5125\n",
      "Episode: 1557 Total reward: -32.31455993652344 Training loss: 1.4993 Explore P: 0.5123\n",
      "Episode: 1558 Total reward: -57.679779052734375 Training loss: 0.4469 Explore P: 0.5121\n",
      "Episode: 1559 Total reward: -72.92916870117188 Training loss: 0.2523 Explore P: 0.5119\n",
      "Episode: 1560 Total reward: -4.2479705810546875 Training loss: 0.4734 Explore P: 0.5117\n",
      "Model Saved\n",
      "Episode: 1561 Total reward: -108.47531127929688 Training loss: 0.4311 Explore P: 0.5115\n",
      "Episode: 1562 Total reward: 6.6226959228515625 Training loss: 0.6385 Explore P: 0.5113\n",
      "Episode: 1563 Total reward: -70.32208251953125 Training loss: 0.6633 Explore P: 0.5112\n",
      "Episode: 1564 Total reward: 3.677032470703125 Training loss: 0.2888 Explore P: 0.5110\n",
      "Episode: 1565 Total reward: -80.85536193847656 Training loss: 0.3444 Explore P: 0.5108\n",
      "Model Saved\n",
      "Episode: 1566 Total reward: -17.692138671875 Training loss: 0.6757 Explore P: 0.5106\n",
      "Episode: 1567 Total reward: -64.09033203125 Training loss: 10.1688 Explore P: 0.5105\n",
      "Episode: 1568 Total reward: -89.82496643066406 Training loss: 0.2982 Explore P: 0.5103\n",
      "Episode: 1569 Total reward: -56.91053771972656 Training loss: 1.1849 Explore P: 0.5101\n",
      "Episode: 1570 Total reward: -57.608428955078125 Training loss: 0.2529 Explore P: 0.5099\n",
      "Model Saved\n",
      "Episode: 1571 Total reward: -18.074447631835938 Training loss: 0.3255 Explore P: 0.5097\n",
      "Episode: 1572 Total reward: 8.3092041015625 Training loss: 0.3622 Explore P: 0.5095\n",
      "Episode: 1573 Total reward: -6.2744598388671875 Training loss: 0.3061 Explore P: 0.5093\n",
      "Episode: 1574 Total reward: -17.18170166015625 Training loss: 0.2456 Explore P: 0.5091\n",
      "Episode: 1575 Total reward: 7.537200927734375 Training loss: 8.3178 Explore P: 0.5088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Episode: 1576 Total reward: -54.39631652832031 Training loss: 0.2631 Explore P: 0.5086\n",
      "Episode: 1577 Total reward: -78.21131896972656 Training loss: 0.6128 Explore P: 0.5084\n",
      "Episode: 1578 Total reward: -56.736907958984375 Training loss: 0.1944 Explore P: 0.5082\n",
      "Episode: 1579 Total reward: -24.927291870117188 Training loss: 0.4871 Explore P: 0.5080\n",
      "Episode: 1580 Total reward: -79.22634887695312 Training loss: 0.4020 Explore P: 0.5078\n",
      "Model Saved\n",
      "Episode: 1581 Total reward: -15.131027221679688 Training loss: 10.8184 Explore P: 0.5076\n",
      "Episode: 1582 Total reward: -6.572357177734375 Training loss: 0.3250 Explore P: 0.5074\n",
      "Episode: 1583 Total reward: -7.88897705078125 Training loss: 0.7160 Explore P: 0.5072\n",
      "Episode: 1584 Total reward: -44.94921875 Training loss: 0.9963 Explore P: 0.5070\n",
      "Episode: 1585 Total reward: -7.37774658203125 Training loss: 0.2452 Explore P: 0.5068\n",
      "Model Saved\n",
      "Episode: 1586 Total reward: 16.663970947265625 Training loss: 0.3529 Explore P: 0.5066\n",
      "Episode: 1587 Total reward: -25.288406372070312 Training loss: 0.4367 Explore P: 0.5064\n",
      "Episode: 1588 Total reward: -0.5849456787109375 Training loss: 0.4474 Explore P: 0.5062\n",
      "Episode: 1589 Total reward: -61.753143310546875 Training loss: 0.2727 Explore P: 0.5059\n",
      "Episode: 1590 Total reward: -56.995697021484375 Training loss: 0.7201 Explore P: 0.5057\n",
      "Model Saved\n",
      "Episode: 1591 Total reward: -2.0611114501953125 Training loss: 0.2453 Explore P: 0.5055\n",
      "Episode: 1592 Total reward: -54.16709899902344 Training loss: 0.2115 Explore P: 0.5053\n",
      "Episode: 1593 Total reward: -34.679107666015625 Training loss: 0.6646 Explore P: 0.5052\n",
      "Episode: 1594 Total reward: -53.66343688964844 Training loss: 1.0861 Explore P: 0.5050\n",
      "Episode: 1595 Total reward: -43.43495178222656 Training loss: 0.4314 Explore P: 0.5047\n",
      "Model Saved\n",
      "Episode: 1596 Total reward: -63.21693420410156 Training loss: 0.1985 Explore P: 0.5046\n",
      "Episode: 1597 Total reward: -63.8409423828125 Training loss: 0.2256 Explore P: 0.5044\n",
      "Episode: 1598 Total reward: -12.883819580078125 Training loss: 0.2559 Explore P: 0.5041\n",
      "Episode: 1599 Total reward: -67.614013671875 Training loss: 1.3516 Explore P: 0.5040\n",
      "Episode: 1600 Total reward: -88.10459899902344 Training loss: 0.1772 Explore P: 0.5038\n",
      "Model Saved\n",
      "Episode: 1601 Total reward: 6.6061553955078125 Training loss: 0.2755 Explore P: 0.5036\n",
      "Episode: 1602 Total reward: -45.882781982421875 Training loss: 0.4954 Explore P: 0.5034\n",
      "Episode: 1603 Total reward: -23.796844482421875 Training loss: 17.8854 Explore P: 0.5032\n",
      "Episode: 1604 Total reward: -35.24034118652344 Training loss: 0.3998 Explore P: 0.5030\n",
      "Episode: 1605 Total reward: -74.30494689941406 Training loss: 0.2060 Explore P: 0.5028\n",
      "Model Saved\n",
      "Episode: 1606 Total reward: -66.70448303222656 Training loss: 0.2352 Explore P: 0.5025\n",
      "Episode: 1607 Total reward: -62.91728210449219 Training loss: 0.4401 Explore P: 0.5023\n",
      "Episode: 1608 Total reward: -65.45527648925781 Training loss: 3.1061 Explore P: 0.5021\n",
      "Episode: 1609 Total reward: -40.75898742675781 Training loss: 0.6850 Explore P: 0.5019\n",
      "Episode: 1610 Total reward: -45.79438781738281 Training loss: 1.5883 Explore P: 0.5017\n",
      "Model Saved\n",
      "Episode: 1611 Total reward: -28.252227783203125 Training loss: 0.3725 Explore P: 0.5015\n",
      "Episode: 1612 Total reward: -43.4613037109375 Training loss: 0.6286 Explore P: 0.5013\n",
      "Episode: 1613 Total reward: -58.58941650390625 Training loss: 0.1999 Explore P: 0.5011\n",
      "Episode: 1614 Total reward: -112.52128601074219 Training loss: 0.3236 Explore P: 0.5010\n",
      "Episode: 1615 Total reward: -63.58131408691406 Training loss: 0.4963 Explore P: 0.5009\n",
      "Model Saved\n",
      "Episode: 1616 Total reward: -72.04426574707031 Training loss: 0.5934 Explore P: 0.5007\n",
      "Episode: 1617 Total reward: -114.71372985839844 Training loss: 2.8147 Explore P: 0.5006\n",
      "Episode: 1618 Total reward: -1.3312835693359375 Training loss: 0.9754 Explore P: 0.5004\n",
      "Episode: 1619 Total reward: -1.53765869140625 Training loss: 7.0590 Explore P: 0.5002\n",
      "Episode: 1620 Total reward: -25.730621337890625 Training loss: 0.2074 Explore P: 0.5000\n",
      "Model Saved\n",
      "Episode: 1621 Total reward: -54.481292724609375 Training loss: 1.3765 Explore P: 0.4997\n",
      "Episode: 1622 Total reward: -41.789947509765625 Training loss: 0.5899 Explore P: 0.4994\n",
      "Episode: 1623 Total reward: -59.11668395996094 Training loss: 0.6040 Explore P: 0.4992\n",
      "Episode: 1624 Total reward: -87.90629577636719 Training loss: 0.6818 Explore P: 0.4991\n",
      "Episode: 1625 Total reward: -82.69062805175781 Training loss: 1.5262 Explore P: 0.4988\n",
      "Model Saved\n",
      "Episode: 1626 Total reward: -46.121917724609375 Training loss: 2.2096 Explore P: 0.4986\n",
      "Episode: 1627 Total reward: -11.8211669921875 Training loss: 0.2920 Explore P: 0.4984\n",
      "Episode: 1628 Total reward: 15.990570068359375 Training loss: 0.3008 Explore P: 0.4982\n",
      "Episode: 1629 Total reward: -62.117279052734375 Training loss: 3.7678 Explore P: 0.4980\n",
      "Episode: 1630 Total reward: -24.86810302734375 Training loss: 0.4913 Explore P: 0.4978\n",
      "Model Saved\n",
      "Episode: 1631 Total reward: -13.435089111328125 Training loss: 1.5792 Explore P: 0.4976\n",
      "Episode: 1632 Total reward: -4.7837982177734375 Training loss: 1.3384 Explore P: 0.4974\n",
      "Episode: 1633 Total reward: -42.12449645996094 Training loss: 0.4811 Explore P: 0.4972\n",
      "Episode: 1634 Total reward: -37.612701416015625 Training loss: 0.6272 Explore P: 0.4970\n",
      "Episode: 1635 Total reward: -35.41477966308594 Training loss: 0.3759 Explore P: 0.4969\n",
      "Model Saved\n",
      "Episode: 1636 Total reward: -33.08612060546875 Training loss: 0.8257 Explore P: 0.4968\n",
      "Episode: 1637 Total reward: 43.218505859375 Training loss: 1.3798 Explore P: 0.4966\n",
      "Episode: 1638 Total reward: -92.32359313964844 Training loss: 0.4171 Explore P: 0.4964\n",
      "Episode: 1639 Total reward: -57.45399475097656 Training loss: 0.3434 Explore P: 0.4962\n",
      "Episode: 1640 Total reward: -9.853607177734375 Training loss: 5.5346 Explore P: 0.4960\n",
      "Model Saved\n",
      "Episode: 1641 Total reward: -57.017303466796875 Training loss: 0.4901 Explore P: 0.4958\n",
      "Episode: 1642 Total reward: 52.85575866699219 Training loss: 0.3224 Explore P: 0.4956\n",
      "Episode: 1643 Total reward: -54.58868408203125 Training loss: 15.4849 Explore P: 0.4955\n",
      "Episode: 1644 Total reward: -60.25883483886719 Training loss: 0.2477 Explore P: 0.4953\n",
      "Episode: 1645 Total reward: -5.672821044921875 Training loss: 0.3845 Explore P: 0.4951\n",
      "Model Saved\n",
      "Episode: 1646 Total reward: -97.65037536621094 Training loss: 0.2355 Explore P: 0.4950\n",
      "Episode: 1647 Total reward: -27.444366455078125 Training loss: 0.5737 Explore P: 0.4948\n",
      "Episode: 1648 Total reward: -72.56942749023438 Training loss: 1.1124 Explore P: 0.4947\n",
      "Episode: 1649 Total reward: -74.0966796875 Training loss: 0.6975 Explore P: 0.4946\n",
      "Episode: 1650 Total reward: -71.59527587890625 Training loss: 0.2502 Explore P: 0.4944\n",
      "Model Saved\n",
      "Episode: 1651 Total reward: -33.12757873535156 Training loss: 0.5971 Explore P: 0.4943\n",
      "Episode: 1652 Total reward: -62.31011962890625 Training loss: 1.2388 Explore P: 0.4941\n",
      "Episode: 1653 Total reward: 8.256103515625 Training loss: 0.3984 Explore P: 0.4939\n",
      "Episode: 1654 Total reward: -87.65864562988281 Training loss: 0.2659 Explore P: 0.4938\n",
      "Episode: 1655 Total reward: -23.625259399414062 Training loss: 0.1527 Explore P: 0.4936\n",
      "Model Saved\n",
      "Episode: 1656 Total reward: -32.18693542480469 Training loss: 0.2189 Explore P: 0.4933\n",
      "Episode: 1657 Total reward: -36.93711853027344 Training loss: 0.7428 Explore P: 0.4931\n",
      "Episode: 1658 Total reward: -63.646392822265625 Training loss: 0.4374 Explore P: 0.4930\n",
      "Episode: 1659 Total reward: -3.0407257080078125 Training loss: 0.2947 Explore P: 0.4928\n",
      "Episode: 1660 Total reward: -25.607742309570312 Training loss: 1.6371 Explore P: 0.4926\n",
      "Model Saved\n",
      "Episode: 1661 Total reward: -27.752426147460938 Training loss: 0.3896 Explore P: 0.4923\n",
      "Episode: 1662 Total reward: -59.055389404296875 Training loss: 0.2418 Explore P: 0.4921\n",
      "Episode: 1663 Total reward: -72.74919128417969 Training loss: 0.8806 Explore P: 0.4919\n",
      "Episode: 1664 Total reward: -85.26203918457031 Training loss: 0.1912 Explore P: 0.4918\n",
      "Episode: 1665 Total reward: -65.35020446777344 Training loss: 6.6770 Explore P: 0.4916\n",
      "Model Saved\n",
      "Episode: 1666 Total reward: -36.70587158203125 Training loss: 0.4002 Explore P: 0.4914\n",
      "Episode: 1667 Total reward: -31.538055419921875 Training loss: 0.2621 Explore P: 0.4912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1668 Total reward: 19.347747802734375 Training loss: 0.5436 Explore P: 0.4911\n",
      "Episode: 1669 Total reward: -63.18296813964844 Training loss: 0.2983 Explore P: 0.4909\n",
      "Episode: 1670 Total reward: -23.006378173828125 Training loss: 0.4634 Explore P: 0.4907\n",
      "Model Saved\n",
      "Episode: 1671 Total reward: -27.243301391601562 Training loss: 0.2748 Explore P: 0.4905\n",
      "Episode: 1672 Total reward: 33.71142578125 Training loss: 0.2876 Explore P: 0.4903\n",
      "Episode: 1673 Total reward: -48.82524108886719 Training loss: 15.5684 Explore P: 0.4900\n",
      "Episode: 1674 Total reward: -40.75927734375 Training loss: 1.6972 Explore P: 0.4898\n",
      "Episode: 1675 Total reward: -81.02975463867188 Training loss: 10.4458 Explore P: 0.4897\n",
      "Model Saved\n",
      "Episode: 1676 Total reward: -48.34629821777344 Training loss: 0.5639 Explore P: 0.4895\n",
      "Episode: 1677 Total reward: -45.465545654296875 Training loss: 10.3093 Explore P: 0.4893\n",
      "Episode: 1678 Total reward: 48.04345703125 Training loss: 9.3864 Explore P: 0.4891\n",
      "Episode: 1679 Total reward: 15.947677612304688 Training loss: 0.2044 Explore P: 0.4889\n",
      "Episode: 1680 Total reward: -68.3316650390625 Training loss: 1.1763 Explore P: 0.4888\n",
      "Model Saved\n",
      "Episode: 1681 Total reward: -77.18841552734375 Training loss: 0.7617 Explore P: 0.4886\n",
      "Episode: 1682 Total reward: 87.43325805664062 Training loss: 1.4069 Explore P: 0.4884\n",
      "Episode: 1683 Total reward: -85.85310363769531 Training loss: 11.7149 Explore P: 0.4882\n",
      "Episode: 1684 Total reward: -32.20509338378906 Training loss: 0.1988 Explore P: 0.4880\n",
      "Episode: 1685 Total reward: -70.2593994140625 Training loss: 0.2593 Explore P: 0.4878\n",
      "Model Saved\n",
      "Episode: 1686 Total reward: -52.04991149902344 Training loss: 0.4061 Explore P: 0.4876\n",
      "Episode: 1687 Total reward: -14.0885009765625 Training loss: 1.4662 Explore P: 0.4874\n",
      "Episode: 1688 Total reward: -69.56196594238281 Training loss: 0.2480 Explore P: 0.4872\n",
      "Episode: 1689 Total reward: -27.260894775390625 Training loss: 0.4547 Explore P: 0.4870\n",
      "Episode: 1690 Total reward: -47.14411926269531 Training loss: 0.4713 Explore P: 0.4868\n",
      "Model Saved\n",
      "Episode: 1691 Total reward: -11.144180297851562 Training loss: 1.7342 Explore P: 0.4866\n",
      "Episode: 1692 Total reward: -59.81632995605469 Training loss: 0.2092 Explore P: 0.4864\n",
      "Episode: 1693 Total reward: -5.2868194580078125 Training loss: 0.2276 Explore P: 0.4862\n",
      "Episode: 1694 Total reward: -42.11737060546875 Training loss: 0.7393 Explore P: 0.4860\n",
      "Episode: 1695 Total reward: -60.84220886230469 Training loss: 7.2664 Explore P: 0.4858\n",
      "Model Saved\n",
      "Episode: 1696 Total reward: 4.9786224365234375 Training loss: 0.5945 Explore P: 0.4856\n",
      "Episode: 1697 Total reward: -70.59039306640625 Training loss: 0.2267 Explore P: 0.4854\n",
      "Episode: 1698 Total reward: -36.01548767089844 Training loss: 0.5401 Explore P: 0.4852\n",
      "Episode: 1699 Total reward: -59.32978820800781 Training loss: 0.4807 Explore P: 0.4850\n",
      "Episode: 1700 Total reward: -40.6697998046875 Training loss: 0.3225 Explore P: 0.4848\n",
      "Model Saved\n",
      "Episode: 1701 Total reward: -76.64695739746094 Training loss: 1.1116 Explore P: 0.4846\n",
      "Episode: 1702 Total reward: -53.23393249511719 Training loss: 0.3528 Explore P: 0.4844\n",
      "Episode: 1703 Total reward: -65.58079528808594 Training loss: 6.0800 Explore P: 0.4842\n",
      "Episode: 1704 Total reward: -25.914840698242188 Training loss: 0.1909 Explore P: 0.4840\n",
      "Episode: 1705 Total reward: -41.68769836425781 Training loss: 0.9494 Explore P: 0.4837\n",
      "Model Saved\n",
      "Episode: 1706 Total reward: -50.343048095703125 Training loss: 0.1718 Explore P: 0.4835\n",
      "Episode: 1707 Total reward: -28.13885498046875 Training loss: 0.3184 Explore P: 0.4833\n",
      "Episode: 1708 Total reward: -92.58995056152344 Training loss: 0.3662 Explore P: 0.4832\n",
      "Episode: 1709 Total reward: -70.25390625 Training loss: 1.3223 Explore P: 0.4829\n",
      "Episode: 1710 Total reward: -51.03532409667969 Training loss: 0.6731 Explore P: 0.4827\n",
      "Model Saved\n",
      "Episode: 1711 Total reward: -66.55081176757812 Training loss: 0.3915 Explore P: 0.4826\n",
      "Episode: 1712 Total reward: -50.66851806640625 Training loss: 0.4073 Explore P: 0.4824\n",
      "Episode: 1713 Total reward: -58.81512451171875 Training loss: 0.4775 Explore P: 0.4822\n",
      "Episode: 1714 Total reward: -45.013580322265625 Training loss: 1.0535 Explore P: 0.4820\n",
      "Episode: 1715 Total reward: -71.65248107910156 Training loss: 0.4458 Explore P: 0.4818\n",
      "Model Saved\n",
      "Episode: 1716 Total reward: -10.896820068359375 Training loss: 0.2508 Explore P: 0.4816\n",
      "Episode: 1717 Total reward: -29.30877685546875 Training loss: 0.4376 Explore P: 0.4814\n",
      "Episode: 1718 Total reward: -66.36286926269531 Training loss: 0.3546 Explore P: 0.4813\n",
      "Episode: 1719 Total reward: -22.922622680664062 Training loss: 13.4647 Explore P: 0.4811\n",
      "Episode: 1720 Total reward: -47.153045654296875 Training loss: 12.0285 Explore P: 0.4809\n",
      "Model Saved\n",
      "Episode: 1721 Total reward: -23.10107421875 Training loss: 0.2749 Explore P: 0.4807\n",
      "Episode: 1722 Total reward: -74.47148132324219 Training loss: 17.2531 Explore P: 0.4806\n",
      "Episode: 1723 Total reward: -43.56819152832031 Training loss: 0.8410 Explore P: 0.4804\n",
      "Episode: 1724 Total reward: -25.722915649414062 Training loss: 0.4247 Explore P: 0.4802\n",
      "Episode: 1725 Total reward: -85.63166809082031 Training loss: 0.3550 Explore P: 0.4801\n",
      "Model Saved\n",
      "Episode: 1726 Total reward: -29.468780517578125 Training loss: 0.7198 Explore P: 0.4799\n",
      "Episode: 1727 Total reward: -31.657150268554688 Training loss: 0.4638 Explore P: 0.4797\n",
      "Episode: 1728 Total reward: -67.85594177246094 Training loss: 0.4676 Explore P: 0.4795\n",
      "Episode: 1729 Total reward: -27.334915161132812 Training loss: 1.0724 Explore P: 0.4793\n",
      "Episode: 1730 Total reward: -16.599288940429688 Training loss: 0.3296 Explore P: 0.4791\n",
      "Model Saved\n",
      "Episode: 1731 Total reward: -2.5417327880859375 Training loss: 0.2712 Explore P: 0.4789\n",
      "Episode: 1732 Total reward: -68.18502807617188 Training loss: 0.2612 Explore P: 0.4787\n",
      "Episode: 1733 Total reward: -18.898880004882812 Training loss: 0.2514 Explore P: 0.4785\n",
      "Episode: 1734 Total reward: 8.01849365234375 Training loss: 0.2500 Explore P: 0.4784\n",
      "Episode: 1735 Total reward: -74.37042236328125 Training loss: 0.3352 Explore P: 0.4782\n",
      "Model Saved\n",
      "Episode: 1736 Total reward: -97.2662353515625 Training loss: 0.2096 Explore P: 0.4781\n",
      "Episode: 1737 Total reward: -13.320648193359375 Training loss: 3.6907 Explore P: 0.4779\n",
      "Episode: 1738 Total reward: -4.527587890625 Training loss: 2.3776 Explore P: 0.4777\n",
      "Episode: 1739 Total reward: -59.6365966796875 Training loss: 0.1878 Explore P: 0.4775\n",
      "Episode: 1740 Total reward: -49.93028259277344 Training loss: 1.5529 Explore P: 0.4774\n",
      "Model Saved\n",
      "Episode: 1741 Total reward: -35.45024108886719 Training loss: 0.2140 Explore P: 0.4772\n",
      "Episode: 1742 Total reward: -21.003921508789062 Training loss: 9.4909 Explore P: 0.4770\n",
      "Episode: 1743 Total reward: -91.02688598632812 Training loss: 0.2332 Explore P: 0.4768\n",
      "Episode: 1744 Total reward: -36.77119445800781 Training loss: 0.2849 Explore P: 0.4766\n",
      "Episode: 1745 Total reward: -76.24053955078125 Training loss: 0.2549 Explore P: 0.4764\n",
      "Model Saved\n",
      "Episode: 1746 Total reward: 17.927444458007812 Training loss: 0.2032 Explore P: 0.4762\n",
      "Episode: 1747 Total reward: -71.85926818847656 Training loss: 0.4846 Explore P: 0.4761\n",
      "Episode: 1748 Total reward: 0.1328582763671875 Training loss: 0.3226 Explore P: 0.4759\n",
      "Episode: 1749 Total reward: -86.14744567871094 Training loss: 0.3221 Explore P: 0.4757\n",
      "Episode: 1750 Total reward: -10.906600952148438 Training loss: 0.6734 Explore P: 0.4755\n",
      "Model Saved\n",
      "Episode: 1751 Total reward: -31.927871704101562 Training loss: 0.4576 Explore P: 0.4753\n",
      "Episode: 1752 Total reward: -84.01136779785156 Training loss: 0.4041 Explore P: 0.4752\n",
      "Episode: 1753 Total reward: 47.69328308105469 Training loss: 0.1626 Explore P: 0.4750\n",
      "Episode: 1754 Total reward: -45.537078857421875 Training loss: 0.3401 Explore P: 0.4747\n",
      "Episode: 1755 Total reward: -6.46307373046875 Training loss: 0.2440 Explore P: 0.4745\n",
      "Model Saved\n",
      "Episode: 1756 Total reward: -70.95664978027344 Training loss: 0.1980 Explore P: 0.4742\n",
      "Episode: 1757 Total reward: -48.360015869140625 Training loss: 0.2385 Explore P: 0.4740\n",
      "Episode: 1758 Total reward: -54.351959228515625 Training loss: 3.0401 Explore P: 0.4738\n",
      "Episode: 1759 Total reward: -75.14315795898438 Training loss: 0.2388 Explore P: 0.4736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1760 Total reward: -51.69517517089844 Training loss: 0.6215 Explore P: 0.4735\n",
      "Model Saved\n",
      "Episode: 1761 Total reward: -21.567184448242188 Training loss: 0.2613 Explore P: 0.4732\n",
      "Episode: 1762 Total reward: -17.883743286132812 Training loss: 0.5506 Explore P: 0.4730\n",
      "Episode: 1763 Total reward: -52.482269287109375 Training loss: 8.8640 Explore P: 0.4728\n",
      "Episode: 1764 Total reward: -18.89813232421875 Training loss: 0.1503 Explore P: 0.4726\n",
      "Episode: 1765 Total reward: -68.19035339355469 Training loss: 0.4583 Explore P: 0.4725\n",
      "Model Saved\n",
      "Episode: 1766 Total reward: -72.83242797851562 Training loss: 1.6843 Explore P: 0.4723\n",
      "Episode: 1767 Total reward: 3.027252197265625 Training loss: 0.2805 Explore P: 0.4722\n",
      "Episode: 1768 Total reward: -22.565216064453125 Training loss: 12.3534 Explore P: 0.4720\n",
      "Episode: 1769 Total reward: -100.80328369140625 Training loss: 0.8054 Explore P: 0.4719\n",
      "Episode: 1770 Total reward: -73.239990234375 Training loss: 0.8070 Explore P: 0.4717\n",
      "Model Saved\n",
      "Episode: 1771 Total reward: -30.2733154296875 Training loss: 0.5699 Explore P: 0.4715\n",
      "Episode: 1772 Total reward: -50.61460876464844 Training loss: 1.1319 Explore P: 0.4713\n",
      "Episode: 1773 Total reward: -27.471755981445312 Training loss: 0.8858 Explore P: 0.4711\n",
      "Episode: 1774 Total reward: -41.09153747558594 Training loss: 0.2505 Explore P: 0.4709\n",
      "Episode: 1775 Total reward: -15.621017456054688 Training loss: 1.6706 Explore P: 0.4708\n",
      "Model Saved\n",
      "Episode: 1776 Total reward: 36.949676513671875 Training loss: 0.1832 Explore P: 0.4705\n",
      "Episode: 1777 Total reward: -74.61769104003906 Training loss: 3.0186 Explore P: 0.4704\n",
      "Episode: 1778 Total reward: -5.241607666015625 Training loss: 0.3755 Explore P: 0.4702\n",
      "Episode: 1779 Total reward: -43.99092102050781 Training loss: 0.3255 Explore P: 0.4700\n",
      "Episode: 1780 Total reward: -73.37196350097656 Training loss: 0.2517 Explore P: 0.4698\n",
      "Model Saved\n",
      "Episode: 1781 Total reward: -20.3046875 Training loss: 0.5661 Explore P: 0.4696\n",
      "Episode: 1782 Total reward: -9.190521240234375 Training loss: 0.2660 Explore P: 0.4694\n",
      "Episode: 1783 Total reward: -75.32939147949219 Training loss: 0.5447 Explore P: 0.4693\n",
      "Episode: 1784 Total reward: -41.82783508300781 Training loss: 0.3683 Explore P: 0.4691\n",
      "Episode: 1785 Total reward: -58.04975891113281 Training loss: 1.2608 Explore P: 0.4690\n",
      "Model Saved\n",
      "Episode: 1786 Total reward: -68.8941650390625 Training loss: 0.1523 Explore P: 0.4688\n",
      "Episode: 1787 Total reward: -16.491348266601562 Training loss: 0.2756 Explore P: 0.4687\n",
      "Episode: 1788 Total reward: -59.577606201171875 Training loss: 0.2617 Explore P: 0.4685\n",
      "Episode: 1789 Total reward: -49.23435974121094 Training loss: 0.2881 Explore P: 0.4683\n",
      "Episode: 1790 Total reward: 23.305923461914062 Training loss: 1.7585 Explore P: 0.4681\n",
      "Model Saved\n",
      "Episode: 1791 Total reward: -48.38471984863281 Training loss: 0.2876 Explore P: 0.4679\n",
      "Episode: 1792 Total reward: 84.68161010742188 Training loss: 3.3954 Explore P: 0.4678\n",
      "Episode: 1793 Total reward: -29.51702880859375 Training loss: 13.4979 Explore P: 0.4676\n",
      "Episode: 1794 Total reward: -53.22796630859375 Training loss: 0.1866 Explore P: 0.4674\n",
      "Episode: 1795 Total reward: -114.36825561523438 Training loss: 9.5247 Explore P: 0.4673\n",
      "Model Saved\n",
      "Episode: 1796 Total reward: -60.29658508300781 Training loss: 0.2599 Explore P: 0.4671\n",
      "Episode: 1797 Total reward: -38.00408935546875 Training loss: 0.5254 Explore P: 0.4669\n",
      "Episode: 1798 Total reward: -0.6523284912109375 Training loss: 0.1795 Explore P: 0.4667\n",
      "Episode: 1799 Total reward: -92.42788696289062 Training loss: 0.2338 Explore P: 0.4667\n",
      "Episode: 1800 Total reward: -44.29718017578125 Training loss: 0.6383 Explore P: 0.4665\n",
      "Model Saved\n",
      "Episode: 1801 Total reward: -20.712066650390625 Training loss: 1.7946 Explore P: 0.4663\n",
      "Episode: 1802 Total reward: -14.637191772460938 Training loss: 5.4106 Explore P: 0.4661\n",
      "Episode: 1803 Total reward: -79.60069274902344 Training loss: 0.2376 Explore P: 0.4660\n",
      "Episode: 1804 Total reward: -25.562393188476562 Training loss: 0.3844 Explore P: 0.4659\n",
      "Episode: 1805 Total reward: -83.12301635742188 Training loss: 0.1966 Explore P: 0.4657\n",
      "Model Saved\n",
      "Episode: 1806 Total reward: -48.72914123535156 Training loss: 3.3159 Explore P: 0.4656\n",
      "Episode: 1807 Total reward: 20.262405395507812 Training loss: 1.8717 Explore P: 0.4654\n",
      "Episode: 1808 Total reward: -32.70222473144531 Training loss: 0.6607 Explore P: 0.4652\n",
      "Episode: 1809 Total reward: -88.45693969726562 Training loss: 0.3698 Explore P: 0.4651\n",
      "Episode: 1810 Total reward: -19.006668090820312 Training loss: 0.3820 Explore P: 0.4649\n",
      "Model Saved\n",
      "Episode: 1811 Total reward: -8.591827392578125 Training loss: 2.5401 Explore P: 0.4647\n",
      "Episode: 1812 Total reward: -14.969146728515625 Training loss: 0.3422 Explore P: 0.4645\n",
      "Episode: 1813 Total reward: -113.32916259765625 Training loss: 0.5272 Explore P: 0.4643\n",
      "Episode: 1814 Total reward: -74.40946960449219 Training loss: 0.7054 Explore P: 0.4642\n",
      "Episode: 1815 Total reward: -30.797515869140625 Training loss: 0.1729 Explore P: 0.4640\n",
      "Model Saved\n",
      "Episode: 1816 Total reward: -58.592742919921875 Training loss: 0.2017 Explore P: 0.4638\n",
      "Episode: 1817 Total reward: -32.167724609375 Training loss: 2.4970 Explore P: 0.4637\n",
      "Episode: 1818 Total reward: -91.95620727539062 Training loss: 0.5042 Explore P: 0.4636\n",
      "Episode: 1819 Total reward: -10.266494750976562 Training loss: 0.4791 Explore P: 0.4634\n",
      "Episode: 1820 Total reward: 32.17863464355469 Training loss: 0.5734 Explore P: 0.4632\n",
      "Model Saved\n",
      "Episode: 1821 Total reward: -29.604232788085938 Training loss: 17.8215 Explore P: 0.4630\n",
      "Episode: 1822 Total reward: -33.095947265625 Training loss: 2.5148 Explore P: 0.4628\n",
      "Episode: 1823 Total reward: -24.624649047851562 Training loss: 0.3985 Explore P: 0.4626\n",
      "Episode: 1824 Total reward: -40.314605712890625 Training loss: 1.3036 Explore P: 0.4625\n",
      "Episode: 1825 Total reward: -73.20170593261719 Training loss: 0.2124 Explore P: 0.4623\n",
      "Model Saved\n",
      "Episode: 1826 Total reward: 3.5107269287109375 Training loss: 0.4896 Explore P: 0.4622\n",
      "Episode: 1827 Total reward: -31.433441162109375 Training loss: 1.6361 Explore P: 0.4621\n",
      "Episode: 1828 Total reward: -49.94691467285156 Training loss: 0.2134 Explore P: 0.4619\n",
      "Episode: 1829 Total reward: -39.41065979003906 Training loss: 0.9121 Explore P: 0.4618\n",
      "Episode: 1830 Total reward: -1.9411773681640625 Training loss: 0.6108 Explore P: 0.4616\n",
      "Model Saved\n",
      "Episode: 1831 Total reward: -83.78239440917969 Training loss: 0.2244 Explore P: 0.4615\n",
      "Episode: 1832 Total reward: -0.8822174072265625 Training loss: 1.3124 Explore P: 0.4613\n",
      "Episode: 1833 Total reward: -51.68150329589844 Training loss: 0.1784 Explore P: 0.4612\n",
      "Episode: 1834 Total reward: -19.729522705078125 Training loss: 1.0911 Explore P: 0.4610\n",
      "Episode: 1835 Total reward: -54.45198059082031 Training loss: 0.2933 Explore P: 0.4608\n",
      "Model Saved\n",
      "Episode: 1836 Total reward: -12.355316162109375 Training loss: 0.1832 Explore P: 0.4606\n",
      "Episode: 1837 Total reward: -53.39619445800781 Training loss: 0.3345 Explore P: 0.4604\n",
      "Episode: 1838 Total reward: -89.95193481445312 Training loss: 0.1732 Explore P: 0.4603\n",
      "Episode: 1839 Total reward: -18.709075927734375 Training loss: 1.0902 Explore P: 0.4601\n",
      "Episode: 1840 Total reward: -100.15959167480469 Training loss: 0.6007 Explore P: 0.4600\n",
      "Model Saved\n",
      "Episode: 1841 Total reward: 43.5452880859375 Training loss: 0.2124 Explore P: 0.4598\n",
      "Episode: 1842 Total reward: 7.6204986572265625 Training loss: 0.2086 Explore P: 0.4596\n",
      "Episode: 1843 Total reward: -51.17182922363281 Training loss: 0.2961 Explore P: 0.4594\n",
      "Episode: 1844 Total reward: -35.86354064941406 Training loss: 0.2877 Explore P: 0.4592\n",
      "Episode: 1845 Total reward: -47.337188720703125 Training loss: 0.1734 Explore P: 0.4591\n",
      "Model Saved\n",
      "Episode: 1846 Total reward: -22.912551879882812 Training loss: 0.8237 Explore P: 0.4589\n",
      "Episode: 1847 Total reward: 51.284912109375 Training loss: 12.3237 Explore P: 0.4588\n",
      "Episode: 1848 Total reward: 18.440383911132812 Training loss: 0.3080 Explore P: 0.4585\n",
      "Episode: 1849 Total reward: -44.25953674316406 Training loss: 0.4003 Explore P: 0.4583\n",
      "Episode: 1850 Total reward: -47.074462890625 Training loss: 0.1294 Explore P: 0.4581\n",
      "Model Saved\n",
      "Episode: 1851 Total reward: -40.969940185546875 Training loss: 2.4236 Explore P: 0.4579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1852 Total reward: -49.05070495605469 Training loss: 2.0653 Explore P: 0.4578\n",
      "Episode: 1853 Total reward: -4.7795562744140625 Training loss: 0.2440 Explore P: 0.4576\n",
      "Episode: 1854 Total reward: -44.197509765625 Training loss: 2.6010 Explore P: 0.4575\n",
      "Episode: 1855 Total reward: -29.551620483398438 Training loss: 0.1904 Explore P: 0.4573\n",
      "Model Saved\n",
      "Episode: 1856 Total reward: -36.472808837890625 Training loss: 0.2085 Explore P: 0.4572\n",
      "Episode: 1857 Total reward: -61.59342956542969 Training loss: 1.0569 Explore P: 0.4570\n",
      "Episode: 1858 Total reward: -31.966293334960938 Training loss: 0.2433 Explore P: 0.4568\n",
      "Episode: 1859 Total reward: 89.83642578125 Training loss: 0.2323 Explore P: 0.4566\n",
      "Episode: 1860 Total reward: -1.123992919921875 Training loss: 3.5730 Explore P: 0.4564\n",
      "Model Saved\n",
      "Episode: 1861 Total reward: 19.174758911132812 Training loss: 0.3544 Explore P: 0.4562\n",
      "Episode: 1862 Total reward: -58.41009521484375 Training loss: 0.3496 Explore P: 0.4560\n",
      "Episode: 1863 Total reward: 62.38111877441406 Training loss: 0.7691 Explore P: 0.4558\n",
      "Episode: 1864 Total reward: -20.3096923828125 Training loss: 0.7501 Explore P: 0.4557\n",
      "Episode: 1865 Total reward: 9.587356567382812 Training loss: 0.3785 Explore P: 0.4555\n",
      "Model Saved\n",
      "Episode: 1866 Total reward: -3.1994781494140625 Training loss: 0.2367 Explore P: 0.4554\n",
      "Episode: 1867 Total reward: -12.397720336914062 Training loss: 0.3061 Explore P: 0.4552\n",
      "Episode: 1868 Total reward: 45.69865417480469 Training loss: 0.5681 Explore P: 0.4550\n",
      "Episode: 1869 Total reward: -23.34344482421875 Training loss: 0.1951 Explore P: 0.4549\n",
      "Episode: 1870 Total reward: -17.6446533203125 Training loss: 2.5784 Explore P: 0.4547\n",
      "Model Saved\n",
      "Episode: 1871 Total reward: -32.65397644042969 Training loss: 0.3243 Explore P: 0.4545\n",
      "Episode: 1872 Total reward: -7.307220458984375 Training loss: 0.8760 Explore P: 0.4543\n",
      "Episode: 1873 Total reward: -36.02742004394531 Training loss: 0.3857 Explore P: 0.4542\n",
      "Episode: 1874 Total reward: -21.847747802734375 Training loss: 0.8291 Explore P: 0.4540\n",
      "Episode: 1875 Total reward: -27.463714599609375 Training loss: 0.2652 Explore P: 0.4538\n",
      "Model Saved\n",
      "Episode: 1876 Total reward: -104.89199829101562 Training loss: 1.0308 Explore P: 0.4537\n",
      "Episode: 1877 Total reward: -28.781448364257812 Training loss: 7.8409 Explore P: 0.4535\n",
      "Episode: 1878 Total reward: 41.33277893066406 Training loss: 0.1742 Explore P: 0.4533\n",
      "Episode: 1879 Total reward: -43.95465087890625 Training loss: 0.2478 Explore P: 0.4531\n",
      "Episode: 1880 Total reward: 25.174835205078125 Training loss: 1.4279 Explore P: 0.4529\n",
      "Model Saved\n",
      "Episode: 1881 Total reward: -8.482208251953125 Training loss: 1.8636 Explore P: 0.4528\n",
      "Episode: 1882 Total reward: -81.6744384765625 Training loss: 0.2638 Explore P: 0.4526\n",
      "Episode: 1883 Total reward: -96.11366271972656 Training loss: 0.6549 Explore P: 0.4525\n",
      "Episode: 1884 Total reward: 17.35235595703125 Training loss: 9.7278 Explore P: 0.4523\n",
      "Episode: 1885 Total reward: -44.56629943847656 Training loss: 3.2195 Explore P: 0.4521\n",
      "Model Saved\n",
      "Episode: 1886 Total reward: -11.687942504882812 Training loss: 0.2056 Explore P: 0.4520\n",
      "Episode: 1887 Total reward: -53.69334411621094 Training loss: 0.8000 Explore P: 0.4518\n",
      "Episode: 1888 Total reward: -38.69053649902344 Training loss: 0.9164 Explore P: 0.4517\n",
      "Episode: 1889 Total reward: 54.422271728515625 Training loss: 0.6292 Explore P: 0.4515\n",
      "Episode: 1890 Total reward: -78.91975402832031 Training loss: 0.1916 Explore P: 0.4514\n",
      "Model Saved\n",
      "Episode: 1891 Total reward: 35.11647033691406 Training loss: 0.7410 Explore P: 0.4512\n",
      "Episode: 1892 Total reward: -51.078857421875 Training loss: 1.0750 Explore P: 0.4511\n",
      "Episode: 1893 Total reward: 4.43426513671875 Training loss: 0.1987 Explore P: 0.4509\n",
      "Episode: 1894 Total reward: -69.44064331054688 Training loss: 3.6665 Explore P: 0.4508\n",
      "Episode: 1895 Total reward: -28.971343994140625 Training loss: 0.7134 Explore P: 0.4506\n",
      "Model Saved\n",
      "Episode: 1896 Total reward: -34.43455505371094 Training loss: 7.4676 Explore P: 0.4505\n",
      "Episode: 1897 Total reward: -48.42311096191406 Training loss: 0.4616 Explore P: 0.4503\n",
      "Episode: 1898 Total reward: 15.77490234375 Training loss: 1.0375 Explore P: 0.4501\n",
      "Episode: 1899 Total reward: -53.44093322753906 Training loss: 0.1424 Explore P: 0.4499\n",
      "Episode: 1900 Total reward: -95.12931823730469 Training loss: 1.2261 Explore P: 0.4498\n",
      "Model Saved\n",
      "Episode: 1901 Total reward: -79.70640563964844 Training loss: 2.2001 Explore P: 0.4497\n",
      "Episode: 1902 Total reward: 66.72941589355469 Training loss: 0.3302 Explore P: 0.4495\n",
      "Episode: 1903 Total reward: -36.96211242675781 Training loss: 3.6412 Explore P: 0.4494\n",
      "Episode: 1904 Total reward: -56.309967041015625 Training loss: 0.3755 Explore P: 0.4492\n",
      "Episode: 1905 Total reward: -15.001785278320312 Training loss: 1.0775 Explore P: 0.4490\n",
      "Model Saved\n",
      "Episode: 1906 Total reward: -68.22935485839844 Training loss: 0.5948 Explore P: 0.4489\n",
      "Episode: 1907 Total reward: -48.48539733886719 Training loss: 0.1245 Explore P: 0.4487\n",
      "Episode: 1908 Total reward: -8.836807250976562 Training loss: 13.6518 Explore P: 0.4485\n",
      "Episode: 1909 Total reward: -19.876235961914062 Training loss: 0.1709 Explore P: 0.4484\n",
      "Episode: 1910 Total reward: -63.00508117675781 Training loss: 0.3277 Explore P: 0.4482\n",
      "Model Saved\n",
      "Episode: 1911 Total reward: -33.13435363769531 Training loss: 0.1123 Explore P: 0.4480\n",
      "Episode: 1912 Total reward: -28.981414794921875 Training loss: 1.5942 Explore P: 0.4478\n",
      "Episode: 1913 Total reward: 24.925704956054688 Training loss: 0.1481 Explore P: 0.4476\n",
      "Episode: 1914 Total reward: -60.32374572753906 Training loss: 0.2560 Explore P: 0.4475\n",
      "Episode: 1915 Total reward: -76.02178955078125 Training loss: 0.2093 Explore P: 0.4474\n",
      "Model Saved\n",
      "Episode: 1916 Total reward: -63.70176696777344 Training loss: 1.0551 Explore P: 0.4473\n",
      "Episode: 1917 Total reward: -48.94013977050781 Training loss: 0.2876 Explore P: 0.4472\n",
      "Episode: 1918 Total reward: -44.083953857421875 Training loss: 0.2191 Explore P: 0.4471\n",
      "Episode: 1919 Total reward: -9.249588012695312 Training loss: 0.6934 Explore P: 0.4468\n",
      "Episode: 1920 Total reward: -43.28715515136719 Training loss: 0.3529 Explore P: 0.4466\n",
      "Model Saved\n",
      "Episode: 1921 Total reward: 15.076934814453125 Training loss: 0.4596 Explore P: 0.4465\n",
      "Episode: 1922 Total reward: 15.948989868164062 Training loss: 0.2737 Explore P: 0.4463\n",
      "Episode: 1923 Total reward: -23.631698608398438 Training loss: 3.1591 Explore P: 0.4461\n",
      "Episode: 1924 Total reward: -8.95916748046875 Training loss: 2.3111 Explore P: 0.4459\n",
      "Episode: 1925 Total reward: -14.350387573242188 Training loss: 0.3333 Explore P: 0.4457\n",
      "Model Saved\n",
      "Episode: 1926 Total reward: -34.27668762207031 Training loss: 0.2418 Explore P: 0.4456\n",
      "Episode: 1927 Total reward: -9.650314331054688 Training loss: 0.1826 Explore P: 0.4455\n",
      "Episode: 1928 Total reward: -53.53413391113281 Training loss: 0.5905 Explore P: 0.4453\n",
      "Episode: 1929 Total reward: -35.35639953613281 Training loss: 0.2320 Explore P: 0.4452\n",
      "Episode: 1930 Total reward: -52.51649475097656 Training loss: 0.4242 Explore P: 0.4450\n",
      "Model Saved\n",
      "Episode: 1931 Total reward: -13.424026489257812 Training loss: 0.4176 Explore P: 0.4449\n",
      "Episode: 1932 Total reward: -58.46522521972656 Training loss: 0.2682 Explore P: 0.4446\n",
      "Episode: 1933 Total reward: -1.6137847900390625 Training loss: 0.4090 Explore P: 0.4444\n",
      "Episode: 1934 Total reward: -14.39019775390625 Training loss: 0.7568 Explore P: 0.4442\n",
      "Episode: 1935 Total reward: 13.437545776367188 Training loss: 0.7872 Explore P: 0.4440\n",
      "Model Saved\n",
      "Episode: 1936 Total reward: -61.427520751953125 Training loss: 0.7252 Explore P: 0.4438\n",
      "Episode: 1937 Total reward: 70.546142578125 Training loss: 0.5708 Explore P: 0.4436\n",
      "Episode: 1938 Total reward: 2.9666900634765625 Training loss: 0.7140 Explore P: 0.4434\n",
      "Episode: 1939 Total reward: 37.21441650390625 Training loss: 0.5203 Explore P: 0.4433\n",
      "Episode: 1940 Total reward: -54.44178771972656 Training loss: 3.0560 Explore P: 0.4431\n",
      "Model Saved\n",
      "Episode: 1941 Total reward: -70.06784057617188 Training loss: 0.6501 Explore P: 0.4429\n",
      "Episode: 1942 Total reward: -19.861114501953125 Training loss: 0.2700 Explore P: 0.4428\n",
      "Episode: 1943 Total reward: -23.9908447265625 Training loss: 0.2715 Explore P: 0.4426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1944 Total reward: -102.42630004882812 Training loss: 2.0444 Explore P: 0.4424\n",
      "Episode: 1945 Total reward: -68.28244018554688 Training loss: 2.1989 Explore P: 0.4423\n",
      "Model Saved\n",
      "Episode: 1946 Total reward: -29.183944702148438 Training loss: 1.2509 Explore P: 0.4421\n",
      "Episode: 1947 Total reward: 67.87394714355469 Training loss: 0.3161 Explore P: 0.4419\n",
      "Episode: 1948 Total reward: -52.136810302734375 Training loss: 0.5287 Explore P: 0.4417\n",
      "Episode: 1949 Total reward: 30.264846801757812 Training loss: 0.4431 Explore P: 0.4416\n",
      "Episode: 1950 Total reward: -16.54229736328125 Training loss: 0.2437 Explore P: 0.4413\n",
      "Model Saved\n",
      "Episode: 1951 Total reward: -14.96466064453125 Training loss: 0.3447 Explore P: 0.4412\n",
      "Episode: 1952 Total reward: 11.146514892578125 Training loss: 0.1776 Explore P: 0.4410\n",
      "Episode: 1953 Total reward: -43.7984619140625 Training loss: 1.3639 Explore P: 0.4408\n",
      "Episode: 1954 Total reward: -11.09429931640625 Training loss: 3.2047 Explore P: 0.4406\n",
      "Episode: 1955 Total reward: 6.4337005615234375 Training loss: 1.6784 Explore P: 0.4404\n",
      "Model Saved\n",
      "Episode: 1956 Total reward: -43.940155029296875 Training loss: 0.2081 Explore P: 0.4403\n",
      "Episode: 1957 Total reward: -100.3333740234375 Training loss: 1.5978 Explore P: 0.4401\n",
      "Episode: 1958 Total reward: -28.642578125 Training loss: 0.1890 Explore P: 0.4400\n",
      "Episode: 1959 Total reward: -84.25291442871094 Training loss: 0.6417 Explore P: 0.4399\n",
      "Episode: 1960 Total reward: -6.3668060302734375 Training loss: 0.9277 Explore P: 0.4397\n",
      "Model Saved\n",
      "Episode: 1961 Total reward: -87.343994140625 Training loss: 16.1174 Explore P: 0.4396\n",
      "Episode: 1962 Total reward: -63.38154602050781 Training loss: 0.4301 Explore P: 0.4395\n",
      "Episode: 1963 Total reward: -54.53489685058594 Training loss: 1.4842 Explore P: 0.4393\n",
      "Episode: 1964 Total reward: 4.0782623291015625 Training loss: 0.3077 Explore P: 0.4391\n",
      "Episode: 1965 Total reward: -40.42427062988281 Training loss: 19.5710 Explore P: 0.4390\n",
      "Model Saved\n",
      "Episode: 1966 Total reward: -83.33222961425781 Training loss: 1.0186 Explore P: 0.4389\n",
      "Episode: 1967 Total reward: -15.791900634765625 Training loss: 4.9739 Explore P: 0.4387\n",
      "Episode: 1968 Total reward: -24.464401245117188 Training loss: 0.8892 Explore P: 0.4385\n",
      "Episode: 1969 Total reward: -96.70703125 Training loss: 0.5488 Explore P: 0.4384\n",
      "Episode: 1970 Total reward: -23.90972900390625 Training loss: 0.3161 Explore P: 0.4383\n",
      "Model Saved\n",
      "Episode: 1971 Total reward: -65.80352783203125 Training loss: 1.2095 Explore P: 0.4381\n",
      "Episode: 1972 Total reward: 13.323287963867188 Training loss: 0.2718 Explore P: 0.4380\n",
      "Episode: 1973 Total reward: -43.861358642578125 Training loss: 0.2651 Explore P: 0.4378\n",
      "Episode: 1974 Total reward: 40.96693420410156 Training loss: 0.7771 Explore P: 0.4376\n",
      "Episode: 1975 Total reward: -101.78338623046875 Training loss: 4.5019 Explore P: 0.4375\n",
      "Model Saved\n",
      "Episode: 1976 Total reward: -49.46490478515625 Training loss: 0.5709 Explore P: 0.4374\n",
      "Episode: 1977 Total reward: 8.469345092773438 Training loss: 0.3799 Explore P: 0.4372\n",
      "Episode: 1978 Total reward: -40.94306945800781 Training loss: 0.4436 Explore P: 0.4370\n",
      "Episode: 1979 Total reward: -25.48095703125 Training loss: 0.3415 Explore P: 0.4368\n",
      "Episode: 1980 Total reward: 48.89698791503906 Training loss: 0.2696 Explore P: 0.4367\n",
      "Model Saved\n",
      "Episode: 1981 Total reward: -38.87797546386719 Training loss: 0.2549 Explore P: 0.4365\n",
      "Episode: 1982 Total reward: -33.06085205078125 Training loss: 0.6033 Explore P: 0.4363\n",
      "Episode: 1983 Total reward: -64.70384216308594 Training loss: 0.3239 Explore P: 0.4361\n",
      "Episode: 1984 Total reward: -55.26995849609375 Training loss: 0.3633 Explore P: 0.4360\n",
      "Episode: 1985 Total reward: -31.47772216796875 Training loss: 0.2491 Explore P: 0.4359\n",
      "Model Saved\n",
      "Episode: 1986 Total reward: 38.464935302734375 Training loss: 0.2238 Explore P: 0.4357\n",
      "Episode: 1987 Total reward: 26.644622802734375 Training loss: 0.6090 Explore P: 0.4355\n",
      "Episode: 1988 Total reward: -13.753677368164062 Training loss: 0.9951 Explore P: 0.4353\n",
      "Episode: 1989 Total reward: -36.323944091796875 Training loss: 1.5595 Explore P: 0.4352\n",
      "Episode: 1990 Total reward: 6.9842071533203125 Training loss: 0.6737 Explore P: 0.4350\n",
      "Model Saved\n",
      "Episode: 1991 Total reward: -50.576751708984375 Training loss: 1.6200 Explore P: 0.4348\n",
      "Episode: 1992 Total reward: -26.122955322265625 Training loss: 0.3046 Explore P: 0.4347\n",
      "Episode: 1993 Total reward: -20.990768432617188 Training loss: 1.2589 Explore P: 0.4345\n",
      "Episode: 1994 Total reward: -39.234954833984375 Training loss: 0.2672 Explore P: 0.4343\n",
      "Episode: 1995 Total reward: 58.28703308105469 Training loss: 0.2194 Explore P: 0.4341\n",
      "Model Saved\n",
      "Episode: 1996 Total reward: -103.16851806640625 Training loss: 0.2540 Explore P: 0.4340\n",
      "Episode: 1997 Total reward: -36.46205139160156 Training loss: 0.1932 Explore P: 0.4338\n",
      "Episode: 1998 Total reward: -27.634292602539062 Training loss: 0.2109 Explore P: 0.4336\n",
      "Episode: 1999 Total reward: 18.643829345703125 Training loss: 1.4862 Explore P: 0.4335\n",
      "Episode: 2000 Total reward: -36.07514953613281 Training loss: 1.2473 Explore P: 0.4333\n",
      "Model Saved\n",
      "Episode: 2001 Total reward: -36.475067138671875 Training loss: 0.3334 Explore P: 0.4332\n",
      "Episode: 2002 Total reward: -36.283966064453125 Training loss: 2.6389 Explore P: 0.4331\n",
      "Episode: 2003 Total reward: -52.2457275390625 Training loss: 0.1860 Explore P: 0.4329\n",
      "Episode: 2004 Total reward: 13.559005737304688 Training loss: 15.9608 Explore P: 0.4327\n",
      "Episode: 2005 Total reward: -51.71258544921875 Training loss: 0.4342 Explore P: 0.4326\n",
      "Model Saved\n",
      "Episode: 2006 Total reward: 4.8467864990234375 Training loss: 0.2646 Explore P: 0.4324\n",
      "Episode: 2007 Total reward: -89.2442626953125 Training loss: 0.9098 Explore P: 0.4323\n",
      "Episode: 2008 Total reward: -70.26502990722656 Training loss: 0.3353 Explore P: 0.4322\n",
      "Episode: 2009 Total reward: 32.7091064453125 Training loss: 10.8237 Explore P: 0.4320\n",
      "Episode: 2010 Total reward: -31.913818359375 Training loss: 0.1909 Explore P: 0.4318\n",
      "Model Saved\n",
      "Episode: 2011 Total reward: -73.18557739257812 Training loss: 0.2603 Explore P: 0.4317\n",
      "Episode: 2012 Total reward: -6.0776519775390625 Training loss: 0.1625 Explore P: 0.4315\n",
      "Episode: 2013 Total reward: -51.96710205078125 Training loss: 1.4325 Explore P: 0.4313\n",
      "Episode: 2014 Total reward: -106.01446533203125 Training loss: 2.6346 Explore P: 0.4312\n",
      "Episode: 2015 Total reward: -39.10694885253906 Training loss: 0.1449 Explore P: 0.4310\n",
      "Model Saved\n",
      "Episode: 2016 Total reward: -38.92796325683594 Training loss: 5.6639 Explore P: 0.4308\n",
      "Episode: 2017 Total reward: -84.68682861328125 Training loss: 0.2256 Explore P: 0.4307\n",
      "Episode: 2018 Total reward: -11.554489135742188 Training loss: 1.7787 Explore P: 0.4306\n",
      "Episode: 2019 Total reward: -40.93751525878906 Training loss: 0.4164 Explore P: 0.4304\n",
      "Episode: 2020 Total reward: 3.4425048828125 Training loss: 0.2636 Explore P: 0.4302\n",
      "Model Saved\n",
      "Episode: 2021 Total reward: -32.85227966308594 Training loss: 0.2821 Explore P: 0.4301\n",
      "Episode: 2022 Total reward: 112.6036376953125 Training loss: 2.1732 Explore P: 0.4299\n",
      "Episode: 2023 Total reward: 24.359527587890625 Training loss: 0.4558 Explore P: 0.4297\n",
      "Episode: 2024 Total reward: -39.35429382324219 Training loss: 1.3874 Explore P: 0.4295\n",
      "Episode: 2025 Total reward: -44.5606689453125 Training loss: 0.6584 Explore P: 0.4294\n",
      "Model Saved\n",
      "Episode: 2026 Total reward: 44.32563781738281 Training loss: 0.3746 Explore P: 0.4292\n",
      "Episode: 2027 Total reward: -77.41357421875 Training loss: 2.2301 Explore P: 0.4291\n",
      "Episode: 2028 Total reward: -69.20866394042969 Training loss: 1.9706 Explore P: 0.4289\n",
      "Episode: 2029 Total reward: -51.46070861816406 Training loss: 0.1593 Explore P: 0.4287\n",
      "Episode: 2030 Total reward: -57.53228759765625 Training loss: 0.3552 Explore P: 0.4286\n",
      "Model Saved\n",
      "Episode: 2031 Total reward: -34.44404602050781 Training loss: 0.1558 Explore P: 0.4285\n",
      "Episode: 2032 Total reward: -56.49871826171875 Training loss: 0.2244 Explore P: 0.4283\n",
      "Episode: 2033 Total reward: 24.346160888671875 Training loss: 1.1200 Explore P: 0.4281\n",
      "Episode: 2034 Total reward: 1.57470703125 Training loss: 0.8876 Explore P: 0.4280\n",
      "Episode: 2035 Total reward: -7.8812713623046875 Training loss: 0.2461 Explore P: 0.4278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Episode: 2036 Total reward: 50.20768737792969 Training loss: 2.3406 Explore P: 0.4276\n",
      "Episode: 2037 Total reward: 53.92460632324219 Training loss: 0.2144 Explore P: 0.4275\n",
      "Episode: 2038 Total reward: -45.93121337890625 Training loss: 0.7778 Explore P: 0.4273\n",
      "Episode: 2039 Total reward: -1.27935791015625 Training loss: 0.3799 Explore P: 0.4271\n",
      "Episode: 2040 Total reward: -22.203811645507812 Training loss: 0.3063 Explore P: 0.4269\n",
      "Model Saved\n",
      "Episode: 2041 Total reward: -8.868194580078125 Training loss: 1.5385 Explore P: 0.4267\n",
      "Episode: 2042 Total reward: -18.177886962890625 Training loss: 0.2550 Explore P: 0.4266\n",
      "Episode: 2043 Total reward: -11.360580444335938 Training loss: 0.2113 Explore P: 0.4264\n",
      "Episode: 2044 Total reward: -12.715667724609375 Training loss: 0.2579 Explore P: 0.4263\n",
      "Episode: 2045 Total reward: -108.8477783203125 Training loss: 0.2521 Explore P: 0.4260\n",
      "Model Saved\n",
      "Episode: 2046 Total reward: -13.126678466796875 Training loss: 1.4490 Explore P: 0.4258\n",
      "Episode: 2047 Total reward: 18.866897583007812 Training loss: 0.2275 Explore P: 0.4257\n",
      "Episode: 2048 Total reward: -21.569610595703125 Training loss: 1.2356 Explore P: 0.4255\n",
      "Episode: 2049 Total reward: -47.36750793457031 Training loss: 0.2542 Explore P: 0.4253\n",
      "Episode: 2050 Total reward: -15.539260864257812 Training loss: 0.9761 Explore P: 0.4252\n",
      "Model Saved\n",
      "Episode: 2051 Total reward: -83.78999328613281 Training loss: 0.6681 Explore P: 0.4251\n",
      "Episode: 2052 Total reward: -34.14772033691406 Training loss: 1.0616 Explore P: 0.4249\n",
      "Episode: 2053 Total reward: 23.511260986328125 Training loss: 0.9847 Explore P: 0.4247\n",
      "Episode: 2054 Total reward: -81.59764099121094 Training loss: 0.3221 Explore P: 0.4246\n",
      "Episode: 2055 Total reward: -34.95146179199219 Training loss: 0.4822 Explore P: 0.4245\n",
      "Model Saved\n",
      "Episode: 2056 Total reward: -26.95037841796875 Training loss: 8.8718 Explore P: 0.4244\n",
      "Episode: 2057 Total reward: -49.15663146972656 Training loss: 0.7804 Explore P: 0.4242\n",
      "Episode: 2058 Total reward: -32.125030517578125 Training loss: 0.1554 Explore P: 0.4240\n",
      "Episode: 2059 Total reward: -76.96673583984375 Training loss: 0.3765 Explore P: 0.4240\n",
      "Episode: 2060 Total reward: 13.1566162109375 Training loss: 0.2768 Explore P: 0.4238\n",
      "Model Saved\n",
      "Episode: 2061 Total reward: -19.757171630859375 Training loss: 0.5605 Explore P: 0.4236\n",
      "Episode: 2062 Total reward: -40.982940673828125 Training loss: 1.4375 Explore P: 0.4234\n",
      "Episode: 2063 Total reward: -39.174591064453125 Training loss: 0.8637 Explore P: 0.4233\n",
      "Episode: 2064 Total reward: 14.527542114257812 Training loss: 0.9900 Explore P: 0.4232\n",
      "Episode: 2065 Total reward: -31.904266357421875 Training loss: 0.5432 Explore P: 0.4230\n",
      "Model Saved\n",
      "Episode: 2066 Total reward: -23.5482177734375 Training loss: 1.3652 Explore P: 0.4228\n",
      "Episode: 2067 Total reward: -47.08271789550781 Training loss: 0.5249 Explore P: 0.4227\n",
      "Episode: 2068 Total reward: -22.439727783203125 Training loss: 0.6707 Explore P: 0.4225\n",
      "Episode: 2069 Total reward: -16.320602416992188 Training loss: 0.5447 Explore P: 0.4223\n",
      "Episode: 2070 Total reward: -36.35835266113281 Training loss: 0.1692 Explore P: 0.4222\n",
      "Model Saved\n",
      "Episode: 2071 Total reward: -9.047500610351562 Training loss: 8.3658 Explore P: 0.4220\n",
      "Episode: 2072 Total reward: 13.895828247070312 Training loss: 0.4458 Explore P: 0.4218\n",
      "Episode: 2073 Total reward: -40.73832702636719 Training loss: 0.8944 Explore P: 0.4217\n",
      "Episode: 2074 Total reward: -8.181671142578125 Training loss: 0.1972 Explore P: 0.4216\n",
      "Episode: 2075 Total reward: -82.69938659667969 Training loss: 0.8018 Explore P: 0.4214\n",
      "Model Saved\n",
      "Episode: 2076 Total reward: -52.740264892578125 Training loss: 0.3333 Explore P: 0.4212\n",
      "Episode: 2077 Total reward: -12.95758056640625 Training loss: 0.1362 Explore P: 0.4211\n",
      "Episode: 2078 Total reward: 0.907257080078125 Training loss: 9.1324 Explore P: 0.4209\n",
      "Episode: 2079 Total reward: 62.05162048339844 Training loss: 0.4995 Explore P: 0.4208\n",
      "Episode: 2080 Total reward: 11.403335571289062 Training loss: 0.2842 Explore P: 0.4206\n",
      "Model Saved\n",
      "Episode: 2081 Total reward: 14.13824462890625 Training loss: 1.0027 Explore P: 0.4204\n",
      "Episode: 2082 Total reward: 48.256256103515625 Training loss: 13.8224 Explore P: 0.4202\n",
      "Episode: 2083 Total reward: -19.947647094726562 Training loss: 0.2466 Explore P: 0.4201\n",
      "Episode: 2084 Total reward: -71.95234680175781 Training loss: 0.5767 Explore P: 0.4200\n",
      "Episode: 2085 Total reward: -65.60595703125 Training loss: 1.2413 Explore P: 0.4198\n",
      "Model Saved\n",
      "Episode: 2086 Total reward: -39.60304260253906 Training loss: 0.2332 Explore P: 0.4197\n",
      "Episode: 2087 Total reward: -8.69989013671875 Training loss: 0.2301 Explore P: 0.4196\n",
      "Episode: 2088 Total reward: -6.527923583984375 Training loss: 0.2646 Explore P: 0.4194\n",
      "Episode: 2089 Total reward: 35.23591613769531 Training loss: 0.3003 Explore P: 0.4192\n",
      "Episode: 2090 Total reward: -41.24127197265625 Training loss: 0.8628 Explore P: 0.4190\n",
      "Model Saved\n",
      "Episode: 2091 Total reward: -20.114837646484375 Training loss: 0.5646 Explore P: 0.4189\n",
      "Episode: 2092 Total reward: 44.85575866699219 Training loss: 1.2859 Explore P: 0.4188\n",
      "Episode: 2093 Total reward: 14.520584106445312 Training loss: 0.1812 Explore P: 0.4186\n",
      "Episode: 2094 Total reward: 35.18597412109375 Training loss: 0.3933 Explore P: 0.4184\n",
      "Episode: 2095 Total reward: 7.3132171630859375 Training loss: 0.3484 Explore P: 0.4183\n",
      "Model Saved\n",
      "Episode: 2096 Total reward: 7.0022125244140625 Training loss: 0.3728 Explore P: 0.4181\n",
      "Episode: 2097 Total reward: -43.80961608886719 Training loss: 0.8908 Explore P: 0.4179\n",
      "Episode: 2098 Total reward: -23.919586181640625 Training loss: 0.5979 Explore P: 0.4178\n",
      "Episode: 2099 Total reward: -1.536956787109375 Training loss: 0.2546 Explore P: 0.4176\n",
      "Episode: 2100 Total reward: -76.49369812011719 Training loss: 1.4102 Explore P: 0.4174\n",
      "Model Saved\n",
      "Episode: 2101 Total reward: -5.3396759033203125 Training loss: 0.2755 Explore P: 0.4173\n",
      "Episode: 2102 Total reward: -75.36964416503906 Training loss: 0.2315 Explore P: 0.4171\n",
      "Episode: 2103 Total reward: -58.23310852050781 Training loss: 0.3336 Explore P: 0.4169\n",
      "Episode: 2104 Total reward: -50.55046081542969 Training loss: 0.2602 Explore P: 0.4168\n",
      "Episode: 2105 Total reward: -25.820816040039062 Training loss: 0.7034 Explore P: 0.4167\n",
      "Model Saved\n",
      "Episode: 2106 Total reward: -19.591537475585938 Training loss: 0.3025 Explore P: 0.4165\n",
      "Episode: 2107 Total reward: -34.819610595703125 Training loss: 5.9803 Explore P: 0.4164\n",
      "Episode: 2108 Total reward: -73.12045288085938 Training loss: 0.7095 Explore P: 0.4163\n",
      "Episode: 2109 Total reward: -83.4708251953125 Training loss: 0.2761 Explore P: 0.4162\n",
      "Episode: 2110 Total reward: -55.22810363769531 Training loss: 0.4435 Explore P: 0.4161\n",
      "Model Saved\n",
      "Episode: 2111 Total reward: -37.17085266113281 Training loss: 0.8340 Explore P: 0.4159\n",
      "Episode: 2112 Total reward: 28.203567504882812 Training loss: 1.0384 Explore P: 0.4157\n",
      "Episode: 2113 Total reward: -46.608795166015625 Training loss: 0.2707 Explore P: 0.4156\n",
      "Episode: 2114 Total reward: -22.524566650390625 Training loss: 1.9875 Explore P: 0.4154\n",
      "Episode: 2115 Total reward: -10.229507446289062 Training loss: 14.6698 Explore P: 0.4153\n",
      "Model Saved\n",
      "Episode: 2116 Total reward: -24.811065673828125 Training loss: 0.6206 Explore P: 0.4151\n",
      "Episode: 2117 Total reward: -63.578704833984375 Training loss: 0.3173 Explore P: 0.4149\n",
      "Episode: 2118 Total reward: -16.818649291992188 Training loss: 0.4412 Explore P: 0.4148\n",
      "Episode: 2119 Total reward: 16.978378295898438 Training loss: 0.2706 Explore P: 0.4146\n",
      "Episode: 2120 Total reward: -7.697662353515625 Training loss: 0.1825 Explore P: 0.4144\n",
      "Model Saved\n",
      "Episode: 2121 Total reward: -67.39115905761719 Training loss: 0.2905 Explore P: 0.4143\n",
      "Episode: 2122 Total reward: -31.53009033203125 Training loss: 0.7019 Explore P: 0.4142\n",
      "Episode: 2123 Total reward: -66.47834777832031 Training loss: 0.2019 Explore P: 0.4141\n",
      "Episode: 2124 Total reward: 61.05387878417969 Training loss: 0.3027 Explore P: 0.4139\n",
      "Episode: 2125 Total reward: -20.501983642578125 Training loss: 0.2600 Explore P: 0.4137\n",
      "Model Saved\n",
      "Episode: 2126 Total reward: -79.93727111816406 Training loss: 0.9713 Explore P: 0.4136\n",
      "Episode: 2127 Total reward: 0.2785797119140625 Training loss: 17.5330 Explore P: 0.4135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2128 Total reward: 5.0299072265625 Training loss: 0.5323 Explore P: 0.4133\n",
      "Episode: 2129 Total reward: -37.68975830078125 Training loss: 1.6776 Explore P: 0.4131\n",
      "Episode: 2130 Total reward: -37.3697509765625 Training loss: 0.9828 Explore P: 0.4130\n",
      "Model Saved\n",
      "Episode: 2131 Total reward: -77.851806640625 Training loss: 0.6023 Explore P: 0.4128\n",
      "Episode: 2132 Total reward: -78.85560607910156 Training loss: 0.2488 Explore P: 0.4127\n",
      "Episode: 2133 Total reward: 4.5402374267578125 Training loss: 0.5364 Explore P: 0.4126\n",
      "Episode: 2134 Total reward: -51.85444641113281 Training loss: 0.4414 Explore P: 0.4124\n",
      "Episode: 2135 Total reward: -34.200836181640625 Training loss: 0.4838 Explore P: 0.4122\n",
      "Model Saved\n",
      "Episode: 2136 Total reward: 1.275604248046875 Training loss: 2.1285 Explore P: 0.4121\n",
      "Episode: 2137 Total reward: -47.65159606933594 Training loss: 0.2472 Explore P: 0.4120\n",
      "Episode: 2138 Total reward: -79.62344360351562 Training loss: 0.3357 Explore P: 0.4119\n",
      "Episode: 2139 Total reward: -7.775238037109375 Training loss: 7.4366 Explore P: 0.4117\n",
      "Episode: 2140 Total reward: -67.13471984863281 Training loss: 0.2125 Explore P: 0.4115\n",
      "Model Saved\n",
      "Episode: 2141 Total reward: -39.03912353515625 Training loss: 0.2821 Explore P: 0.4114\n",
      "Episode: 2142 Total reward: -83.12825012207031 Training loss: 1.9203 Explore P: 0.4112\n",
      "Episode: 2143 Total reward: -14.055572509765625 Training loss: 0.3747 Explore P: 0.4111\n",
      "Episode: 2144 Total reward: -45.44769287109375 Training loss: 0.2274 Explore P: 0.4109\n",
      "Episode: 2145 Total reward: 31.801055908203125 Training loss: 0.7200 Explore P: 0.4107\n",
      "Model Saved\n",
      "Episode: 2146 Total reward: -9.41009521484375 Training loss: 0.9141 Explore P: 0.4106\n",
      "Episode: 2147 Total reward: 27.961563110351562 Training loss: 0.1781 Explore P: 0.4104\n",
      "Episode: 2148 Total reward: -15.722442626953125 Training loss: 0.4760 Explore P: 0.4103\n",
      "Episode: 2149 Total reward: 13.12261962890625 Training loss: 2.5738 Explore P: 0.4101\n",
      "Episode: 2150 Total reward: -31.529647827148438 Training loss: 0.3005 Explore P: 0.4099\n",
      "Model Saved\n",
      "Episode: 2151 Total reward: 23.188079833984375 Training loss: 0.3773 Explore P: 0.4098\n",
      "Episode: 2152 Total reward: -55.307830810546875 Training loss: 0.1906 Explore P: 0.4097\n",
      "Episode: 2153 Total reward: -37.838592529296875 Training loss: 0.2732 Explore P: 0.4095\n",
      "Episode: 2154 Total reward: -74.67323303222656 Training loss: 2.3790 Explore P: 0.4094\n",
      "Episode: 2155 Total reward: -18.124923706054688 Training loss: 0.6673 Explore P: 0.4092\n",
      "Model Saved\n",
      "Episode: 2156 Total reward: -42.66432189941406 Training loss: 0.3787 Explore P: 0.4091\n",
      "Episode: 2157 Total reward: -22.863174438476562 Training loss: 0.8461 Explore P: 0.4089\n",
      "Episode: 2158 Total reward: 31.3431396484375 Training loss: 0.2739 Explore P: 0.4088\n",
      "Episode: 2159 Total reward: -6.614501953125 Training loss: 0.2592 Explore P: 0.4087\n",
      "Episode: 2160 Total reward: -16.460281372070312 Training loss: 13.2735 Explore P: 0.4085\n",
      "Model Saved\n",
      "Episode: 2161 Total reward: -91.8397216796875 Training loss: 0.4073 Explore P: 0.4084\n",
      "Episode: 2162 Total reward: 9.677749633789062 Training loss: 0.1784 Explore P: 0.4082\n",
      "Episode: 2163 Total reward: -12.5640869140625 Training loss: 0.1621 Explore P: 0.4080\n",
      "Episode: 2164 Total reward: -1.471710205078125 Training loss: 0.1836 Explore P: 0.4079\n",
      "Episode: 2165 Total reward: -33.342254638671875 Training loss: 0.3426 Explore P: 0.4078\n",
      "Model Saved\n",
      "Episode: 2166 Total reward: 2.1313934326171875 Training loss: 14.2178 Explore P: 0.4076\n",
      "Episode: 2167 Total reward: -48.74748229980469 Training loss: 1.3864 Explore P: 0.4074\n",
      "Episode: 2168 Total reward: -60.523773193359375 Training loss: 0.4995 Explore P: 0.4073\n",
      "Episode: 2169 Total reward: -18.062118530273438 Training loss: 2.4420 Explore P: 0.4071\n",
      "Episode: 2170 Total reward: 15.917510986328125 Training loss: 1.0484 Explore P: 0.4070\n",
      "Model Saved\n",
      "Episode: 2171 Total reward: -7.3624725341796875 Training loss: 0.2207 Explore P: 0.4068\n",
      "Episode: 2172 Total reward: -21.615951538085938 Training loss: 0.7544 Explore P: 0.4067\n",
      "Episode: 2173 Total reward: -19.824966430664062 Training loss: 0.3307 Explore P: 0.4065\n",
      "Episode: 2174 Total reward: -4.1997528076171875 Training loss: 0.6589 Explore P: 0.4063\n",
      "Episode: 2175 Total reward: 8.420989990234375 Training loss: 0.5191 Explore P: 0.4062\n",
      "Model Saved\n",
      "Episode: 2176 Total reward: -27.279830932617188 Training loss: 0.2089 Explore P: 0.4060\n",
      "Episode: 2177 Total reward: 20.152618408203125 Training loss: 0.3264 Explore P: 0.4058\n",
      "Episode: 2178 Total reward: 0.181243896484375 Training loss: 0.1336 Explore P: 0.4057\n",
      "Episode: 2179 Total reward: -6.160247802734375 Training loss: 7.0919 Explore P: 0.4055\n",
      "Episode: 2180 Total reward: -79.7884521484375 Training loss: 3.3038 Explore P: 0.4054\n",
      "Model Saved\n",
      "Episode: 2181 Total reward: -16.539474487304688 Training loss: 0.2116 Explore P: 0.4053\n",
      "Episode: 2182 Total reward: -19.212814331054688 Training loss: 0.4516 Explore P: 0.4052\n",
      "Episode: 2183 Total reward: -60.03810119628906 Training loss: 0.4770 Explore P: 0.4050\n",
      "Episode: 2184 Total reward: -76.78976440429688 Training loss: 0.2365 Explore P: 0.4049\n",
      "Episode: 2185 Total reward: -60.9869384765625 Training loss: 0.4930 Explore P: 0.4047\n",
      "Model Saved\n",
      "Episode: 2186 Total reward: 18.532089233398438 Training loss: 1.5459 Explore P: 0.4046\n",
      "Episode: 2187 Total reward: -30.30499267578125 Training loss: 0.3265 Explore P: 0.4045\n",
      "Episode: 2188 Total reward: -18.73687744140625 Training loss: 0.1917 Explore P: 0.4043\n",
      "Episode: 2189 Total reward: 8.80816650390625 Training loss: 0.4918 Explore P: 0.4041\n",
      "Episode: 2190 Total reward: 1.631866455078125 Training loss: 0.2931 Explore P: 0.4040\n",
      "Model Saved\n",
      "Episode: 2191 Total reward: -27.506103515625 Training loss: 0.2827 Explore P: 0.4038\n",
      "Episode: 2192 Total reward: -11.290863037109375 Training loss: 0.2532 Explore P: 0.4037\n",
      "Episode: 2193 Total reward: 5.8232269287109375 Training loss: 0.2041 Explore P: 0.4035\n",
      "Episode: 2194 Total reward: 13.2427978515625 Training loss: 0.6815 Explore P: 0.4033\n",
      "Episode: 2195 Total reward: -82.0689697265625 Training loss: 1.4861 Explore P: 0.4032\n",
      "Model Saved\n",
      "Episode: 2196 Total reward: -77.56556701660156 Training loss: 0.2186 Explore P: 0.4030\n",
      "Episode: 2197 Total reward: -22.320449829101562 Training loss: 3.5514 Explore P: 0.4030\n",
      "Episode: 2198 Total reward: -31.19091796875 Training loss: 0.2883 Explore P: 0.4028\n",
      "Episode: 2199 Total reward: 55.63580322265625 Training loss: 0.1602 Explore P: 0.4026\n",
      "Episode: 2200 Total reward: -53.697967529296875 Training loss: 9.7072 Explore P: 0.4025\n",
      "Model Saved\n",
      "Episode: 2201 Total reward: -53.982421875 Training loss: 0.3493 Explore P: 0.4023\n",
      "Episode: 2202 Total reward: 43.67933654785156 Training loss: 0.7402 Explore P: 0.4022\n",
      "Episode: 2203 Total reward: -59.89265441894531 Training loss: 7.9193 Explore P: 0.4021\n",
      "Episode: 2204 Total reward: -76.29252624511719 Training loss: 0.2008 Explore P: 0.4020\n",
      "Episode: 2205 Total reward: 16.992324829101562 Training loss: 2.3758 Explore P: 0.4018\n",
      "Model Saved\n",
      "Episode: 2206 Total reward: -20.433547973632812 Training loss: 0.2264 Explore P: 0.4017\n",
      "Episode: 2207 Total reward: -8.1217041015625 Training loss: 0.1877 Explore P: 0.4016\n",
      "Episode: 2208 Total reward: -25.300994873046875 Training loss: 0.8123 Explore P: 0.4014\n",
      "Episode: 2209 Total reward: -57.72029113769531 Training loss: 0.3375 Explore P: 0.4013\n",
      "Episode: 2210 Total reward: -16.539382934570312 Training loss: 2.5728 Explore P: 0.4011\n",
      "Model Saved\n",
      "Episode: 2211 Total reward: -16.56561279296875 Training loss: 0.1545 Explore P: 0.4009\n",
      "Episode: 2212 Total reward: -32.39830017089844 Training loss: 1.5109 Explore P: 0.4008\n",
      "Episode: 2213 Total reward: -32.34733581542969 Training loss: 0.2003 Explore P: 0.4007\n",
      "Episode: 2214 Total reward: 33.23052978515625 Training loss: 0.2050 Explore P: 0.4005\n",
      "Episode: 2215 Total reward: -20.811798095703125 Training loss: 0.3038 Explore P: 0.4004\n",
      "Model Saved\n",
      "Episode: 2216 Total reward: -24.141952514648438 Training loss: 0.5828 Explore P: 0.4002\n",
      "Episode: 2217 Total reward: -70.19039916992188 Training loss: 15.5483 Explore P: 0.4001\n",
      "Episode: 2218 Total reward: 55.09381103515625 Training loss: 0.2192 Explore P: 0.4000\n",
      "Episode: 2219 Total reward: -16.0904541015625 Training loss: 0.2543 Explore P: 0.3999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2220 Total reward: -15.617446899414062 Training loss: 0.2769 Explore P: 0.3997\n",
      "Model Saved\n",
      "Episode: 2221 Total reward: -52.860321044921875 Training loss: 0.7155 Explore P: 0.3995\n",
      "Episode: 2222 Total reward: -60.461456298828125 Training loss: 13.0680 Explore P: 0.3994\n",
      "Episode: 2223 Total reward: -41.72447204589844 Training loss: 0.9248 Explore P: 0.3992\n",
      "Episode: 2224 Total reward: 16.802932739257812 Training loss: 0.3574 Explore P: 0.3991\n",
      "Episode: 2225 Total reward: -70.36886596679688 Training loss: 0.7280 Explore P: 0.3990\n",
      "Model Saved\n",
      "Episode: 2226 Total reward: -72.93392944335938 Training loss: 0.2537 Explore P: 0.3989\n",
      "Episode: 2227 Total reward: 31.724380493164062 Training loss: 1.8645 Explore P: 0.3987\n",
      "Episode: 2228 Total reward: -26.133712768554688 Training loss: 0.2843 Explore P: 0.3986\n",
      "Episode: 2229 Total reward: 98.41883850097656 Training loss: 0.2528 Explore P: 0.3984\n",
      "Episode: 2230 Total reward: -29.225784301757812 Training loss: 0.2457 Explore P: 0.3983\n",
      "Model Saved\n",
      "Episode: 2231 Total reward: 71.49186706542969 Training loss: 0.5295 Explore P: 0.3981\n",
      "Episode: 2232 Total reward: -17.95697021484375 Training loss: 0.2716 Explore P: 0.3980\n",
      "Episode: 2233 Total reward: -33.91326904296875 Training loss: 0.9249 Explore P: 0.3979\n",
      "Episode: 2234 Total reward: -88.97589111328125 Training loss: 0.2527 Explore P: 0.3979\n",
      "Episode: 2235 Total reward: -4.40386962890625 Training loss: 0.1746 Explore P: 0.3977\n",
      "Model Saved\n",
      "Episode: 2236 Total reward: -74.38197326660156 Training loss: 0.1946 Explore P: 0.3976\n",
      "Episode: 2237 Total reward: -13.42236328125 Training loss: 0.2678 Explore P: 0.3974\n",
      "Episode: 2238 Total reward: -42.71473693847656 Training loss: 0.7236 Explore P: 0.3973\n",
      "Episode: 2239 Total reward: -21.478729248046875 Training loss: 1.8064 Explore P: 0.3972\n",
      "Episode: 2240 Total reward: -14.211227416992188 Training loss: 0.6875 Explore P: 0.3970\n",
      "Model Saved\n",
      "Episode: 2241 Total reward: 76.26820373535156 Training loss: 0.5374 Explore P: 0.3968\n",
      "Episode: 2242 Total reward: 4.737548828125 Training loss: 0.2682 Explore P: 0.3967\n",
      "Episode: 2243 Total reward: -26.638870239257812 Training loss: 1.1351 Explore P: 0.3965\n",
      "Episode: 2244 Total reward: -26.79901123046875 Training loss: 0.5976 Explore P: 0.3964\n",
      "Episode: 2245 Total reward: 8.450897216796875 Training loss: 0.3012 Explore P: 0.3962\n",
      "Model Saved\n",
      "Episode: 2246 Total reward: 0.052276611328125 Training loss: 1.7737 Explore P: 0.3961\n",
      "Episode: 2247 Total reward: -33.84455871582031 Training loss: 0.1144 Explore P: 0.3959\n",
      "Episode: 2248 Total reward: -4.0878753662109375 Training loss: 0.4466 Explore P: 0.3958\n",
      "Episode: 2249 Total reward: 18.692306518554688 Training loss: 0.2972 Explore P: 0.3956\n",
      "Episode: 2250 Total reward: -21.9761962890625 Training loss: 0.4133 Explore P: 0.3955\n",
      "Model Saved\n",
      "Episode: 2251 Total reward: 11.262710571289062 Training loss: 0.4081 Explore P: 0.3953\n",
      "Episode: 2252 Total reward: -0.846893310546875 Training loss: 0.3714 Explore P: 0.3951\n",
      "Episode: 2253 Total reward: -84.00555419921875 Training loss: 0.3498 Explore P: 0.3951\n",
      "Episode: 2254 Total reward: -8.43701171875 Training loss: 0.3358 Explore P: 0.3950\n",
      "Episode: 2255 Total reward: -17.884780883789062 Training loss: 1.2976 Explore P: 0.3948\n",
      "Model Saved\n",
      "Episode: 2256 Total reward: 94.68533325195312 Training loss: 0.3093 Explore P: 0.3946\n",
      "Episode: 2257 Total reward: 4.3527679443359375 Training loss: 0.1756 Explore P: 0.3945\n",
      "Episode: 2258 Total reward: -20.295883178710938 Training loss: 0.2450 Explore P: 0.3944\n",
      "Episode: 2259 Total reward: -28.937210083007812 Training loss: 0.6915 Explore P: 0.3942\n",
      "Episode: 2260 Total reward: -17.712814331054688 Training loss: 0.4522 Explore P: 0.3941\n",
      "Model Saved\n",
      "Episode: 2261 Total reward: -50.60215759277344 Training loss: 0.3446 Explore P: 0.3939\n",
      "Episode: 2262 Total reward: -32.52098083496094 Training loss: 0.3620 Explore P: 0.3938\n",
      "Episode: 2263 Total reward: -19.964797973632812 Training loss: 1.5432 Explore P: 0.3936\n",
      "Episode: 2264 Total reward: -12.207275390625 Training loss: 0.2152 Explore P: 0.3935\n",
      "Episode: 2265 Total reward: -2.535888671875 Training loss: 0.4949 Explore P: 0.3933\n",
      "Model Saved\n",
      "Episode: 2266 Total reward: -34.41505432128906 Training loss: 5.0552 Explore P: 0.3932\n",
      "Episode: 2267 Total reward: 30.694931030273438 Training loss: 0.5460 Explore P: 0.3930\n",
      "Episode: 2268 Total reward: -16.208297729492188 Training loss: 0.3303 Explore P: 0.3928\n",
      "Episode: 2269 Total reward: 49.340484619140625 Training loss: 1.0330 Explore P: 0.3927\n",
      "Episode: 2270 Total reward: 35.882598876953125 Training loss: 0.8804 Explore P: 0.3925\n",
      "Model Saved\n",
      "Episode: 2271 Total reward: 23.44976806640625 Training loss: 1.1457 Explore P: 0.3924\n",
      "Episode: 2272 Total reward: -44.44255065917969 Training loss: 0.8727 Explore P: 0.3922\n",
      "Episode: 2273 Total reward: 9.066848754882812 Training loss: 0.5308 Explore P: 0.3921\n",
      "Episode: 2274 Total reward: -9.654708862304688 Training loss: 13.5842 Explore P: 0.3919\n",
      "Episode: 2275 Total reward: 66.54315185546875 Training loss: 1.0681 Explore P: 0.3917\n",
      "Model Saved\n",
      "Episode: 2276 Total reward: -9.908279418945312 Training loss: 0.2473 Explore P: 0.3916\n",
      "Episode: 2277 Total reward: 6.8238372802734375 Training loss: 3.1832 Explore P: 0.3914\n",
      "Episode: 2278 Total reward: -91.39396667480469 Training loss: 1.1927 Explore P: 0.3913\n",
      "Episode: 2279 Total reward: -38.813934326171875 Training loss: 0.7507 Explore P: 0.3912\n",
      "Episode: 2280 Total reward: -88.61341857910156 Training loss: 0.5863 Explore P: 0.3911\n",
      "Model Saved\n",
      "Episode: 2281 Total reward: 27.531143188476562 Training loss: 0.3792 Explore P: 0.3909\n",
      "Episode: 2282 Total reward: -49.77342224121094 Training loss: 0.7392 Explore P: 0.3908\n",
      "Episode: 2283 Total reward: 11.3145751953125 Training loss: 0.2879 Explore P: 0.3906\n",
      "Episode: 2284 Total reward: -44.50605773925781 Training loss: 1.4968 Explore P: 0.3906\n",
      "Episode: 2285 Total reward: -80.38735961914062 Training loss: 1.2009 Explore P: 0.3904\n",
      "Model Saved\n",
      "Episode: 2286 Total reward: -8.975570678710938 Training loss: 0.8264 Explore P: 0.3903\n",
      "Episode: 2287 Total reward: -102.9970703125 Training loss: 0.8819 Explore P: 0.3902\n",
      "Episode: 2288 Total reward: -24.139419555664062 Training loss: 0.3645 Explore P: 0.3900\n",
      "Episode: 2289 Total reward: -11.893814086914062 Training loss: 2.7225 Explore P: 0.3899\n",
      "Episode: 2290 Total reward: 19.087112426757812 Training loss: 1.7085 Explore P: 0.3897\n",
      "Model Saved\n",
      "Episode: 2291 Total reward: -50.85911560058594 Training loss: 0.4195 Explore P: 0.3894\n",
      "Episode: 2292 Total reward: -55.06520080566406 Training loss: 1.9618 Explore P: 0.3893\n",
      "Episode: 2293 Total reward: -22.424163818359375 Training loss: 0.1507 Explore P: 0.3892\n",
      "Episode: 2294 Total reward: -31.117630004882812 Training loss: 5.6726 Explore P: 0.3890\n",
      "Episode: 2295 Total reward: -76.53643798828125 Training loss: 0.2796 Explore P: 0.3890\n",
      "Model Saved\n",
      "Episode: 2296 Total reward: -24.99822998046875 Training loss: 0.1874 Explore P: 0.3888\n",
      "Episode: 2297 Total reward: -49.512420654296875 Training loss: 0.2853 Explore P: 0.3886\n",
      "Episode: 2298 Total reward: -49.452667236328125 Training loss: 2.0281 Explore P: 0.3885\n",
      "Episode: 2299 Total reward: 38.05799865722656 Training loss: 0.5838 Explore P: 0.3884\n",
      "Episode: 2300 Total reward: -9.244522094726562 Training loss: 3.4800 Explore P: 0.3882\n",
      "Model Saved\n",
      "Episode: 2301 Total reward: -91.33419799804688 Training loss: 0.7963 Explore P: 0.3880\n",
      "Episode: 2302 Total reward: -35.21000671386719 Training loss: 0.2007 Explore P: 0.3878\n",
      "Episode: 2303 Total reward: -11.153106689453125 Training loss: 0.2847 Explore P: 0.3876\n",
      "Episode: 2304 Total reward: 12.887039184570312 Training loss: 0.2190 Explore P: 0.3875\n",
      "Episode: 2305 Total reward: -29.62957763671875 Training loss: 0.3988 Explore P: 0.3874\n",
      "Model Saved\n",
      "Episode: 2306 Total reward: 41.2115478515625 Training loss: 0.9914 Explore P: 0.3873\n",
      "Episode: 2307 Total reward: -30.250167846679688 Training loss: 0.7111 Explore P: 0.3871\n",
      "Episode: 2308 Total reward: -12.077667236328125 Training loss: 14.2186 Explore P: 0.3869\n",
      "Episode: 2309 Total reward: 6.30084228515625 Training loss: 1.7008 Explore P: 0.3868\n",
      "Episode: 2310 Total reward: -31.96875 Training loss: 0.5465 Explore P: 0.3866\n",
      "Model Saved\n",
      "Episode: 2311 Total reward: -46.519927978515625 Training loss: 0.4877 Explore P: 0.3864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2312 Total reward: -0.8868865966796875 Training loss: 1.0343 Explore P: 0.3863\n",
      "Episode: 2313 Total reward: 46.5220947265625 Training loss: 0.3236 Explore P: 0.3861\n",
      "Episode: 2314 Total reward: 89.11686706542969 Training loss: 2.6473 Explore P: 0.3860\n",
      "Episode: 2315 Total reward: -27.362136840820312 Training loss: 0.3314 Explore P: 0.3858\n",
      "Model Saved\n",
      "Episode: 2316 Total reward: -15.900146484375 Training loss: 0.1906 Explore P: 0.3856\n",
      "Episode: 2317 Total reward: 4.6962738037109375 Training loss: 0.2923 Explore P: 0.3855\n",
      "Episode: 2318 Total reward: -32.70780944824219 Training loss: 1.2767 Explore P: 0.3853\n",
      "Episode: 2319 Total reward: 35.5758056640625 Training loss: 1.0274 Explore P: 0.3852\n",
      "Episode: 2320 Total reward: -80.88156127929688 Training loss: 7.3514 Explore P: 0.3850\n",
      "Model Saved\n",
      "Episode: 2321 Total reward: -97.81341552734375 Training loss: 0.3799 Explore P: 0.3849\n",
      "Episode: 2322 Total reward: 53.28514099121094 Training loss: 0.3109 Explore P: 0.3847\n",
      "Episode: 2323 Total reward: -75.85525512695312 Training loss: 9.1874 Explore P: 0.3846\n",
      "Episode: 2324 Total reward: -47.56132507324219 Training loss: 1.5600 Explore P: 0.3845\n",
      "Episode: 2325 Total reward: -98.45404052734375 Training loss: 0.5762 Explore P: 0.3844\n",
      "Model Saved\n",
      "Episode: 2326 Total reward: -68.41705322265625 Training loss: 0.2174 Explore P: 0.3843\n",
      "Episode: 2327 Total reward: 30.560134887695312 Training loss: 0.1530 Explore P: 0.3842\n",
      "Episode: 2328 Total reward: -76.39044189453125 Training loss: 0.8814 Explore P: 0.3840\n",
      "Episode: 2329 Total reward: 12.094268798828125 Training loss: 0.3127 Explore P: 0.3839\n",
      "Episode: 2330 Total reward: 11.427276611328125 Training loss: 0.3026 Explore P: 0.3837\n",
      "Model Saved\n",
      "Episode: 2331 Total reward: 11.867340087890625 Training loss: 1.6247 Explore P: 0.3836\n",
      "Episode: 2332 Total reward: -10.213226318359375 Training loss: 0.3602 Explore P: 0.3834\n",
      "Episode: 2333 Total reward: -11.58966064453125 Training loss: 0.7563 Explore P: 0.3833\n",
      "Episode: 2334 Total reward: 54.63591003417969 Training loss: 0.3144 Explore P: 0.3831\n",
      "Episode: 2335 Total reward: -17.798202514648438 Training loss: 0.1504 Explore P: 0.3830\n",
      "Model Saved\n",
      "Episode: 2336 Total reward: 34.04217529296875 Training loss: 0.3087 Explore P: 0.3828\n",
      "Episode: 2337 Total reward: 20.464691162109375 Training loss: 0.5565 Explore P: 0.3827\n",
      "Episode: 2338 Total reward: -18.006805419921875 Training loss: 1.3426 Explore P: 0.3825\n",
      "Episode: 2339 Total reward: 26.282730102539062 Training loss: 0.2358 Explore P: 0.3824\n",
      "Episode: 2340 Total reward: 31.59893798828125 Training loss: 0.1851 Explore P: 0.3822\n",
      "Model Saved\n",
      "Episode: 2341 Total reward: 20.942428588867188 Training loss: 1.4692 Explore P: 0.3821\n",
      "Episode: 2342 Total reward: 6.918548583984375 Training loss: 2.0466 Explore P: 0.3819\n",
      "Episode: 2343 Total reward: -60.36250305175781 Training loss: 0.1574 Explore P: 0.3818\n",
      "Episode: 2344 Total reward: 21.501205444335938 Training loss: 4.3986 Explore P: 0.3816\n",
      "Episode: 2345 Total reward: -74.240478515625 Training loss: 1.2830 Explore P: 0.3815\n",
      "Model Saved\n",
      "Episode: 2346 Total reward: -21.8509521484375 Training loss: 0.6097 Explore P: 0.3814\n",
      "Episode: 2347 Total reward: -20.06439208984375 Training loss: 0.5517 Explore P: 0.3813\n",
      "Episode: 2348 Total reward: 0.1745758056640625 Training loss: 0.3413 Explore P: 0.3811\n",
      "Episode: 2349 Total reward: 3.7562103271484375 Training loss: 0.1528 Explore P: 0.3810\n",
      "Episode: 2350 Total reward: -47.28437805175781 Training loss: 1.6438 Explore P: 0.3808\n",
      "Model Saved\n",
      "Episode: 2351 Total reward: -40.90435791015625 Training loss: 0.2147 Explore P: 0.3807\n",
      "Episode: 2352 Total reward: -51.98809814453125 Training loss: 0.2391 Explore P: 0.3806\n",
      "Episode: 2353 Total reward: -30.729507446289062 Training loss: 1.0362 Explore P: 0.3805\n",
      "Episode: 2354 Total reward: 12.621337890625 Training loss: 0.4773 Explore P: 0.3803\n",
      "Episode: 2355 Total reward: -15.067657470703125 Training loss: 0.2122 Explore P: 0.3802\n",
      "Model Saved\n",
      "Episode: 2356 Total reward: -30.777481079101562 Training loss: 1.1556 Explore P: 0.3801\n",
      "Episode: 2357 Total reward: 52.97747802734375 Training loss: 0.7254 Explore P: 0.3800\n",
      "Episode: 2358 Total reward: 1.7759857177734375 Training loss: 1.3640 Explore P: 0.3798\n",
      "Episode: 2359 Total reward: -65.22328186035156 Training loss: 0.4765 Explore P: 0.3797\n",
      "Episode: 2360 Total reward: 18.647201538085938 Training loss: 0.2900 Explore P: 0.3796\n",
      "Model Saved\n",
      "Episode: 2361 Total reward: -39.470947265625 Training loss: 0.6451 Explore P: 0.3794\n",
      "Episode: 2362 Total reward: -55.46903991699219 Training loss: 0.1896 Explore P: 0.3793\n",
      "Episode: 2363 Total reward: 25.856369018554688 Training loss: 1.8887 Explore P: 0.3792\n",
      "Episode: 2364 Total reward: -12.881805419921875 Training loss: 0.4959 Explore P: 0.3790\n",
      "Episode: 2365 Total reward: -2.727691650390625 Training loss: 0.2301 Explore P: 0.3789\n",
      "Model Saved\n",
      "Episode: 2366 Total reward: 34.7518310546875 Training loss: 0.3650 Explore P: 0.3787\n",
      "Episode: 2367 Total reward: -31.190460205078125 Training loss: 1.8760 Explore P: 0.3786\n",
      "Episode: 2368 Total reward: 43.87794494628906 Training loss: 0.1625 Explore P: 0.3785\n",
      "Episode: 2369 Total reward: 40.30072021484375 Training loss: 0.3636 Explore P: 0.3783\n",
      "Episode: 2370 Total reward: 5.652923583984375 Training loss: 0.4444 Explore P: 0.3781\n",
      "Model Saved\n",
      "Episode: 2371 Total reward: -33.90217590332031 Training loss: 0.3184 Explore P: 0.3780\n",
      "Episode: 2372 Total reward: -42.546783447265625 Training loss: 0.6591 Explore P: 0.3778\n",
      "Episode: 2373 Total reward: 63.28460693359375 Training loss: 1.0931 Explore P: 0.3777\n",
      "Episode: 2374 Total reward: -78.50848388671875 Training loss: 0.4700 Explore P: 0.3776\n",
      "Episode: 2375 Total reward: -30.595947265625 Training loss: 0.3221 Explore P: 0.3774\n",
      "Model Saved\n",
      "Episode: 2376 Total reward: 6.8155517578125 Training loss: 0.1931 Explore P: 0.3773\n",
      "Episode: 2377 Total reward: 32.36669921875 Training loss: 20.9193 Explore P: 0.3772\n",
      "Episode: 2378 Total reward: -22.693618774414062 Training loss: 0.3579 Explore P: 0.3770\n",
      "Episode: 2379 Total reward: -57.81001281738281 Training loss: 0.1184 Explore P: 0.3769\n",
      "Episode: 2380 Total reward: -28.843093872070312 Training loss: 0.5746 Explore P: 0.3767\n",
      "Model Saved\n",
      "Episode: 2381 Total reward: 61.10069274902344 Training loss: 0.3143 Explore P: 0.3766\n",
      "Episode: 2382 Total reward: 4.4194183349609375 Training loss: 1.2468 Explore P: 0.3764\n",
      "Episode: 2383 Total reward: -7.2637786865234375 Training loss: 0.2766 Explore P: 0.3763\n",
      "Episode: 2384 Total reward: -28.163070678710938 Training loss: 0.2536 Explore P: 0.3762\n",
      "Episode: 2385 Total reward: -65.6343994140625 Training loss: 0.3492 Explore P: 0.3761\n",
      "Model Saved\n",
      "Episode: 2386 Total reward: 87.95504760742188 Training loss: 18.1104 Explore P: 0.3759\n",
      "Episode: 2387 Total reward: -11.07977294921875 Training loss: 0.4011 Explore P: 0.3758\n",
      "Episode: 2388 Total reward: -83.65695190429688 Training loss: 0.4062 Explore P: 0.3756\n",
      "Episode: 2389 Total reward: -1.8516693115234375 Training loss: 0.2234 Explore P: 0.3755\n",
      "Episode: 2390 Total reward: 5.1023101806640625 Training loss: 0.1654 Explore P: 0.3754\n",
      "Model Saved\n",
      "Episode: 2391 Total reward: -21.201873779296875 Training loss: 0.2665 Explore P: 0.3753\n",
      "Episode: 2392 Total reward: -16.486465454101562 Training loss: 0.5854 Explore P: 0.3751\n",
      "Episode: 2393 Total reward: -8.951812744140625 Training loss: 18.1932 Explore P: 0.3750\n",
      "Episode: 2394 Total reward: -11.558578491210938 Training loss: 0.2077 Explore P: 0.3748\n",
      "Episode: 2395 Total reward: -83.80255126953125 Training loss: 0.2100 Explore P: 0.3747\n",
      "Model Saved\n",
      "Episode: 2396 Total reward: -10.3756103515625 Training loss: 0.6017 Explore P: 0.3746\n",
      "Episode: 2397 Total reward: -7.908447265625 Training loss: 0.4870 Explore P: 0.3745\n",
      "Episode: 2398 Total reward: 61.82965087890625 Training loss: 1.1028 Explore P: 0.3743\n",
      "Episode: 2399 Total reward: -40.32612609863281 Training loss: 0.9566 Explore P: 0.3742\n",
      "Episode: 2400 Total reward: -72.62770080566406 Training loss: 0.3328 Explore P: 0.3741\n",
      "Model Saved\n",
      "Episode: 2401 Total reward: 27.9759521484375 Training loss: 0.2860 Explore P: 0.3739\n",
      "Episode: 2402 Total reward: -30.544998168945312 Training loss: 0.7123 Explore P: 0.3738\n",
      "Episode: 2403 Total reward: -18.429412841796875 Training loss: 17.5619 Explore P: 0.3736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2404 Total reward: -73.1607666015625 Training loss: 14.5019 Explore P: 0.3736\n",
      "Episode: 2405 Total reward: -48.47508239746094 Training loss: 0.2950 Explore P: 0.3734\n",
      "Model Saved\n",
      "Episode: 2406 Total reward: 28.433837890625 Training loss: 0.4802 Explore P: 0.3733\n",
      "Episode: 2407 Total reward: 21.669540405273438 Training loss: 0.1783 Explore P: 0.3732\n",
      "Episode: 2408 Total reward: -69.20222473144531 Training loss: 3.5737 Explore P: 0.3731\n",
      "Episode: 2409 Total reward: 50.41981506347656 Training loss: 0.4433 Explore P: 0.3729\n",
      "Episode: 2410 Total reward: -76.35707092285156 Training loss: 0.2116 Explore P: 0.3728\n",
      "Model Saved\n",
      "Episode: 2411 Total reward: -13.51739501953125 Training loss: 0.5485 Explore P: 0.3727\n",
      "Episode: 2412 Total reward: -20.434173583984375 Training loss: 0.2331 Explore P: 0.3725\n",
      "Episode: 2413 Total reward: -47.94427490234375 Training loss: 0.1609 Explore P: 0.3723\n",
      "Episode: 2414 Total reward: -39.752716064453125 Training loss: 1.0870 Explore P: 0.3722\n",
      "Episode: 2415 Total reward: -9.341949462890625 Training loss: 0.2600 Explore P: 0.3720\n",
      "Model Saved\n",
      "Episode: 2416 Total reward: 30.789596557617188 Training loss: 0.5475 Explore P: 0.3719\n",
      "Episode: 2417 Total reward: -31.903121948242188 Training loss: 1.1352 Explore P: 0.3717\n",
      "Episode: 2418 Total reward: -12.0699462890625 Training loss: 0.6484 Explore P: 0.3716\n",
      "Episode: 2419 Total reward: -53.536834716796875 Training loss: 0.2416 Explore P: 0.3715\n",
      "Episode: 2420 Total reward: -55.24266052246094 Training loss: 0.1923 Explore P: 0.3713\n",
      "Model Saved\n",
      "Episode: 2421 Total reward: -60.0499267578125 Training loss: 2.6212 Explore P: 0.3712\n",
      "Episode: 2422 Total reward: 45.197113037109375 Training loss: 0.6909 Explore P: 0.3711\n",
      "Episode: 2423 Total reward: -89.66453552246094 Training loss: 0.3779 Explore P: 0.3709\n",
      "Episode: 2424 Total reward: 12.988876342773438 Training loss: 0.2832 Explore P: 0.3707\n",
      "Episode: 2425 Total reward: -35.23435974121094 Training loss: 0.3522 Explore P: 0.3706\n",
      "Model Saved\n",
      "Episode: 2426 Total reward: -5.5556182861328125 Training loss: 0.2882 Explore P: 0.3705\n",
      "Episode: 2427 Total reward: -55.6497802734375 Training loss: 0.6890 Explore P: 0.3704\n",
      "Episode: 2428 Total reward: -19.858688354492188 Training loss: 1.0077 Explore P: 0.3702\n",
      "Episode: 2429 Total reward: -13.4586181640625 Training loss: 1.5349 Explore P: 0.3701\n",
      "Episode: 2430 Total reward: 53.323333740234375 Training loss: 0.9900 Explore P: 0.3699\n",
      "Model Saved\n",
      "Episode: 2431 Total reward: 0.7342681884765625 Training loss: 0.6387 Explore P: 0.3698\n",
      "Episode: 2432 Total reward: 42.262939453125 Training loss: 7.5728 Explore P: 0.3697\n",
      "Episode: 2433 Total reward: -6.6466217041015625 Training loss: 0.4864 Explore P: 0.3695\n",
      "Episode: 2434 Total reward: 2.7541656494140625 Training loss: 0.2479 Explore P: 0.3694\n",
      "Episode: 2435 Total reward: -13.55657958984375 Training loss: 0.2436 Explore P: 0.3692\n",
      "Model Saved\n",
      "Episode: 2436 Total reward: 51.198577880859375 Training loss: 0.4764 Explore P: 0.3691\n",
      "Episode: 2437 Total reward: -24.977340698242188 Training loss: 0.6189 Explore P: 0.3689\n",
      "Episode: 2438 Total reward: 40.162811279296875 Training loss: 1.8985 Explore P: 0.3688\n",
      "Episode: 2439 Total reward: -68.08892822265625 Training loss: 0.2303 Explore P: 0.3687\n",
      "Episode: 2440 Total reward: -11.416732788085938 Training loss: 0.3833 Explore P: 0.3685\n",
      "Model Saved\n",
      "Episode: 2441 Total reward: 19.7412109375 Training loss: 0.2206 Explore P: 0.3684\n",
      "Episode: 2442 Total reward: 9.277130126953125 Training loss: 0.6526 Explore P: 0.3682\n",
      "Episode: 2443 Total reward: -70.06610107421875 Training loss: 0.5379 Explore P: 0.3682\n",
      "Episode: 2444 Total reward: -32.471710205078125 Training loss: 0.8985 Explore P: 0.3681\n",
      "Episode: 2445 Total reward: 14.486618041992188 Training loss: 0.4011 Explore P: 0.3679\n",
      "Model Saved\n",
      "Episode: 2446 Total reward: -21.750320434570312 Training loss: 1.6577 Explore P: 0.3679\n",
      "Episode: 2447 Total reward: 20.575607299804688 Training loss: 0.2614 Explore P: 0.3677\n",
      "Episode: 2448 Total reward: 64.11299133300781 Training loss: 0.4066 Explore P: 0.3676\n",
      "Episode: 2449 Total reward: -31.370574951171875 Training loss: 1.6570 Explore P: 0.3675\n",
      "Episode: 2450 Total reward: -4.0277862548828125 Training loss: 14.2623 Explore P: 0.3673\n",
      "Model Saved\n",
      "Episode: 2451 Total reward: -13.142776489257812 Training loss: 0.3329 Explore P: 0.3672\n",
      "Episode: 2452 Total reward: -79.03132629394531 Training loss: 0.2986 Explore P: 0.3670\n",
      "Episode: 2453 Total reward: -19.109359741210938 Training loss: 0.2433 Explore P: 0.3670\n",
      "Episode: 2454 Total reward: 16.86346435546875 Training loss: 0.1653 Explore P: 0.3669\n",
      "Episode: 2455 Total reward: -7.7622222900390625 Training loss: 0.4531 Explore P: 0.3667\n",
      "Model Saved\n",
      "Episode: 2456 Total reward: -6.4436798095703125 Training loss: 0.6174 Explore P: 0.3666\n",
      "Episode: 2457 Total reward: 23.634109497070312 Training loss: 2.5871 Explore P: 0.3664\n",
      "Episode: 2458 Total reward: 41.50596618652344 Training loss: 1.2010 Explore P: 0.3663\n",
      "Episode: 2459 Total reward: 79.81573486328125 Training loss: 15.4692 Explore P: 0.3661\n",
      "Episode: 2460 Total reward: 39.5234375 Training loss: 16.6623 Explore P: 0.3660\n",
      "Model Saved\n",
      "Episode: 2461 Total reward: 40.987518310546875 Training loss: 0.3257 Explore P: 0.3658\n",
      "Episode: 2462 Total reward: 27.1297607421875 Training loss: 1.1191 Explore P: 0.3657\n",
      "Episode: 2463 Total reward: -3.086273193359375 Training loss: 0.3437 Explore P: 0.3655\n",
      "Episode: 2464 Total reward: 131.1254119873047 Training loss: 1.1978 Explore P: 0.3654\n",
      "Episode: 2465 Total reward: 73.54075622558594 Training loss: 0.1747 Explore P: 0.3653\n",
      "Model Saved\n",
      "Episode: 2466 Total reward: -60.357177734375 Training loss: 0.7700 Explore P: 0.3652\n",
      "Episode: 2467 Total reward: 9.879180908203125 Training loss: 0.6984 Explore P: 0.3650\n",
      "Episode: 2468 Total reward: -8.983551025390625 Training loss: 0.7341 Explore P: 0.3649\n",
      "Episode: 2469 Total reward: 46.00254821777344 Training loss: 0.4575 Explore P: 0.3648\n",
      "Episode: 2470 Total reward: -27.572463989257812 Training loss: 0.6066 Explore P: 0.3646\n",
      "Model Saved\n",
      "Episode: 2471 Total reward: -34.20753479003906 Training loss: 0.7390 Explore P: 0.3645\n",
      "Episode: 2472 Total reward: -50.74250793457031 Training loss: 0.1700 Explore P: 0.3643\n",
      "Episode: 2473 Total reward: -18.178909301757812 Training loss: 0.7111 Explore P: 0.3642\n",
      "Episode: 2474 Total reward: -63.28498840332031 Training loss: 1.0255 Explore P: 0.3641\n",
      "Episode: 2475 Total reward: -6.0406036376953125 Training loss: 0.8829 Explore P: 0.3639\n",
      "Model Saved\n",
      "Episode: 2476 Total reward: 61.31884765625 Training loss: 0.2226 Explore P: 0.3638\n",
      "Episode: 2477 Total reward: -8.637176513671875 Training loss: 0.2110 Explore P: 0.3636\n",
      "Episode: 2478 Total reward: 19.912811279296875 Training loss: 0.2565 Explore P: 0.3635\n",
      "Episode: 2479 Total reward: 77.20809936523438 Training loss: 0.3526 Explore P: 0.3633\n",
      "Episode: 2480 Total reward: -17.503738403320312 Training loss: 1.4808 Explore P: 0.3632\n",
      "Model Saved\n",
      "Episode: 2481 Total reward: -13.039413452148438 Training loss: 1.7902 Explore P: 0.3630\n",
      "Episode: 2482 Total reward: -26.50445556640625 Training loss: 0.8486 Explore P: 0.3629\n",
      "Episode: 2483 Total reward: -8.790267944335938 Training loss: 0.7536 Explore P: 0.3628\n",
      "Episode: 2484 Total reward: -32.39210510253906 Training loss: 0.9192 Explore P: 0.3627\n",
      "Episode: 2485 Total reward: 30.80419921875 Training loss: 0.1714 Explore P: 0.3625\n",
      "Model Saved\n",
      "Episode: 2486 Total reward: 24.380126953125 Training loss: 0.3297 Explore P: 0.3624\n",
      "Episode: 2487 Total reward: -36.31805419921875 Training loss: 0.2877 Explore P: 0.3623\n",
      "Episode: 2488 Total reward: -9.047012329101562 Training loss: 0.4853 Explore P: 0.3622\n",
      "Episode: 2489 Total reward: 46.580291748046875 Training loss: 0.9967 Explore P: 0.3620\n",
      "Episode: 2490 Total reward: 8.495223999023438 Training loss: 1.3035 Explore P: 0.3619\n",
      "Model Saved\n",
      "Episode: 2491 Total reward: -74.515625 Training loss: 0.7417 Explore P: 0.3618\n",
      "Episode: 2492 Total reward: -27.362030029296875 Training loss: 0.5221 Explore P: 0.3617\n",
      "Episode: 2493 Total reward: 38.42547607421875 Training loss: 1.4552 Explore P: 0.3616\n",
      "Episode: 2494 Total reward: 16.83697509765625 Training loss: 0.4767 Explore P: 0.3614\n",
      "Episode: 2495 Total reward: 31.155960083007812 Training loss: 0.2812 Explore P: 0.3613\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2496 Total reward: 16.16583251953125 Training loss: 0.4060 Explore P: 0.3611\n",
      "Episode: 2497 Total reward: -18.386627197265625 Training loss: 0.7756 Explore P: 0.3610\n",
      "Episode: 2498 Total reward: 27.787063598632812 Training loss: 0.2315 Explore P: 0.3608\n",
      "Episode: 2499 Total reward: -10.233535766601562 Training loss: 0.3797 Explore P: 0.3607\n",
      "Episode: 2500 Total reward: 62.781646728515625 Training loss: 0.9558 Explore P: 0.3605\n",
      "Model Saved\n",
      "Episode: 2501 Total reward: 19.265884399414062 Training loss: 0.3156 Explore P: 0.3604\n",
      "Episode: 2502 Total reward: 63.953460693359375 Training loss: 0.1904 Explore P: 0.3603\n",
      "Episode: 2503 Total reward: -15.7781982421875 Training loss: 0.3280 Explore P: 0.3602\n",
      "Episode: 2504 Total reward: 5.4593353271484375 Training loss: 0.1859 Explore P: 0.3600\n",
      "Episode: 2505 Total reward: 2.973388671875 Training loss: 5.9975 Explore P: 0.3599\n",
      "Model Saved\n",
      "Episode: 2506 Total reward: -49.84480285644531 Training loss: 0.3077 Explore P: 0.3598\n",
      "Episode: 2507 Total reward: 39.91893005371094 Training loss: 0.5632 Explore P: 0.3596\n",
      "Episode: 2508 Total reward: -5.3174591064453125 Training loss: 1.3634 Explore P: 0.3595\n",
      "Episode: 2509 Total reward: -28.686630249023438 Training loss: 0.4333 Explore P: 0.3594\n",
      "Episode: 2510 Total reward: -18.883255004882812 Training loss: 0.2696 Explore P: 0.3593\n",
      "Model Saved\n",
      "Episode: 2511 Total reward: -18.166107177734375 Training loss: 0.5634 Explore P: 0.3591\n",
      "Episode: 2512 Total reward: 25.130996704101562 Training loss: 0.3449 Explore P: 0.3590\n",
      "Episode: 2513 Total reward: 36.64857482910156 Training loss: 0.1929 Explore P: 0.3588\n",
      "Episode: 2514 Total reward: -0.7557525634765625 Training loss: 0.6739 Explore P: 0.3587\n",
      "Episode: 2515 Total reward: -7.393035888671875 Training loss: 0.2883 Explore P: 0.3586\n",
      "Model Saved\n",
      "Episode: 2516 Total reward: 10.510009765625 Training loss: 2.0455 Explore P: 0.3585\n",
      "Episode: 2517 Total reward: 33.86773681640625 Training loss: 0.8720 Explore P: 0.3583\n",
      "Episode: 2518 Total reward: 9.044830322265625 Training loss: 0.5988 Explore P: 0.3582\n",
      "Episode: 2519 Total reward: -21.069595336914062 Training loss: 0.7030 Explore P: 0.3580\n",
      "Episode: 2520 Total reward: 18.436187744140625 Training loss: 0.6769 Explore P: 0.3579\n",
      "Model Saved\n",
      "Episode: 2521 Total reward: 39.40071105957031 Training loss: 0.3453 Explore P: 0.3577\n",
      "Episode: 2522 Total reward: -81.91322326660156 Training loss: 0.2371 Explore P: 0.3577\n",
      "Episode: 2523 Total reward: -37.75444030761719 Training loss: 0.3400 Explore P: 0.3575\n",
      "Episode: 2524 Total reward: -73.59599304199219 Training loss: 0.2651 Explore P: 0.3574\n",
      "Episode: 2525 Total reward: -2.021331787109375 Training loss: 1.6469 Explore P: 0.3572\n",
      "Model Saved\n",
      "Episode: 2526 Total reward: -55.75291442871094 Training loss: 0.5520 Explore P: 0.3572\n",
      "Episode: 2527 Total reward: -29.352935791015625 Training loss: 0.3870 Explore P: 0.3571\n",
      "Episode: 2528 Total reward: 33.73753356933594 Training loss: 0.3037 Explore P: 0.3569\n",
      "Episode: 2529 Total reward: -10.750381469726562 Training loss: 2.5791 Explore P: 0.3568\n",
      "Episode: 2530 Total reward: -7.3818359375 Training loss: 0.2349 Explore P: 0.3566\n",
      "Model Saved\n",
      "Episode: 2531 Total reward: -6.7445526123046875 Training loss: 0.9372 Explore P: 0.3565\n",
      "Episode: 2532 Total reward: -7.8434295654296875 Training loss: 0.2832 Explore P: 0.3564\n",
      "Episode: 2533 Total reward: -82.04707336425781 Training loss: 1.5401 Explore P: 0.3562\n",
      "Episode: 2534 Total reward: -6.5564422607421875 Training loss: 3.0396 Explore P: 0.3561\n",
      "Episode: 2535 Total reward: 92.06932067871094 Training loss: 0.2064 Explore P: 0.3560\n",
      "Model Saved\n",
      "Episode: 2536 Total reward: -24.152069091796875 Training loss: 0.5931 Explore P: 0.3558\n",
      "Episode: 2537 Total reward: 56.23884582519531 Training loss: 0.9877 Explore P: 0.3557\n",
      "Episode: 2538 Total reward: 34.67845153808594 Training loss: 0.2311 Explore P: 0.3555\n",
      "Episode: 2539 Total reward: -38.902557373046875 Training loss: 0.5499 Explore P: 0.3554\n",
      "Episode: 2540 Total reward: 24.177444458007812 Training loss: 1.6994 Explore P: 0.3553\n",
      "Model Saved\n",
      "Episode: 2541 Total reward: -17.618881225585938 Training loss: 10.8236 Explore P: 0.3552\n",
      "Episode: 2542 Total reward: -30.0721435546875 Training loss: 0.2974 Explore P: 0.3550\n",
      "Episode: 2543 Total reward: -70.58573913574219 Training loss: 0.4679 Explore P: 0.3550\n",
      "Episode: 2544 Total reward: -63.07905578613281 Training loss: 0.7675 Explore P: 0.3549\n",
      "Episode: 2545 Total reward: -45.955291748046875 Training loss: 3.0978 Explore P: 0.3548\n",
      "Model Saved\n",
      "Episode: 2546 Total reward: -45.891357421875 Training loss: 4.7320 Explore P: 0.3547\n",
      "Episode: 2547 Total reward: -4.9220733642578125 Training loss: 0.4499 Explore P: 0.3545\n",
      "Episode: 2548 Total reward: -25.38677978515625 Training loss: 0.2014 Explore P: 0.3544\n",
      "Episode: 2549 Total reward: 15.6702880859375 Training loss: 0.4194 Explore P: 0.3543\n",
      "Episode: 2550 Total reward: 5.9474029541015625 Training loss: 0.3549 Explore P: 0.3542\n",
      "Model Saved\n",
      "Episode: 2551 Total reward: -5.2779541015625 Training loss: 0.2364 Explore P: 0.3540\n",
      "Episode: 2552 Total reward: -12.672073364257812 Training loss: 0.7618 Explore P: 0.3539\n",
      "Episode: 2553 Total reward: -15.241531372070312 Training loss: 2.8753 Explore P: 0.3538\n",
      "Episode: 2554 Total reward: -45.455810546875 Training loss: 0.9721 Explore P: 0.3537\n",
      "Episode: 2555 Total reward: -15.2135009765625 Training loss: 1.4315 Explore P: 0.3536\n",
      "Model Saved\n",
      "Episode: 2556 Total reward: 9.901138305664062 Training loss: 0.1750 Explore P: 0.3535\n",
      "Episode: 2557 Total reward: -18.07061767578125 Training loss: 0.3935 Explore P: 0.3533\n",
      "Episode: 2558 Total reward: -10.08050537109375 Training loss: 0.2326 Explore P: 0.3532\n",
      "Episode: 2559 Total reward: -16.054397583007812 Training loss: 2.6148 Explore P: 0.3530\n",
      "Episode: 2560 Total reward: 61.1405029296875 Training loss: 0.1981 Explore P: 0.3529\n",
      "Model Saved\n",
      "Episode: 2561 Total reward: 0.5958251953125 Training loss: 1.0659 Explore P: 0.3528\n",
      "Episode: 2562 Total reward: 29.9326171875 Training loss: 0.2740 Explore P: 0.3526\n",
      "Episode: 2563 Total reward: -16.159561157226562 Training loss: 4.8367 Explore P: 0.3524\n",
      "Episode: 2564 Total reward: 19.966720581054688 Training loss: 0.3837 Explore P: 0.3523\n",
      "Episode: 2565 Total reward: -22.628265380859375 Training loss: 0.3387 Explore P: 0.3522\n",
      "Model Saved\n",
      "Episode: 2566 Total reward: 3.5012969970703125 Training loss: 14.2574 Explore P: 0.3520\n",
      "Episode: 2567 Total reward: 1.9921722412109375 Training loss: 18.7761 Explore P: 0.3519\n",
      "Episode: 2568 Total reward: -62.867950439453125 Training loss: 11.2170 Explore P: 0.3518\n",
      "Episode: 2569 Total reward: -34.70770263671875 Training loss: 0.2793 Explore P: 0.3517\n",
      "Episode: 2570 Total reward: 34.75126647949219 Training loss: 0.4520 Explore P: 0.3515\n",
      "Model Saved\n",
      "Episode: 2571 Total reward: -56.63362121582031 Training loss: 0.3056 Explore P: 0.3515\n",
      "Episode: 2572 Total reward: 2.1440582275390625 Training loss: 0.5204 Explore P: 0.3513\n",
      "Episode: 2573 Total reward: -80.85026550292969 Training loss: 0.1753 Explore P: 0.3513\n",
      "Episode: 2574 Total reward: -53.22950744628906 Training loss: 0.3337 Explore P: 0.3511\n",
      "Episode: 2575 Total reward: -83.10517883300781 Training loss: 0.3579 Explore P: 0.3511\n",
      "Model Saved\n",
      "Episode: 2576 Total reward: -17.244033813476562 Training loss: 0.2145 Explore P: 0.3509\n",
      "Episode: 2577 Total reward: 165.3691864013672 Training loss: 0.2745 Explore P: 0.3508\n",
      "Episode: 2578 Total reward: -65.02845764160156 Training loss: 0.3735 Explore P: 0.3507\n",
      "Episode: 2579 Total reward: -59.75750732421875 Training loss: 0.4296 Explore P: 0.3506\n",
      "Episode: 2580 Total reward: -26.982421875 Training loss: 1.3637 Explore P: 0.3505\n",
      "Model Saved\n",
      "Episode: 2581 Total reward: 44.79051208496094 Training loss: 0.7308 Explore P: 0.3503\n",
      "Episode: 2582 Total reward: 257.1190643310547 Training loss: 0.2670 Explore P: 0.3501\n",
      "Episode: 2583 Total reward: -32.165435791015625 Training loss: 1.0612 Explore P: 0.3501\n",
      "Episode: 2584 Total reward: 13.695724487304688 Training loss: 0.3017 Explore P: 0.3499\n",
      "Episode: 2585 Total reward: 7.456787109375 Training loss: 0.2771 Explore P: 0.3498\n",
      "Model Saved\n",
      "Episode: 2586 Total reward: -14.366546630859375 Training loss: 0.1739 Explore P: 0.3497\n",
      "Episode: 2587 Total reward: -52.82096862792969 Training loss: 0.3711 Explore P: 0.3496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2588 Total reward: -79.22816467285156 Training loss: 2.0441 Explore P: 0.3495\n",
      "Episode: 2589 Total reward: -82.268310546875 Training loss: 0.4489 Explore P: 0.3495\n",
      "Episode: 2590 Total reward: -59.94526672363281 Training loss: 0.3531 Explore P: 0.3494\n",
      "Model Saved\n",
      "Episode: 2591 Total reward: -7.368743896484375 Training loss: 0.2666 Explore P: 0.3493\n",
      "Episode: 2592 Total reward: -7.0311279296875 Training loss: 0.5991 Explore P: 0.3491\n",
      "Episode: 2593 Total reward: -33.73478698730469 Training loss: 11.0655 Explore P: 0.3490\n",
      "Episode: 2594 Total reward: 46.13853454589844 Training loss: 0.9330 Explore P: 0.3489\n",
      "Episode: 2595 Total reward: 36.60444641113281 Training loss: 0.2919 Explore P: 0.3487\n",
      "Model Saved\n",
      "Episode: 2596 Total reward: -22.8087158203125 Training loss: 0.7663 Explore P: 0.3486\n",
      "Episode: 2597 Total reward: -29.433746337890625 Training loss: 18.6878 Explore P: 0.3485\n",
      "Episode: 2598 Total reward: -38.75300598144531 Training loss: 0.7402 Explore P: 0.3484\n",
      "Episode: 2599 Total reward: -59.84283447265625 Training loss: 0.1255 Explore P: 0.3483\n",
      "Episode: 2600 Total reward: 9.97216796875 Training loss: 0.5930 Explore P: 0.3482\n",
      "Model Saved\n",
      "Episode: 2601 Total reward: 10.896148681640625 Training loss: 0.1550 Explore P: 0.3480\n",
      "Episode: 2602 Total reward: 7.8070831298828125 Training loss: 0.1636 Explore P: 0.3479\n",
      "Episode: 2603 Total reward: -49.54795837402344 Training loss: 0.4375 Explore P: 0.3478\n",
      "Episode: 2604 Total reward: 7.21075439453125 Training loss: 0.1713 Explore P: 0.3477\n",
      "Episode: 2605 Total reward: -13.1934814453125 Training loss: 0.3925 Explore P: 0.3476\n",
      "Model Saved\n",
      "Episode: 2606 Total reward: -47.594940185546875 Training loss: 0.3039 Explore P: 0.3475\n",
      "Episode: 2607 Total reward: -72.22222900390625 Training loss: 0.2727 Explore P: 0.3474\n",
      "Episode: 2608 Total reward: 36.53900146484375 Training loss: 0.5866 Explore P: 0.3473\n",
      "Episode: 2609 Total reward: 53.449737548828125 Training loss: 20.0599 Explore P: 0.3471\n",
      "Episode: 2610 Total reward: -43.25067138671875 Training loss: 0.2620 Explore P: 0.3470\n",
      "Model Saved\n",
      "Episode: 2611 Total reward: -30.983154296875 Training loss: 0.7266 Explore P: 0.3468\n",
      "Episode: 2612 Total reward: 17.942489624023438 Training loss: 0.2186 Explore P: 0.3468\n",
      "Episode: 2613 Total reward: -47.0130615234375 Training loss: 0.4884 Explore P: 0.3467\n",
      "Episode: 2614 Total reward: -40.304779052734375 Training loss: 0.5089 Explore P: 0.3466\n",
      "Episode: 2615 Total reward: 134.07260131835938 Training loss: 0.3099 Explore P: 0.3464\n",
      "Model Saved\n",
      "Episode: 2616 Total reward: -13.284194946289062 Training loss: 0.6368 Explore P: 0.3463\n",
      "Episode: 2617 Total reward: 154.8291015625 Training loss: 0.3854 Explore P: 0.3462\n",
      "Episode: 2618 Total reward: 76.99537658691406 Training loss: 4.9443 Explore P: 0.3460\n",
      "Episode: 2619 Total reward: -10.941741943359375 Training loss: 0.7783 Explore P: 0.3459\n",
      "Episode: 2620 Total reward: 11.453567504882812 Training loss: 0.6076 Explore P: 0.3457\n",
      "Model Saved\n",
      "Episode: 2621 Total reward: 39.62060546875 Training loss: 0.2299 Explore P: 0.3456\n",
      "Episode: 2622 Total reward: 25.519851684570312 Training loss: 0.1951 Explore P: 0.3455\n",
      "Episode: 2623 Total reward: -59.81884765625 Training loss: 0.4803 Explore P: 0.3454\n",
      "Episode: 2624 Total reward: -81.77796936035156 Training loss: 0.1981 Explore P: 0.3453\n",
      "Episode: 2625 Total reward: 56.457550048828125 Training loss: 0.1704 Explore P: 0.3451\n",
      "Model Saved\n",
      "Episode: 2626 Total reward: -7.3042449951171875 Training loss: 0.8605 Explore P: 0.3450\n",
      "Episode: 2627 Total reward: -21.9102783203125 Training loss: 0.1693 Explore P: 0.3449\n",
      "Episode: 2628 Total reward: 12.993606567382812 Training loss: 2.9594 Explore P: 0.3448\n",
      "Episode: 2629 Total reward: -27.036407470703125 Training loss: 1.8195 Explore P: 0.3446\n",
      "Episode: 2630 Total reward: -85.61833190917969 Training loss: 0.2821 Explore P: 0.3445\n",
      "Model Saved\n",
      "Episode: 2631 Total reward: 86.05787658691406 Training loss: 0.2773 Explore P: 0.3443\n",
      "Episode: 2632 Total reward: -9.988922119140625 Training loss: 0.4277 Explore P: 0.3441\n",
      "Episode: 2633 Total reward: 47.76121520996094 Training loss: 0.6919 Explore P: 0.3440\n",
      "Episode: 2634 Total reward: 41.11296081542969 Training loss: 0.3313 Explore P: 0.3439\n",
      "Episode: 2635 Total reward: 19.876861572265625 Training loss: 0.5236 Explore P: 0.3437\n",
      "Model Saved\n",
      "Episode: 2636 Total reward: 17.609283447265625 Training loss: 0.8302 Explore P: 0.3436\n",
      "Episode: 2637 Total reward: 30.279205322265625 Training loss: 0.2087 Explore P: 0.3434\n",
      "Episode: 2638 Total reward: 1.2341766357421875 Training loss: 0.5593 Explore P: 0.3433\n",
      "Episode: 2639 Total reward: 62.31443786621094 Training loss: 0.8512 Explore P: 0.3431\n",
      "Episode: 2640 Total reward: -30.134368896484375 Training loss: 0.3801 Explore P: 0.3431\n",
      "Model Saved\n",
      "Episode: 2641 Total reward: 24.106231689453125 Training loss: 0.2945 Explore P: 0.3429\n",
      "Episode: 2642 Total reward: 154.24269104003906 Training loss: 0.3735 Explore P: 0.3427\n",
      "Episode: 2643 Total reward: 17.474624633789062 Training loss: 0.4478 Explore P: 0.3426\n",
      "Episode: 2644 Total reward: 0.0745086669921875 Training loss: 0.2211 Explore P: 0.3425\n",
      "Episode: 2645 Total reward: 59.07231140136719 Training loss: 7.6079 Explore P: 0.3423\n",
      "Model Saved\n",
      "Episode: 2646 Total reward: 10.159652709960938 Training loss: 0.2295 Explore P: 0.3422\n",
      "Episode: 2647 Total reward: 29.305938720703125 Training loss: 0.5047 Explore P: 0.3421\n",
      "Episode: 2648 Total reward: 70.08311462402344 Training loss: 0.6524 Explore P: 0.3419\n",
      "Episode: 2649 Total reward: -47.33180236816406 Training loss: 0.5125 Explore P: 0.3419\n",
      "Episode: 2650 Total reward: -5.008758544921875 Training loss: 0.7569 Explore P: 0.3417\n",
      "Model Saved\n",
      "Episode: 2651 Total reward: -23.242584228515625 Training loss: 0.3947 Explore P: 0.3416\n",
      "Episode: 2652 Total reward: -40.71284484863281 Training loss: 0.2577 Explore P: 0.3415\n",
      "Episode: 2653 Total reward: -10.976577758789062 Training loss: 0.3599 Explore P: 0.3415\n",
      "Episode: 2654 Total reward: -0.6396942138671875 Training loss: 0.3184 Explore P: 0.3414\n",
      "Episode: 2655 Total reward: -24.757537841796875 Training loss: 0.4283 Explore P: 0.3412\n",
      "Model Saved\n",
      "Episode: 2656 Total reward: 16.386093139648438 Training loss: 0.2426 Explore P: 0.3411\n",
      "Episode: 2657 Total reward: 33.940521240234375 Training loss: 1.1760 Explore P: 0.3410\n",
      "Episode: 2658 Total reward: 21.455963134765625 Training loss: 1.2268 Explore P: 0.3408\n",
      "Episode: 2659 Total reward: 2.0673828125 Training loss: 0.7918 Explore P: 0.3408\n",
      "Episode: 2660 Total reward: -23.274444580078125 Training loss: 0.6217 Explore P: 0.3407\n",
      "Model Saved\n",
      "Episode: 2661 Total reward: 21.148239135742188 Training loss: 0.3954 Explore P: 0.3405\n",
      "Episode: 2662 Total reward: -8.091323852539062 Training loss: 0.6717 Explore P: 0.3404\n",
      "Episode: 2663 Total reward: 13.820297241210938 Training loss: 0.3940 Explore P: 0.3403\n",
      "Episode: 2664 Total reward: 100.15727233886719 Training loss: 0.4406 Explore P: 0.3402\n",
      "Episode: 2665 Total reward: -9.72161865234375 Training loss: 11.7347 Explore P: 0.3400\n",
      "Model Saved\n",
      "Episode: 2666 Total reward: -25.847900390625 Training loss: 0.3274 Explore P: 0.3399\n",
      "Episode: 2667 Total reward: 5.95648193359375 Training loss: 1.0072 Explore P: 0.3398\n",
      "Episode: 2668 Total reward: -14.539581298828125 Training loss: 1.9447 Explore P: 0.3397\n",
      "Episode: 2669 Total reward: 75.14849853515625 Training loss: 0.4830 Explore P: 0.3396\n",
      "Episode: 2670 Total reward: -36.2393798828125 Training loss: 0.4461 Explore P: 0.3394\n",
      "Model Saved\n",
      "Episode: 2671 Total reward: 57.73797607421875 Training loss: 0.3365 Explore P: 0.3393\n",
      "Episode: 2672 Total reward: -53.05133056640625 Training loss: 0.2587 Explore P: 0.3392\n",
      "Episode: 2673 Total reward: 13.998977661132812 Training loss: 0.3110 Explore P: 0.3390\n",
      "Episode: 2674 Total reward: 36.01789855957031 Training loss: 0.3019 Explore P: 0.3389\n",
      "Episode: 2675 Total reward: -47.60890197753906 Training loss: 0.2729 Explore P: 0.3388\n",
      "Model Saved\n",
      "Episode: 2676 Total reward: -14.71759033203125 Training loss: 1.5571 Explore P: 0.3387\n",
      "Episode: 2677 Total reward: 55.9736328125 Training loss: 0.5976 Explore P: 0.3385\n",
      "Episode: 2678 Total reward: -52.077178955078125 Training loss: 0.3821 Explore P: 0.3385\n",
      "Episode: 2679 Total reward: 3.210601806640625 Training loss: 2.1677 Explore P: 0.3384\n",
      "Episode: 2680 Total reward: -41.265411376953125 Training loss: 0.3567 Explore P: 0.3383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Episode: 2681 Total reward: -16.786361694335938 Training loss: 0.2978 Explore P: 0.3382\n",
      "Episode: 2682 Total reward: -59.24674987792969 Training loss: 0.2092 Explore P: 0.3381\n",
      "Episode: 2683 Total reward: 11.713226318359375 Training loss: 0.2923 Explore P: 0.3380\n",
      "Episode: 2684 Total reward: 48.07037353515625 Training loss: 1.0223 Explore P: 0.3378\n",
      "Episode: 2685 Total reward: 33.00053405761719 Training loss: 0.1904 Explore P: 0.3377\n",
      "Model Saved\n",
      "Episode: 2686 Total reward: 29.101791381835938 Training loss: 0.2021 Explore P: 0.3376\n",
      "Episode: 2687 Total reward: 195.07901000976562 Training loss: 0.6414 Explore P: 0.3373\n",
      "Episode: 2688 Total reward: 3.1396331787109375 Training loss: 0.2422 Explore P: 0.3372\n",
      "Episode: 2689 Total reward: 37.428863525390625 Training loss: 1.9466 Explore P: 0.3370\n",
      "Episode: 2690 Total reward: -75.57559204101562 Training loss: 15.5575 Explore P: 0.3369\n",
      "Model Saved\n",
      "Episode: 2691 Total reward: -8.518218994140625 Training loss: 0.3398 Explore P: 0.3368\n",
      "Episode: 2692 Total reward: -3.2067413330078125 Training loss: 0.3695 Explore P: 0.3367\n",
      "Episode: 2693 Total reward: 6.1396636962890625 Training loss: 0.7566 Explore P: 0.3366\n",
      "Episode: 2694 Total reward: 26.60882568359375 Training loss: 0.4917 Explore P: 0.3364\n",
      "Episode: 2695 Total reward: 51.750579833984375 Training loss: 0.3467 Explore P: 0.3363\n",
      "Model Saved\n",
      "Episode: 2696 Total reward: -77.514404296875 Training loss: 0.3183 Explore P: 0.3362\n",
      "Episode: 2697 Total reward: 16.776214599609375 Training loss: 0.1696 Explore P: 0.3360\n",
      "Episode: 2698 Total reward: 15.476226806640625 Training loss: 0.6752 Explore P: 0.3359\n",
      "Episode: 2699 Total reward: 30.871063232421875 Training loss: 2.8969 Explore P: 0.3358\n",
      "Episode: 2700 Total reward: -9.74163818359375 Training loss: 2.4814 Explore P: 0.3356\n",
      "Model Saved\n",
      "Episode: 2701 Total reward: 91.34454345703125 Training loss: 1.2254 Explore P: 0.3355\n",
      "Episode: 2702 Total reward: 43.55415344238281 Training loss: 16.5574 Explore P: 0.3353\n",
      "Episode: 2703 Total reward: 15.80206298828125 Training loss: 15.5098 Explore P: 0.3352\n",
      "Episode: 2704 Total reward: -8.014205932617188 Training loss: 0.1448 Explore P: 0.3351\n",
      "Episode: 2705 Total reward: -12.419342041015625 Training loss: 0.1971 Explore P: 0.3349\n",
      "Model Saved\n",
      "Episode: 2706 Total reward: 43.074676513671875 Training loss: 3.2264 Explore P: 0.3348\n",
      "Episode: 2707 Total reward: 39.610565185546875 Training loss: 0.2478 Explore P: 0.3347\n",
      "Episode: 2708 Total reward: 13.3167724609375 Training loss: 0.2196 Explore P: 0.3345\n",
      "Episode: 2709 Total reward: -38.43817138671875 Training loss: 0.3414 Explore P: 0.3345\n",
      "Episode: 2710 Total reward: -54.704345703125 Training loss: 1.3601 Explore P: 0.3343\n",
      "Model Saved\n",
      "Episode: 2711 Total reward: 5.1353607177734375 Training loss: 0.3902 Explore P: 0.3343\n",
      "Episode: 2712 Total reward: 17.76214599609375 Training loss: 0.2414 Explore P: 0.3341\n",
      "Episode: 2713 Total reward: 45.079803466796875 Training loss: 10.0576 Explore P: 0.3340\n",
      "Episode: 2714 Total reward: 42.5494384765625 Training loss: 0.7830 Explore P: 0.3339\n",
      "Episode: 2715 Total reward: -38.48805236816406 Training loss: 1.3047 Explore P: 0.3338\n",
      "Model Saved\n",
      "Episode: 2716 Total reward: -93.75019836425781 Training loss: 0.4746 Explore P: 0.3337\n",
      "Episode: 2717 Total reward: 21.6983642578125 Training loss: 1.4307 Explore P: 0.3336\n",
      "Episode: 2718 Total reward: -20.582199096679688 Training loss: 0.1890 Explore P: 0.3334\n",
      "Episode: 2719 Total reward: 92.78929138183594 Training loss: 1.4621 Explore P: 0.3333\n",
      "Episode: 2720 Total reward: 26.2908935546875 Training loss: 2.8790 Explore P: 0.3332\n",
      "Model Saved\n",
      "Episode: 2721 Total reward: 27.92608642578125 Training loss: 0.1940 Explore P: 0.3330\n",
      "Episode: 2722 Total reward: 4.79901123046875 Training loss: 1.3664 Explore P: 0.3329\n",
      "Episode: 2723 Total reward: 55.40809631347656 Training loss: 0.1651 Explore P: 0.3328\n",
      "Episode: 2724 Total reward: -46.58256530761719 Training loss: 0.3454 Explore P: 0.3326\n",
      "Episode: 2725 Total reward: -41.90065002441406 Training loss: 0.2823 Explore P: 0.3325\n",
      "Model Saved\n",
      "Episode: 2726 Total reward: 45.09130859375 Training loss: 1.7652 Explore P: 0.3324\n",
      "Episode: 2727 Total reward: 47.990631103515625 Training loss: 0.4151 Explore P: 0.3323\n",
      "Episode: 2728 Total reward: -51.481658935546875 Training loss: 0.4207 Explore P: 0.3322\n",
      "Episode: 2729 Total reward: 0.9011993408203125 Training loss: 0.7721 Explore P: 0.3320\n",
      "Episode: 2730 Total reward: 24.451644897460938 Training loss: 0.3604 Explore P: 0.3318\n",
      "Model Saved\n",
      "Episode: 2731 Total reward: -33.2723388671875 Training loss: 0.3038 Explore P: 0.3317\n",
      "Episode: 2732 Total reward: 8.878570556640625 Training loss: 0.2565 Explore P: 0.3316\n",
      "Episode: 2733 Total reward: -23.020980834960938 Training loss: 1.4919 Explore P: 0.3315\n",
      "Episode: 2734 Total reward: 37.196624755859375 Training loss: 2.6347 Explore P: 0.3313\n",
      "Episode: 2735 Total reward: -26.758438110351562 Training loss: 0.6869 Explore P: 0.3312\n",
      "Model Saved\n",
      "Episode: 2736 Total reward: -24.90594482421875 Training loss: 0.1882 Explore P: 0.3311\n",
      "Episode: 2737 Total reward: -10.731460571289062 Training loss: 2.5373 Explore P: 0.3310\n",
      "Episode: 2738 Total reward: 13.769088745117188 Training loss: 1.7578 Explore P: 0.3308\n",
      "Episode: 2739 Total reward: 25.477218627929688 Training loss: 1.6511 Explore P: 0.3308\n",
      "Episode: 2740 Total reward: -42.31269836425781 Training loss: 0.5937 Explore P: 0.3307\n",
      "Model Saved\n",
      "Episode: 2741 Total reward: 28.16357421875 Training loss: 0.2064 Explore P: 0.3305\n",
      "Episode: 2742 Total reward: 47.923797607421875 Training loss: 0.2255 Explore P: 0.3304\n",
      "Episode: 2743 Total reward: 42.92765808105469 Training loss: 4.0001 Explore P: 0.3303\n",
      "Episode: 2744 Total reward: 53.53926086425781 Training loss: 0.4245 Explore P: 0.3302\n",
      "Episode: 2745 Total reward: 21.075088500976562 Training loss: 0.2805 Explore P: 0.3300\n",
      "Model Saved\n",
      "Episode: 2746 Total reward: -55.38099670410156 Training loss: 0.1930 Explore P: 0.3299\n",
      "Episode: 2747 Total reward: 35.46360778808594 Training loss: 2.0676 Explore P: 0.3298\n",
      "Episode: 2748 Total reward: -40.88771057128906 Training loss: 2.4107 Explore P: 0.3297\n",
      "Episode: 2749 Total reward: -59.08857727050781 Training loss: 0.2853 Explore P: 0.3296\n",
      "Episode: 2750 Total reward: 28.361953735351562 Training loss: 0.2534 Explore P: 0.3295\n",
      "Model Saved\n",
      "Episode: 2751 Total reward: -47.67146301269531 Training loss: 0.5065 Explore P: 0.3293\n",
      "Episode: 2752 Total reward: 13.916290283203125 Training loss: 0.1215 Explore P: 0.3292\n",
      "Episode: 2753 Total reward: 13.436294555664062 Training loss: 2.0440 Explore P: 0.3291\n",
      "Episode: 2754 Total reward: -18.9039306640625 Training loss: 11.7420 Explore P: 0.3290\n",
      "Episode: 2755 Total reward: 45.93292236328125 Training loss: 0.1709 Explore P: 0.3288\n",
      "Model Saved\n",
      "Episode: 2756 Total reward: 9.227218627929688 Training loss: 0.4238 Explore P: 0.3287\n",
      "Episode: 2757 Total reward: 33.97175598144531 Training loss: 16.3510 Explore P: 0.3286\n",
      "Episode: 2758 Total reward: -26.532562255859375 Training loss: 3.7696 Explore P: 0.3284\n",
      "Episode: 2759 Total reward: 11.368423461914062 Training loss: 0.2969 Explore P: 0.3283\n",
      "Episode: 2760 Total reward: -52.226348876953125 Training loss: 0.3884 Explore P: 0.3282\n",
      "Model Saved\n",
      "Episode: 2761 Total reward: -6.772735595703125 Training loss: 0.2191 Explore P: 0.3281\n",
      "Episode: 2762 Total reward: 27.042999267578125 Training loss: 3.6101 Explore P: 0.3279\n",
      "Episode: 2763 Total reward: -9.92620849609375 Training loss: 0.2713 Explore P: 0.3278\n",
      "Episode: 2764 Total reward: 48.09370422363281 Training loss: 0.2030 Explore P: 0.3277\n",
      "Episode: 2765 Total reward: -8.587600708007812 Training loss: 1.4116 Explore P: 0.3276\n",
      "Model Saved\n",
      "Episode: 2766 Total reward: 33.72955322265625 Training loss: 9.8873 Explore P: 0.3275\n",
      "Episode: 2767 Total reward: -11.674606323242188 Training loss: 1.3650 Explore P: 0.3273\n",
      "Episode: 2768 Total reward: -31.838180541992188 Training loss: 0.2058 Explore P: 0.3271\n",
      "Episode: 2769 Total reward: -72.73295593261719 Training loss: 0.6551 Explore P: 0.3270\n",
      "Episode: 2770 Total reward: -24.804672241210938 Training loss: 2.3366 Explore P: 0.3269\n",
      "Model Saved\n",
      "Episode: 2771 Total reward: 18.63690185546875 Training loss: 0.1891 Explore P: 0.3268\n",
      "Episode: 2772 Total reward: 27.6761474609375 Training loss: 0.7899 Explore P: 0.3267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2773 Total reward: -66.00131225585938 Training loss: 0.6607 Explore P: 0.3266\n",
      "Episode: 2774 Total reward: -1.1996917724609375 Training loss: 0.8246 Explore P: 0.3265\n",
      "Episode: 2775 Total reward: -11.212356567382812 Training loss: 0.1941 Explore P: 0.3263\n",
      "Model Saved\n",
      "Episode: 2776 Total reward: 25.559494018554688 Training loss: 0.5767 Explore P: 0.3262\n",
      "Episode: 2777 Total reward: 32.73443603515625 Training loss: 0.4228 Explore P: 0.3261\n",
      "Episode: 2778 Total reward: 47.29740905761719 Training loss: 0.5938 Explore P: 0.3259\n",
      "Episode: 2779 Total reward: 36.61305236816406 Training loss: 1.3928 Explore P: 0.3258\n",
      "Episode: 2780 Total reward: 27.707595825195312 Training loss: 11.7284 Explore P: 0.3257\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer.add_graph(sess.graph)\n",
    "        tau = 0\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "        \n",
    "                \n",
    "        # Update the parameters of our TargetNetwork with DQN_weights\n",
    "        update_target = update_target_network()\n",
    "        sess.run(update_target)\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            \n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                tau += 1\n",
    "                \n",
    "                # Increase decay_step\n",
    "                decay_step +=1\n",
    "                \n",
    "                # Predict the action to take and take it\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "                # Do the action\n",
    "                reward = game.make_action(action)\n",
    "\n",
    "                # Look if the episode is finished\n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                # Add the reward to total reward\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((84,84), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "\n",
    "                else:\n",
    "                    # Get the next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "                    \n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "                ### LEARNING PART            \n",
    "                # Obtain random mini-batch from memory\n",
    "                tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "                \n",
    "                states_mb = np.array([each[0][0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[0][1] for each in batch])\n",
    "                rewards_mb = np.array([each[0][2] for each in batch]) \n",
    "                next_states_mb = np.array([each[0][3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[0][4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                 # Get Q values for next_state \n",
    "                Qs_next_state = sess.run(DeepQNetwork.output, feed_dict = {DeepQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                #calculate q target in the target network\n",
    "                q_target = sess.run(TargetQNetwork.output, feed_dict={ TargetQNetwork.inputs_: next_states_mb})\n",
    "\n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    #a' for s'\n",
    "                    action = np.argmax(Qs_next_state[i])\n",
    "                    \n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * q_target[i][action]\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                _,loss,absolute_errors = sess.run([DeepQNetwork.optimizer, DeepQNetwork.loss, DeepQNetwork.absolute_errors],\n",
    "                                    feed_dict={DeepQNetwork.inputs_: states_mb,\n",
    "                                               DeepQNetwork.target_Q: targets_mb,\n",
    "                                               DeepQNetwork.actions_: actions_mb,\n",
    "                                              DeepQNetwork.ISWeights_ : ISWeights_mb})\n",
    "                # Update priority\n",
    "                memory.batch_update(tree_idx, absolute_errors)\n",
    "\n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DeepQNetwork.inputs_: states_mb,\n",
    "                                                   DeepQNetwork.target_Q: targets_mb,\n",
    "                                                   DeepQNetwork.actions_: actions_mb,\n",
    "                                                       DeepQNetwork.ISWeights_ : ISWeights_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                                \n",
    "                #update the target weights by running the operations \n",
    "                if tau >= max_tau:\n",
    "                    update_target_ops = update_target_network()\n",
    "                    sess.run(update_target_ops)\n",
    "                    tau = 0\n",
    "                \n",
    "\n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/ddqn/model.ckpt\")\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Watch our Agent play üëÄ\n",
    "Now that we trained our agent, we can test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration (TESTING)\n",
    "    game.load_config(\"deadly_corridor_testing.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case deadly_corridor scenario)\n",
    "    game.set_doom_scenario_path(\"deadly_corridor.wad\")\n",
    "    game.init()\n",
    "    \n",
    "    totalScore = 0\n",
    "    \n",
    "   \n",
    "    # Load the model\n",
    "    saver = tf.train.import_meta_graph(\"./models/ddqn/model.ckpt.meta\")\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(\"./models/ddqn\"))\n",
    "    game.init()\n",
    "                  \n",
    "    for i in range(200):\n",
    "        \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "        while not game.is_episode_finished():\n",
    "            exp_exp_tradeoff = np.random.rand()\n",
    "            \n",
    "\n",
    "            explore_probability = 0.01\n",
    "    \n",
    "            if (explore_probability > exp_exp_tradeoff):\n",
    "                # Make a random action (exploration)\n",
    "                action = random.choice(possible_actions)\n",
    "            else:\n",
    "                # Take the biggest Q value (= the best action)\n",
    "                Qs = sess.run(DeepQNetwork.output, feed_dict = {DeepQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "                \n",
    "                choice = np.argmax(Qs)\n",
    "                action = possible_actions[int(action)]\n",
    "\n",
    "            game.make_action(action) \n",
    "            done = game.is_episode_finished()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "                \n",
    "        score = game.get_total_reward()\n",
    "        time.sleep(1)\n",
    "        print(\"TOTAL_SCORE\", score)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
