{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "- Now that we imported the libraries/dependencies, we will create our environment.\n",
    "- Doom environment takes:\n",
    "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
    "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
    "- Note: We have 3 possible actions `[[0,0,1], [1,0,0], [0,1,0]]` so we don't need to do one hot encoding (thanks to < a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out. \n",
    "\n",
    "### Our environment\n",
    "<img src=\"assets/doom.png\" style=\"max-width:500px;\" alt=\"Doom\"/>\n",
    "                                    \n",
    "- A monster is spawned **randomly somewhere along the opposite wall**. \n",
    "- Player can only go **left/right and shoot**. \n",
    "- 1 hit is enough **to kill the monster**. \n",
    "- Episode finishes when **monster is killed or on timeout (300)**.\n",
    "<br><br>\n",
    "REWARDS:\n",
    "\n",
    "- +101 for killing the monster \n",
    "- -5 for missing \n",
    "- Episode ends after killing the monster or on timeout.\n",
    "- living reward = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we create our environment\n",
    "\"\"\"\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case basic scenario)\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    \n",
    "    # Here our possible actions\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions\n",
    "       \n",
    "\"\"\"\n",
    "Here we performing random action to test the environment\n",
    "\"\"\"\n",
    "def test_environment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.init()\n",
    "    shoot = [0, 0, 1]\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    actions = [shoot, left, right]\n",
    "\n",
    "    episodes = 100\n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(actions)\n",
    "            print(action)\n",
    "            reward = game.make_action(action)\n",
    "            time.sleep(0.02)\n",
    "        print (\"Result:\", game.get_total_reward())\n",
    "        time.sleep(1)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game,possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the preprocessing functions ‚öôÔ∏è\n",
    "### preprocess_frame\n",
    "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
    "<br><br>\n",
    "Our steps:\n",
    "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
    "- Crop the screen (in our case we remove the roof because it contains no information)\n",
    "- We normalize pixel values\n",
    "- Finally we resize the preprocessed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess_frame:\n",
    "    Take a frame.\n",
    "    Resize it.\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "        \n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "    Normalize it.\n",
    "    \n",
    "    return preprocessed_frame\n",
    "    \n",
    "    \"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Greyscale frame already done in our vizdoom config\n",
    "    # x = np.mean(frame,-1)\n",
    "    \n",
    "    # Crop the screen (remove the roof because it contains no information)\n",
    "    cropped_frame = frame[30:-10,30:-30]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84,84])\n",
    "    \n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack_frames\n",
    "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
    "\n",
    "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\n",
    "\n",
    "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
    "\n",
    "- First we preprocess frame\n",
    "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
    "- Finally we **build the stacked state**\n",
    "\n",
    "This is how work stack:\n",
    "- For the first frame, we feed 4 frames\n",
    "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
    "- And so on\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\n",
    "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
    "\n",
    "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
    "- Then, you'll add the training hyperparameters when you implement the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [84,84,4]      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size()              # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.0002      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 500        # Total episodes for training\n",
    "max_steps = 100              # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "#targetq hyperparameters\n",
    "max_tau = 5\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000          # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Deep Q-learning Neural Network model üß†\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/doom/assets/model.png\" alt=\"Model\" />\n",
    "This is our Deep Q-learning model:\n",
    "- We take a stack of 4 frames as input\n",
    "- It passes through 3 convnets\n",
    "- Then it is flatened\n",
    "- Finally it passes through 2 FC layers\n",
    "- It outputs a Q value for each actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name\n",
    "        \n",
    "        with tf.variable_scope(self.name):\n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "            # [None, 84, 84, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name=\"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\"\n",
    "            First convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            # Input is 84x84x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm1')\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "            ## --> [20, 20, 32]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Second convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "        \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm2')\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "            ## --> [9, 9, 64]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Third convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "        \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm3')\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "            ## --> [3, 3, 128]\n",
    "            \n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            ## --> [1152]\n",
    "            \n",
    "            ##This branch of the net calculates the value of the state, how good is to be at that state\n",
    "            ##if you are about to die no matter what action you choose the state has low value\n",
    "            ##--input: flatten layer, 1152\n",
    "            ##--output: 512 neurons\n",
    "            self.value_fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                            units = 512,\n",
    "                                            activation = tf.nn.elu,\n",
    "                                            kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                            name = \"value_fc\")\n",
    "            \n",
    "            ##--input: flatten layer, 512\n",
    "            ##--output: 1 neuron with the V value\n",
    "            self.value = tf.layers.dense(inputs = self.value_fc,\n",
    "                                        units = 1,\n",
    "                                        activation = None,\n",
    "                                        kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                        name = \"value\")\n",
    "            \n",
    "            #calculates the advantage of an action over the rest for that state\n",
    "            #input: flatten layer, 1152\n",
    "            #output: 512 neurons\n",
    "            self.advantage_fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                              units = 512,\n",
    "                                              activation = tf.nn.elu,\n",
    "                                              kernel_initializer= tf.contrib.layers.xavier_initializer(),\n",
    "                                              name = \"advantage_fc\")\n",
    "            \n",
    "            self.advantage = tf.layers.dense(inputs = self.advantage_fc,\n",
    "                                           units = action_size,\n",
    "                                           activation = None,\n",
    "                                           kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                            name = \"advantage\")\n",
    "            \n",
    "        \n",
    "            #aggregation layer\n",
    "            self.output = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage,axis = 1, keepdims = True))\n",
    "\n",
    "  \n",
    "            # Q is our predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            \n",
    "            # The loss is the difference between our predicted Q_values and the Q_target\n",
    "            # Sum(Qtarget - Q)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DeepQNetwork = DQNetwork(state_size, action_size, learning_rate,\"DeepQNetwork\")\n",
    "\n",
    "#create the QTargetNetwork\n",
    "TargetQNetwork = DQNetwork(state_size,action_size, learning_rate, \"TargetQNetwork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_network():\n",
    "    #every tau steps we copy the weights from our deep netwokr to our TargetNetwok\n",
    "    \n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"DQNetwork\")\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"TargetQNetwork\")\n",
    "    \n",
    "    op_holder = []\n",
    "    \n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Experience Replay üîÅ\n",
    "Now that we create our Neural Network, **we need to implement the Experience Replay method.** <br><br>\n",
    "Here we'll create the Memory object that creates a deque.A deque (double ended queue) is a data type that **removes the oldest element each time that you add a new element.**\n",
    "\n",
    "This part was taken from Udacity : <a href=\"https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb\" Cartpole DQN</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll **deal with the empty memory problem**: we pre-populate our memory by taking random actions and storing the experience (state, action, reward, new_state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "\n",
    "game.init()\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state\n",
    "\n",
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/dqn/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DeepQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train our Agent üèÉ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Our algorithm:\n",
    "<br>\n",
    "* Initialize the weights\n",
    "* Init the environment\n",
    "* Initialize the decay rate (that will use to reduce epsilon) \n",
    "<br><br>\n",
    "* **For** episode to max_episode **do** \n",
    "    * Make new episode\n",
    "    * Set step to 0\n",
    "    * Observe the first state $s_0$\n",
    "    <br><br>\n",
    "    * **While** step < max_steps **do**:\n",
    "        * Increase decay_rate\n",
    "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
    "        * Set $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma \\max_{a'}{Q(s', a')}$\n",
    "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "    * **endfor**\n",
    "    <br><br>\n",
    "* **endfor**\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With œµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DeepQNetwork.output, feed_dict = {DeepQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 95.0 Training loss: 262.2542 Explore P: 0.9994\n",
      "Model Saved\n",
      "Episode: 2 Total reward: 92.0 Training loss: 5.7736 Explore P: 0.9887\n",
      "Episode: 3 Total reward: 95.0 Training loss: 111.3848 Explore P: 0.9881\n",
      "Episode: 4 Total reward: 94.0 Training loss: 17.8130 Explore P: 0.9874\n",
      "Model Saved\n",
      "Episode: 8 Total reward: 94.0 Training loss: 2.2912 Explore P: 0.9579\n",
      "Episode: 10 Total reward: 35.0 Training loss: 0.9588 Explore P: 0.9432\n",
      "Model Saved\n",
      "Episode: 12 Total reward: 79.0 Training loss: 1.6652 Explore P: 0.9319\n",
      "Episode: 13 Total reward: 92.0 Training loss: 110.3438 Explore P: 0.9310\n",
      "Model Saved\n",
      "Episode: 16 Total reward: 94.0 Training loss: 79.0564 Explore P: 0.9122\n",
      "Episode: 19 Total reward: 93.0 Training loss: 8.6875 Explore P: 0.8936\n",
      "Episode: 20 Total reward: -24.0 Training loss: 7.6281 Explore P: 0.8848\n",
      "Model Saved\n",
      "Episode: 22 Total reward: 93.0 Training loss: 113.1976 Explore P: 0.8754\n",
      "Episode: 23 Total reward: 28.0 Training loss: 4.7635 Explore P: 0.8704\n",
      "Episode: 25 Total reward: 95.0 Training loss: 0.7085 Explore P: 0.8613\n",
      "Model Saved\n",
      "Episode: 26 Total reward: 95.0 Training loss: 8.7047 Explore P: 0.8608\n",
      "Episode: 29 Total reward: 95.0 Training loss: 1.0948 Explore P: 0.8435\n",
      "Episode: 30 Total reward: 95.0 Training loss: 1.0268 Explore P: 0.8430\n",
      "Model Saved\n",
      "Episode: 32 Total reward: 18.0 Training loss: 0.4588 Explore P: 0.8291\n",
      "Episode: 33 Total reward: 68.0 Training loss: 0.8075 Explore P: 0.8268\n",
      "Episode: 34 Total reward: 95.0 Training loss: 2.1821 Explore P: 0.8263\n",
      "Episode: 35 Total reward: 95.0 Training loss: 0.4640 Explore P: 0.8258\n",
      "Model Saved\n",
      "Episode: 36 Total reward: -7.0 Training loss: 0.7531 Explore P: 0.8187\n",
      "Episode: 38 Total reward: 95.0 Training loss: 3.8631 Explore P: 0.8102\n",
      "Model Saved\n",
      "Episode: 41 Total reward: 25.0 Training loss: 1.1094 Explore P: 0.7895\n",
      "Episode: 42 Total reward: 95.0 Training loss: 1.1286 Explore P: 0.7891\n",
      "Episode: 43 Total reward: 94.0 Training loss: 10.9611 Explore P: 0.7885\n",
      "Episode: 44 Total reward: 95.0 Training loss: 1.0963 Explore P: 0.7881\n",
      "Model Saved\n",
      "Episode: 46 Total reward: 95.0 Training loss: 6.5269 Explore P: 0.7799\n",
      "Episode: 47 Total reward: 95.0 Training loss: 6.1726 Explore P: 0.7794\n",
      "Episode: 49 Total reward: 95.0 Training loss: 7.6462 Explore P: 0.7713\n",
      "Model Saved\n",
      "Episode: 51 Total reward: -14.0 Training loss: 1.7796 Explore P: 0.7566\n",
      "Episode: 53 Total reward: 95.0 Training loss: 3.9760 Explore P: 0.7487\n",
      "Model Saved\n",
      "Episode: 56 Total reward: 75.0 Training loss: 0.9013 Explore P: 0.7326\n",
      "Episode: 58 Total reward: 95.0 Training loss: 1.0620 Explore P: 0.7249\n",
      "Episode: 59 Total reward: 89.0 Training loss: 8.3038 Explore P: 0.7241\n",
      "Episode: 60 Total reward: 67.0 Training loss: 1.4702 Explore P: 0.7220\n",
      "Model Saved\n",
      "Episode: 61 Total reward: 33.0 Training loss: 1.7424 Explore P: 0.7179\n",
      "Episode: 62 Total reward: 95.0 Training loss: 0.5156 Explore P: 0.7175\n",
      "Episode: 63 Total reward: 95.0 Training loss: 94.0226 Explore P: 0.7171\n",
      "Episode: 64 Total reward: 95.0 Training loss: 29.5930 Explore P: 0.7166\n",
      "Episode: 65 Total reward: 95.0 Training loss: 0.9871 Explore P: 0.7162\n",
      "Model Saved\n",
      "Episode: 66 Total reward: 95.0 Training loss: 10.2818 Explore P: 0.7158\n",
      "Episode: 68 Total reward: 48.0 Training loss: 0.9229 Explore P: 0.7058\n",
      "Episode: 69 Total reward: 20.0 Training loss: 0.9703 Explore P: 0.7012\n",
      "Episode: 70 Total reward: 47.0 Training loss: 44.3582 Explore P: 0.6981\n",
      "Model Saved\n",
      "Episode: 71 Total reward: -14.0 Training loss: 0.8631 Explore P: 0.6920\n",
      "Episode: 72 Total reward: 91.0 Training loss: 3.0696 Explore P: 0.6913\n",
      "Model Saved\n",
      "Episode: 76 Total reward: 94.0 Training loss: 1.2600 Explore P: 0.6707\n",
      "Episode: 77 Total reward: 94.0 Training loss: 18.2180 Explore P: 0.6702\n",
      "Episode: 78 Total reward: 86.0 Training loss: 3.9883 Explore P: 0.6693\n",
      "Episode: 79 Total reward: 18.0 Training loss: 3.6301 Explore P: 0.6648\n",
      "Episode: 80 Total reward: 87.0 Training loss: 2.2024 Explore P: 0.6639\n",
      "Model Saved\n",
      "Episode: 81 Total reward: 23.0 Training loss: 8.0951 Explore P: 0.6598\n",
      "Episode: 83 Total reward: 95.0 Training loss: 2.3271 Explore P: 0.6529\n",
      "Episode: 84 Total reward: 93.0 Training loss: 58.6538 Explore P: 0.6524\n",
      "Model Saved\n",
      "Episode: 87 Total reward: 24.0 Training loss: 1.4019 Explore P: 0.6358\n",
      "Episode: 89 Total reward: 95.0 Training loss: 0.6810 Explore P: 0.6292\n",
      "Episode: 90 Total reward: 16.0 Training loss: 0.6666 Explore P: 0.6249\n",
      "Model Saved\n",
      "Episode: 91 Total reward: 93.0 Training loss: 13.0160 Explore P: 0.6244\n",
      "Episode: 93 Total reward: 95.0 Training loss: 2.4486 Explore P: 0.6179\n",
      "Episode: 94 Total reward: 14.0 Training loss: 3.0164 Explore P: 0.6135\n",
      "Episode: 95 Total reward: 93.0 Training loss: 2.9475 Explore P: 0.6131\n",
      "Model Saved\n",
      "Episode: 96 Total reward: 95.0 Training loss: 73.9604 Explore P: 0.6127\n",
      "Episode: 99 Total reward: 92.0 Training loss: 0.8735 Explore P: 0.6002\n",
      "Model Saved\n",
      "Episode: 101 Total reward: 68.0 Training loss: 22.4870 Explore P: 0.5927\n",
      "Episode: 105 Total reward: 93.0 Training loss: 0.7309 Explore P: 0.5750\n",
      "Model Saved\n",
      "Episode: 109 Total reward: 23.0 Training loss: 0.7254 Explore P: 0.5549\n",
      "Model Saved\n",
      "Episode: 111 Total reward: 93.0 Training loss: 1.7396 Explore P: 0.5490\n",
      "Model Saved\n",
      "Episode: 117 Total reward: 95.0 Training loss: 4.5264 Explore P: 0.5225\n",
      "Episode: 118 Total reward: 94.0 Training loss: 0.9234 Explore P: 0.5221\n",
      "Model Saved\n",
      "Episode: 121 Total reward: 95.0 Training loss: 0.7513 Explore P: 0.5117\n",
      "Episode: 123 Total reward: 93.0 Training loss: 0.6112 Explore P: 0.5063\n",
      "Episode: 124 Total reward: 68.0 Training loss: 3.3733 Explore P: 0.5049\n",
      "Episode: 125 Total reward: 90.0 Training loss: 0.7238 Explore P: 0.5043\n",
      "Model Saved\n",
      "Episode: 126 Total reward: 67.0 Training loss: 0.7049 Explore P: 0.5029\n",
      "Episode: 127 Total reward: 18.0 Training loss: 3.1957 Explore P: 0.4996\n",
      "Episode: 128 Total reward: 95.0 Training loss: 0.6159 Explore P: 0.4993\n",
      "Episode: 129 Total reward: 12.0 Training loss: 2.9718 Explore P: 0.4957\n",
      "Model Saved\n",
      "Episode: 134 Total reward: 95.0 Training loss: 0.5358 Explore P: 0.4763\n",
      "Episode: 135 Total reward: 66.0 Training loss: 0.6484 Explore P: 0.4749\n",
      "Model Saved\n",
      "Episode: 137 Total reward: 95.0 Training loss: 0.6369 Explore P: 0.4700\n",
      "Episode: 138 Total reward: 93.0 Training loss: 2.5393 Explore P: 0.4697\n",
      "Episode: 139 Total reward: 92.0 Training loss: 11.0091 Explore P: 0.4693\n",
      "Episode: 140 Total reward: 64.0 Training loss: 0.6942 Explore P: 0.4678\n",
      "Model Saved\n",
      "Episode: 141 Total reward: 45.0 Training loss: 0.7009 Explore P: 0.4657\n",
      "Episode: 143 Total reward: 95.0 Training loss: 0.6894 Explore P: 0.4609\n",
      "Model Saved\n",
      "Episode: 146 Total reward: 94.0 Training loss: 4.2784 Explore P: 0.4516\n",
      "Episode: 148 Total reward: 92.0 Training loss: 2.2943 Explore P: 0.4469\n",
      "Episode: 150 Total reward: 90.0 Training loss: 16.6637 Explore P: 0.4420\n",
      "Model Saved\n",
      "Episode: 151 Total reward: 95.0 Training loss: 0.6406 Explore P: 0.4418\n",
      "Episode: 152 Total reward: 93.0 Training loss: 10.7742 Explore P: 0.4414\n",
      "Episode: 153 Total reward: 52.0 Training loss: 3.5715 Explore P: 0.4398\n",
      "Episode: 155 Total reward: 95.0 Training loss: 1.2988 Explore P: 0.4352\n",
      "Model Saved\n",
      "Episode: 156 Total reward: 95.0 Training loss: 1.0443 Explore P: 0.4350\n",
      "Model Saved\n",
      "Episode: 161 Total reward: 93.0 Training loss: 0.9611 Explore P: 0.4180\n",
      "Episode: 162 Total reward: 19.0 Training loss: 0.8834 Explore P: 0.4153\n",
      "Episode: 163 Total reward: 74.0 Training loss: 1.0668 Explore P: 0.4144\n",
      "Episode: 165 Total reward: 95.0 Training loss: 0.6518 Explore P: 0.4101\n",
      "Model Saved\n",
      "Episode: 166 Total reward: 94.0 Training loss: 1.3510 Explore P: 0.4098\n",
      "Episode: 167 Total reward: 94.0 Training loss: 1.0453 Explore P: 0.4095\n",
      "Episode: 168 Total reward: 93.0 Training loss: 2.0972 Explore P: 0.4092\n",
      "Episode: 169 Total reward: 67.0 Training loss: 3.4386 Explore P: 0.4081\n",
      "Model Saved\n",
      "Episode: 172 Total reward: 94.0 Training loss: 6.0692 Explore P: 0.3999\n",
      "Episode: 173 Total reward: 40.0 Training loss: 0.8694 Explore P: 0.3979\n",
      "Episode: 175 Total reward: 13.0 Training loss: 0.9508 Explore P: 0.3913\n",
      "Model Saved\n",
      "Episode: 176 Total reward: 95.0 Training loss: 0.6938 Explore P: 0.3910\n",
      "Episode: 177 Total reward: 93.0 Training loss: 1.4855 Explore P: 0.3907\n",
      "Episode: 178 Total reward: 95.0 Training loss: 2.0365 Explore P: 0.3905\n",
      "Model Saved\n",
      "Episode: 181 Total reward: 95.0 Training loss: 4.7959 Explore P: 0.3827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 182 Total reward: 91.0 Training loss: 0.8754 Explore P: 0.3824\n",
      "Episode: 185 Total reward: 93.0 Training loss: 10.6632 Explore P: 0.3747\n",
      "Model Saved\n",
      "Episode: 187 Total reward: 94.0 Training loss: 0.6841 Explore P: 0.3708\n",
      "Model Saved\n",
      "Episode: 192 Total reward: 94.0 Training loss: 2.8312 Explore P: 0.3564\n",
      "Episode: 193 Total reward: -4.0 Training loss: 0.7416 Explore P: 0.3535\n",
      "Episode: 195 Total reward: 95.0 Training loss: 0.8070 Explore P: 0.3499\n",
      "Model Saved\n",
      "Episode: 197 Total reward: 94.0 Training loss: 0.6960 Explore P: 0.3463\n",
      "Episode: 198 Total reward: 93.0 Training loss: 0.6578 Explore P: 0.3460\n",
      "Episode: 199 Total reward: 93.0 Training loss: 0.8906 Explore P: 0.3457\n",
      "Model Saved\n",
      "Episode: 202 Total reward: 69.0 Training loss: 0.8977 Explore P: 0.3382\n",
      "Episode: 203 Total reward: 95.0 Training loss: 0.6016 Explore P: 0.3380\n",
      "Episode: 204 Total reward: 94.0 Training loss: 1.7909 Explore P: 0.3378\n",
      "Episode: 205 Total reward: 85.0 Training loss: 0.7047 Explore P: 0.3372\n",
      "Model Saved\n",
      "Episode: 207 Total reward: 92.0 Training loss: 1.8106 Explore P: 0.3337\n",
      "Episode: 208 Total reward: 94.0 Training loss: 2.0293 Explore P: 0.3335\n",
      "Episode: 209 Total reward: 22.0 Training loss: 1.5645 Explore P: 0.3314\n",
      "Episode: 210 Total reward: 95.0 Training loss: 1.1868 Explore P: 0.3312\n",
      "Model Saved\n",
      "Episode: 212 Total reward: 92.0 Training loss: 0.9624 Explore P: 0.3277\n",
      "Episode: 214 Total reward: 90.0 Training loss: 1.9199 Explore P: 0.3242\n",
      "Episode: 215 Total reward: 17.0 Training loss: 0.9244 Explore P: 0.3221\n",
      "Model Saved\n",
      "Episode: 216 Total reward: 91.0 Training loss: 1.6256 Explore P: 0.3218\n",
      "Episode: 218 Total reward: 12.0 Training loss: 0.5040 Explore P: 0.3164\n",
      "Episode: 219 Total reward: -10.0 Training loss: 0.8967 Explore P: 0.3136\n",
      "Episode: 220 Total reward: 88.0 Training loss: 0.6758 Explore P: 0.3132\n",
      "Model Saved\n",
      "Episode: 221 Total reward: 24.0 Training loss: 1.7559 Explore P: 0.3113\n",
      "Episode: 223 Total reward: 90.0 Training loss: 0.5644 Explore P: 0.3080\n",
      "Episode: 224 Total reward: 72.0 Training loss: 5.5535 Explore P: 0.3073\n",
      "Episode: 225 Total reward: 91.0 Training loss: 0.7996 Explore P: 0.3070\n",
      "Model Saved\n",
      "Episode: 226 Total reward: 16.0 Training loss: 156.5570 Explore P: 0.3048\n",
      "Episode: 227 Total reward: 90.0 Training loss: 2.6067 Explore P: 0.3044\n",
      "Episode: 228 Total reward: 91.0 Training loss: 0.7706 Explore P: 0.3042\n",
      "Episode: 229 Total reward: 94.0 Training loss: 1.3772 Explore P: 0.3039\n",
      "Episode: 230 Total reward: -21.0 Training loss: 1.9807 Explore P: 0.3011\n",
      "Model Saved\n",
      "Episode: 232 Total reward: 33.0 Training loss: 4.0885 Explore P: 0.2965\n",
      "Episode: 233 Total reward: 92.0 Training loss: 4.1557 Explore P: 0.2963\n",
      "Model Saved\n",
      "Episode: 236 Total reward: 25.0 Training loss: 154.4908 Explore P: 0.2889\n",
      "Episode: 237 Total reward: 90.0 Training loss: 0.9912 Explore P: 0.2886\n",
      "Episode: 238 Total reward: 92.0 Training loss: 5.3842 Explore P: 0.2884\n",
      "Model Saved\n",
      "Episode: 242 Total reward: 90.0 Training loss: 0.9491 Explore P: 0.2798\n",
      "Episode: 243 Total reward: 68.0 Training loss: 1.4494 Explore P: 0.2791\n",
      "Episode: 244 Total reward: 72.0 Training loss: 1.1945 Explore P: 0.2784\n",
      "Model Saved\n",
      "Episode: 246 Total reward: 88.0 Training loss: 1.4687 Explore P: 0.2754\n",
      "Episode: 247 Total reward: 92.0 Training loss: 0.9914 Explore P: 0.2752\n",
      "Episode: 248 Total reward: 40.0 Training loss: 0.8315 Explore P: 0.2738\n",
      "Model Saved\n",
      "Episode: 251 Total reward: 63.0 Training loss: 0.9043 Explore P: 0.2678\n",
      "Episode: 252 Total reward: 61.0 Training loss: 0.8051 Explore P: 0.2669\n",
      "Episode: 253 Total reward: 41.0 Training loss: 0.5043 Explore P: 0.2656\n",
      "Episode: 254 Total reward: 90.0 Training loss: 1.1675 Explore P: 0.2653\n",
      "Episode: 255 Total reward: 66.0 Training loss: 1.2348 Explore P: 0.2645\n",
      "Model Saved\n",
      "Episode: 256 Total reward: 62.0 Training loss: 0.6355 Explore P: 0.2637\n",
      "Episode: 258 Total reward: 92.0 Training loss: 1.9359 Explore P: 0.2609\n",
      "Episode: 260 Total reward: 92.0 Training loss: 2.3670 Explore P: 0.2582\n",
      "Model Saved\n",
      "Episode: 261 Total reward: -15.0 Training loss: 2.1857 Explore P: 0.2559\n",
      "Episode: 264 Total reward: 92.0 Training loss: 0.7471 Explore P: 0.2509\n",
      "Episode: 265 Total reward: 38.0 Training loss: 0.9396 Explore P: 0.2496\n",
      "Model Saved\n",
      "Episode: 267 Total reward: 93.0 Training loss: 11.1074 Explore P: 0.2470\n",
      "Episode: 268 Total reward: 70.0 Training loss: 0.5926 Explore P: 0.2464\n",
      "Episode: 270 Total reward: 22.0 Training loss: 1.0283 Explore P: 0.2425\n",
      "Model Saved\n",
      "Episode: 271 Total reward: 95.0 Training loss: 2.1030 Explore P: 0.2424\n",
      "Episode: 274 Total reward: 1.0 Training loss: 4.1394 Explore P: 0.2360\n",
      "Model Saved\n",
      "Episode: 280 Total reward: 73.0 Training loss: 0.6634 Explore P: 0.2245\n",
      "Model Saved\n",
      "Episode: 282 Total reward: 61.0 Training loss: 0.8294 Explore P: 0.2216\n",
      "Episode: 283 Total reward: 88.0 Training loss: 0.9478 Explore P: 0.2213\n",
      "Episode: 284 Total reward: 19.0 Training loss: 0.7963 Explore P: 0.2199\n",
      "Model Saved\n",
      "Episode: 286 Total reward: 87.0 Training loss: 0.7226 Explore P: 0.2175\n",
      "Episode: 290 Total reward: 91.0 Training loss: 0.9665 Explore P: 0.2112\n",
      "Model Saved\n",
      "Episode: 293 Total reward: 61.0 Training loss: 1.7253 Explore P: 0.2065\n",
      "Episode: 295 Total reward: -19.0 Training loss: 1.1577 Explore P: 0.2027\n",
      "Model Saved\n",
      "Episode: 297 Total reward: 93.0 Training loss: 1.4828 Explore P: 0.2007\n",
      "Episode: 298 Total reward: 93.0 Training loss: 0.7292 Explore P: 0.2005\n",
      "Episode: 299 Total reward: 95.0 Training loss: 0.6601 Explore P: 0.2004\n",
      "Model Saved\n",
      "Episode: 301 Total reward: 89.0 Training loss: 0.7920 Explore P: 0.1983\n",
      "Episode: 302 Total reward: 70.0 Training loss: 0.5576 Explore P: 0.1978\n",
      "Episode: 305 Total reward: 92.0 Training loss: 0.9409 Explore P: 0.1939\n",
      "Model Saved\n",
      "Episode: 307 Total reward: 90.0 Training loss: 2.2580 Explore P: 0.1919\n",
      "Episode: 310 Total reward: -21.0 Training loss: 0.6534 Explore P: 0.1866\n",
      "Model Saved\n",
      "Episode: 312 Total reward: 23.0 Training loss: 0.5638 Explore P: 0.1837\n",
      "Episode: 313 Total reward: 65.0 Training loss: 1.4457 Explore P: 0.1832\n",
      "Model Saved\n",
      "Episode: 316 Total reward: 90.0 Training loss: 0.8039 Explore P: 0.1795\n",
      "Episode: 317 Total reward: 91.0 Training loss: 1.5762 Explore P: 0.1794\n",
      "Episode: 319 Total reward: 68.0 Training loss: 0.8154 Explore P: 0.1772\n",
      "Episode: 320 Total reward: 61.0 Training loss: 1.6222 Explore P: 0.1766\n",
      "Model Saved\n",
      "Episode: 323 Total reward: 91.0 Training loss: 1.0538 Explore P: 0.1732\n",
      "Model Saved\n",
      "Episode: 326 Total reward: 91.0 Training loss: 0.6207 Explore P: 0.1698\n",
      "Episode: 327 Total reward: 5.0 Training loss: 1.0814 Explore P: 0.1685\n",
      "Episode: 328 Total reward: 94.0 Training loss: 0.5400 Explore P: 0.1684\n",
      "Episode: 330 Total reward: 58.0 Training loss: 0.9697 Explore P: 0.1662\n",
      "Model Saved\n",
      "Episode: 332 Total reward: 95.0 Training loss: 1.4943 Explore P: 0.1646\n",
      "Episode: 333 Total reward: 95.0 Training loss: 0.5832 Explore P: 0.1645\n",
      "Model Saved\n",
      "Episode: 336 Total reward: 91.0 Training loss: 0.6541 Explore P: 0.1613\n",
      "Episode: 337 Total reward: 94.0 Training loss: 2.2232 Explore P: 0.1612\n",
      "Episode: 338 Total reward: 92.0 Training loss: 12.8634 Explore P: 0.1610\n",
      "Episode: 339 Total reward: 93.0 Training loss: 3.8233 Explore P: 0.1609\n",
      "Model Saved\n",
      "Episode: 343 Total reward: 2.0 Training loss: 0.6757 Explore P: 0.1553\n",
      "Episode: 345 Total reward: 92.0 Training loss: 0.7705 Explore P: 0.1537\n",
      "Model Saved\n",
      "Model Saved\n",
      "Episode: 352 Total reward: 90.0 Training loss: 1.0259 Explore P: 0.1452\n",
      "Episode: 353 Total reward: 90.0 Training loss: 0.4586 Explore P: 0.1450\n",
      "Episode: 354 Total reward: -24.0 Training loss: 4.2629 Explore P: 0.1437\n",
      "Model Saved\n",
      "Episode: 356 Total reward: 90.0 Training loss: 1.9055 Explore P: 0.1422\n",
      "Episode: 359 Total reward: 95.0 Training loss: 0.6474 Explore P: 0.1395\n",
      "Model Saved\n",
      "Episode: 361 Total reward: 95.0 Training loss: 0.5803 Explore P: 0.1382\n",
      "Episode: 362 Total reward: 93.0 Training loss: 6.3073 Explore P: 0.1381\n",
      "Episode: 364 Total reward: 89.0 Training loss: 0.6929 Explore P: 0.1366\n",
      "Model Saved\n",
      "Episode: 366 Total reward: 67.0 Training loss: 0.6117 Explore P: 0.1350\n",
      "Episode: 368 Total reward: 93.0 Training loss: 0.6718 Explore P: 0.1337\n",
      "Episode: 369 Total reward: 90.0 Training loss: 0.5277 Explore P: 0.1335\n",
      "Episode: 370 Total reward: 91.0 Training loss: 0.9964 Explore P: 0.1334\n",
      "Model Saved\n",
      "Episode: 371 Total reward: 91.0 Training loss: 0.8923 Explore P: 0.1333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 373 Total reward: 92.0 Training loss: 0.6745 Explore P: 0.1319\n",
      "Episode: 374 Total reward: 26.0 Training loss: 0.6884 Explore P: 0.1312\n",
      "Model Saved\n",
      "Episode: 377 Total reward: 95.0 Training loss: 1.3265 Explore P: 0.1287\n",
      "Episode: 379 Total reward: 93.0 Training loss: 0.5921 Explore P: 0.1275\n",
      "Model Saved\n",
      "Episode: 382 Total reward: 85.0 Training loss: 9.4578 Explore P: 0.1250\n",
      "Episode: 384 Total reward: 74.0 Training loss: 0.5366 Explore P: 0.1236\n",
      "Episode: 385 Total reward: 93.0 Training loss: 0.8239 Explore P: 0.1235\n",
      "Model Saved\n",
      "Episode: 386 Total reward: 93.0 Training loss: 0.7174 Explore P: 0.1234\n",
      "Episode: 387 Total reward: 95.0 Training loss: 0.9113 Explore P: 0.1233\n",
      "Model Saved\n",
      "Episode: 392 Total reward: 93.0 Training loss: 0.6927 Explore P: 0.1188\n",
      "Episode: 394 Total reward: 88.0 Training loss: 0.8784 Explore P: 0.1176\n",
      "Episode: 395 Total reward: 91.0 Training loss: 0.7949 Explore P: 0.1175\n",
      "Model Saved\n",
      "Episode: 398 Total reward: 92.0 Training loss: 1.0385 Explore P: 0.1152\n",
      "Episode: 400 Total reward: -13.0 Training loss: 0.8907 Explore P: 0.1132\n",
      "Model Saved\n",
      "Episode: 401 Total reward: 89.0 Training loss: 1.3864 Explore P: 0.1131\n",
      "Episode: 402 Total reward: 39.0 Training loss: 1.0195 Explore P: 0.1126\n",
      "Episode: 403 Total reward: 95.0 Training loss: 2.1351 Explore P: 0.1125\n",
      "Episode: 405 Total reward: 92.0 Training loss: 1.1507 Explore P: 0.1114\n",
      "Model Saved\n",
      "Episode: 409 Total reward: 6.0 Training loss: 0.8148 Explore P: 0.1076\n",
      "Model Saved\n",
      "Episode: 412 Total reward: 89.0 Training loss: 1.2386 Explore P: 0.1056\n",
      "Episode: 413 Total reward: 93.0 Training loss: 0.9994 Explore P: 0.1055\n",
      "Episode: 414 Total reward: 39.0 Training loss: 0.9185 Explore P: 0.1050\n",
      "Model Saved\n",
      "Episode: 417 Total reward: 93.0 Training loss: 0.7762 Explore P: 0.1030\n",
      "Episode: 420 Total reward: 24.0 Training loss: 0.4390 Explore P: 0.1006\n",
      "Model Saved\n",
      "Episode: 423 Total reward: 92.0 Training loss: 1.0522 Explore P: 0.0987\n",
      "Episode: 424 Total reward: 61.0 Training loss: 1.2543 Explore P: 0.0984\n",
      "Episode: 425 Total reward: 94.0 Training loss: 0.6522 Explore P: 0.0983\n",
      "Model Saved\n",
      "Episode: 428 Total reward: 93.0 Training loss: 1.0118 Explore P: 0.0965\n",
      "Episode: 429 Total reward: -24.0 Training loss: 1.2199 Explore P: 0.0957\n",
      "Model Saved\n",
      "Episode: 431 Total reward: 93.0 Training loss: 0.6722 Explore P: 0.0947\n",
      "Episode: 432 Total reward: 91.0 Training loss: 0.8062 Explore P: 0.0946\n",
      "Episode: 434 Total reward: 95.0 Training loss: 3.2323 Explore P: 0.0938\n",
      "Episode: 435 Total reward: 7.0 Training loss: 0.7227 Explore P: 0.0931\n",
      "Model Saved\n",
      "Episode: 436 Total reward: 63.0 Training loss: 1.1920 Explore P: 0.0929\n",
      "Episode: 440 Total reward: 67.0 Training loss: 0.8674 Explore P: 0.0902\n",
      "Model Saved\n",
      "Episode: 441 Total reward: 68.0 Training loss: 2.2152 Explore P: 0.0900\n",
      "Episode: 442 Total reward: 93.0 Training loss: 29.0138 Explore P: 0.0899\n",
      "Episode: 443 Total reward: 93.0 Training loss: 0.7493 Explore P: 0.0898\n",
      "Model Saved\n",
      "Episode: 446 Total reward: 95.0 Training loss: 0.9029 Explore P: 0.0882\n",
      "Episode: 447 Total reward: 70.0 Training loss: 0.8779 Explore P: 0.0880\n",
      "Model Saved\n",
      "Episode: 451 Total reward: 93.0 Training loss: 1.7761 Explore P: 0.0856\n",
      "Episode: 453 Total reward: 93.0 Training loss: 1.0834 Explore P: 0.0848\n",
      "Episode: 455 Total reward: 70.0 Training loss: 1.3294 Explore P: 0.0839\n",
      "Model Saved\n",
      "Episode: 456 Total reward: 91.0 Training loss: 1.0404 Explore P: 0.0838\n",
      "Episode: 458 Total reward: 93.0 Training loss: 1.0527 Explore P: 0.0830\n",
      "Episode: 459 Total reward: 95.0 Training loss: 6.8664 Explore P: 0.0830\n",
      "Model Saved\n",
      "Episode: 462 Total reward: 95.0 Training loss: 3.3486 Explore P: 0.0815\n",
      "Episode: 465 Total reward: 95.0 Training loss: 0.7806 Explore P: 0.0800\n",
      "Model Saved\n",
      "Episode: 467 Total reward: 92.0 Training loss: 1.6335 Explore P: 0.0793\n",
      "Episode: 469 Total reward: 93.0 Training loss: 1.8485 Explore P: 0.0785\n",
      "Episode: 470 Total reward: 93.0 Training loss: 0.9614 Explore P: 0.0785\n",
      "Model Saved\n",
      "Episode: 471 Total reward: 95.0 Training loss: 0.4884 Explore P: 0.0784\n",
      "Episode: 472 Total reward: 86.0 Training loss: 9.5411 Explore P: 0.0783\n",
      "Episode: 473 Total reward: 65.0 Training loss: 0.5601 Explore P: 0.0781\n",
      "Episode: 474 Total reward: 56.0 Training loss: 1.1232 Explore P: 0.0778\n",
      "Episode: 475 Total reward: 95.0 Training loss: 0.7891 Explore P: 0.0778\n",
      "Model Saved\n",
      "Episode: 476 Total reward: 93.0 Training loss: 0.6594 Explore P: 0.0777\n",
      "Episode: 477 Total reward: 95.0 Training loss: 1.8926 Explore P: 0.0777\n",
      "Episode: 479 Total reward: 92.0 Training loss: 0.6740 Explore P: 0.0770\n",
      "Episode: 480 Total reward: 93.0 Training loss: 2.9969 Explore P: 0.0769\n",
      "Model Saved\n",
      "Episode: 481 Total reward: 95.0 Training loss: 0.7176 Explore P: 0.0769\n",
      "Episode: 482 Total reward: 91.0 Training loss: 0.8045 Explore P: 0.0768\n",
      "Episode: 483 Total reward: 93.0 Training loss: 1.1025 Explore P: 0.0768\n",
      "Episode: 484 Total reward: 90.0 Training loss: 0.9586 Explore P: 0.0767\n",
      "Episode: 485 Total reward: 93.0 Training loss: 1.7741 Explore P: 0.0766\n",
      "Model Saved\n",
      "Episode: 487 Total reward: 85.0 Training loss: 1.0762 Explore P: 0.0759\n",
      "Episode: 488 Total reward: 95.0 Training loss: 0.6767 Explore P: 0.0758\n",
      "Episode: 489 Total reward: 94.0 Training loss: 1.0460 Explore P: 0.0758\n",
      "Model Saved\n",
      "Episode: 491 Total reward: 95.0 Training loss: 3.0383 Explore P: 0.0751\n",
      "Episode: 494 Total reward: 88.0 Training loss: 0.7013 Explore P: 0.0737\n",
      "Episode: 495 Total reward: 95.0 Training loss: 1.2561 Explore P: 0.0737\n",
      "Model Saved\n",
      "Episode: 497 Total reward: 95.0 Training loss: 0.5741 Explore P: 0.0730\n",
      "Episode: 498 Total reward: 95.0 Training loss: 0.7789 Explore P: 0.0730\n",
      "Episode: 499 Total reward: 95.0 Training loss: 2.2996 Explore P: 0.0729\n"
     ]
    }
   ],
   "source": [
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer.add_graph(sess.graph)\n",
    "        tau = 0\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            \n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                tau += 1\n",
    "                \n",
    "                # Increase decay_step\n",
    "                decay_step +=1\n",
    "                \n",
    "                # Predict the action to take and take it\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "                # Do the action\n",
    "                reward = game.make_action(action)\n",
    "\n",
    "                # Look if the episode is finished\n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                # Add the reward to total reward\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((84,84), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                else:\n",
    "                    # Get the next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "                ### LEARNING PART            \n",
    "                # Obtain random mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch]) \n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                 # Get Q values for next_state \n",
    "                Qs_next_state = sess.run(DeepQNetwork.output, feed_dict = {DeepQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                #calculate q target in the target network\n",
    "                q_target = sess.run(TargetQNetwork.output, feed_dict={ TargetQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                #update the target weights by running the operations \n",
    "                if tau >= max_tau:\n",
    "                    update_target_ops = update_target_network()\n",
    "                    sess.run(update_target_ops)\n",
    "                    tau = 0\n",
    "                \n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    #a' for s'\n",
    "                    action = np.argmax(Qs_next_state[i])\n",
    "                    \n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * q_target[i][action]\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                loss, _ = sess.run([DeepQNetwork.loss, DeepQNetwork.optimizer],\n",
    "                                    feed_dict={DeepQNetwork.inputs_: states_mb,\n",
    "                                               DeepQNetwork.target_Q: targets_mb,\n",
    "                                               DeepQNetwork.actions_: actions_mb})\n",
    "\n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DeepQNetwork.inputs_: states_mb,\n",
    "                                                   DeepQNetwork.target_Q: targets_mb,\n",
    "                                                   DeepQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "\n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Watch our Agent play üëÄ\n",
    "Now that we trained our agent, we can test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models\\model.ckpt\n",
      "Score:  12.0\n",
      "Score:  95.0\n",
      "Score:  -385.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  -24.0\n",
      "Score:  95.0\n",
      "Score:  74.0\n",
      "Score:  42.0\n",
      "Score:  93.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-9703a2e640ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpossible_actions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_total_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    totalScore = 0\n",
    "    \n",
    "   \n",
    "    # Load the model\n",
    "    saver = tf.train.import_meta_graph(\"./models/model.ckpt.meta\")\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(\"./models\"))\n",
    "                  \n",
    "    for i in range(200):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            frame = game.get_state().screen_buffer\n",
    "            state,_ = stack_frames(stacked_frames, frame,False)\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            Qs = sess.run(DeepQNetwork.output, feed_dict = {DeepQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            action = np.argmax(Qs)\n",
    "            action = possible_actions[int(action)]\n",
    "            game.make_action(action)        \n",
    "            score = game.get_total_reward()\n",
    "            time.sleep(0.01)\n",
    "        print(\"Score: \", score)\n",
    "        totalScore += score\n",
    "        time.sleep(1)\n",
    "    print(\"TOTAL_SCORE\", totalScore/100.0)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
